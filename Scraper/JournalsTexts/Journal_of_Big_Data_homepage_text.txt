Texte du site principal (https://journalofbigdata.springeropen.com/):
Home page | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Featured Collections on Computationally Intensive Problems in General Math and Engineering
This two-part special issue covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
Open Special Issues
Customization and fine-tuning of machine learning models
Submission Deadline: 15 December 2024
Advancements on Automated Data Platform Management, Orchestration, and Optimization
Submission Deadline: 30 September 2024
Emergent architectures and technologies for big data management and analysis
Submission Deadline: 1 October 2024
View our collection of open and closed special issues
Read our guidelines for special issue proposals
here
.
Articles
Recent
Most accessed
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Content type:
Research
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Content type:
Research
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Content type:
Research
18 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Content type:
Research
17 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Content type:
Research
12 September 2024
Most recent articles RSS
View all articles
A survey on Image Data Augmentation for Deep Learning
Authors:
Connor Shorten and Taghi M. Khoshgoftaar
Content type:
Survey paper
6 July 2019
Big data in healthcare: management, analysis and future prospects
Authors:
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
Content type:
Survey paper
19 June 2019
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Authors:
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
Content type:
Survey Paper
31 March 2021
Deep learning applications and challenges in big data analytics
Authors:
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
Content type:
Research
24 February 2015
Short-term stock market price trend prediction using a comprehensive deep learning system
Authors:
Jingyi Shen and M. Omair Shafiq
Content type:
Research
28 August 2020
Most accessed articles RSS
View all articles
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Top 10 Most Cited Articles 2023
A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications
Alzubaidi L., Bai J., Al-Sabaawi A. et al., (2023)
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset
Yin Y., Jang-Jaccard J., Xu W. et al., (2023)
A deep learning-based model using hybrid feature extraction approach for consumer sentiment analysis
Gagandeep Kaur, Amit Sharma (2023)
Skin-Net: a novel deep residual network for skin lesions classification using multilevel feature extraction and cross-channel correlation with detection of outlier
Yousef S. Alsahafi, Mohamed A. Kassem, Khalid M. Hosny
Read the rest of the list
here
.
Latest Tweets
View Twitter timeline
Your browser needs to have JavaScript enabled to view this timeline
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Advertisement
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/#main-content):
Home page | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Featured Collections on Computationally Intensive Problems in General Math and Engineering
This two-part special issue covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
Open Special Issues
Customization and fine-tuning of machine learning models
Submission Deadline: 15 December 2024
Advancements on Automated Data Platform Management, Orchestration, and Optimization
Submission Deadline: 30 September 2024
Emergent architectures and technologies for big data management and analysis
Submission Deadline: 1 October 2024
View our collection of open and closed special issues
Read our guidelines for special issue proposals
here
.
Articles
Recent
Most accessed
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Content type:
Research
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Content type:
Research
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Content type:
Research
18 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Content type:
Research
17 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Content type:
Research
12 September 2024
Most recent articles RSS
View all articles
A survey on Image Data Augmentation for Deep Learning
Authors:
Connor Shorten and Taghi M. Khoshgoftaar
Content type:
Survey paper
6 July 2019
Big data in healthcare: management, analysis and future prospects
Authors:
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
Content type:
Survey paper
19 June 2019
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Authors:
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
Content type:
Survey Paper
31 March 2021
Deep learning applications and challenges in big data analytics
Authors:
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
Content type:
Research
24 February 2015
Short-term stock market price trend prediction using a comprehensive deep learning system
Authors:
Jingyi Shen and M. Omair Shafiq
Content type:
Research
28 August 2020
Most accessed articles RSS
View all articles
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Top 10 Most Cited Articles 2023
A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications
Alzubaidi L., Bai J., Al-Sabaawi A. et al., (2023)
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset
Yin Y., Jang-Jaccard J., Xu W. et al., (2023)
A deep learning-based model using hybrid feature extraction approach for consumer sentiment analysis
Gagandeep Kaur, Amit Sharma (2023)
Skin-Net: a novel deep residual network for skin lesions classification using multilevel feature extraction and cross-channel correlation with detection of outlier
Yousef S. Alsahafi, Mohamed A. Kassem, Khalid M. Hosny
Read the rest of the list
here
.
Latest Tweets
View Twitter timeline
Your browser needs to have JavaScript enabled to view this timeline
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Advertisement
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/):
Home page | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Featured Collections on Computationally Intensive Problems in General Math and Engineering
This two-part special issue covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
Open Special Issues
Customization and fine-tuning of machine learning models
Submission Deadline: 15 December 2024
Advancements on Automated Data Platform Management, Orchestration, and Optimization
Submission Deadline: 30 September 2024
Emergent architectures and technologies for big data management and analysis
Submission Deadline: 1 October 2024
View our collection of open and closed special issues
Read our guidelines for special issue proposals
here
.
Articles
Recent
Most accessed
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Content type:
Research
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Content type:
Research
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Content type:
Research
18 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Content type:
Research
17 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Content type:
Research
12 September 2024
Most recent articles RSS
View all articles
A survey on Image Data Augmentation for Deep Learning
Authors:
Connor Shorten and Taghi M. Khoshgoftaar
Content type:
Survey paper
6 July 2019
Big data in healthcare: management, analysis and future prospects
Authors:
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
Content type:
Survey paper
19 June 2019
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Authors:
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
Content type:
Survey Paper
31 March 2021
Deep learning applications and challenges in big data analytics
Authors:
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
Content type:
Research
24 February 2015
Short-term stock market price trend prediction using a comprehensive deep learning system
Authors:
Jingyi Shen and M. Omair Shafiq
Content type:
Research
28 August 2020
Most accessed articles RSS
View all articles
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Top 10 Most Cited Articles 2023
A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications
Alzubaidi L., Bai J., Al-Sabaawi A. et al., (2023)
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset
Yin Y., Jang-Jaccard J., Xu W. et al., (2023)
A deep learning-based model using hybrid feature extraction approach for consumer sentiment analysis
Gagandeep Kaur, Amit Sharma (2023)
Skin-Net: a novel deep residual network for skin lesions classification using multilevel feature extraction and cross-channel correlation with detection of outlier
Yousef S. Alsahafi, Mohamed A. Kassem, Khalid M. Hosny
Read the rest of the list
here
.
Latest Tweets
View Twitter timeline
Your browser needs to have JavaScript enabled to view this timeline
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Advertisement
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/):
Home page | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Featured Collections on Computationally Intensive Problems in General Math and Engineering
This two-part special issue covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
Open Special Issues
Customization and fine-tuning of machine learning models
Submission Deadline: 15 December 2024
Advancements on Automated Data Platform Management, Orchestration, and Optimization
Submission Deadline: 30 September 2024
Emergent architectures and technologies for big data management and analysis
Submission Deadline: 1 October 2024
View our collection of open and closed special issues
Read our guidelines for special issue proposals
here
.
Articles
Recent
Most accessed
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Content type:
Research
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Content type:
Research
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Content type:
Research
18 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Content type:
Research
17 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Content type:
Research
12 September 2024
Most recent articles RSS
View all articles
A survey on Image Data Augmentation for Deep Learning
Authors:
Connor Shorten and Taghi M. Khoshgoftaar
Content type:
Survey paper
6 July 2019
Big data in healthcare: management, analysis and future prospects
Authors:
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
Content type:
Survey paper
19 June 2019
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Authors:
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
Content type:
Survey Paper
31 March 2021
Deep learning applications and challenges in big data analytics
Authors:
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
Content type:
Research
24 February 2015
Short-term stock market price trend prediction using a comprehensive deep learning system
Authors:
Jingyi Shen and M. Omair Shafiq
Content type:
Research
28 August 2020
Most accessed articles RSS
View all articles
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Top 10 Most Cited Articles 2023
A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications
Alzubaidi L., Bai J., Al-Sabaawi A. et al., (2023)
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset
Yin Y., Jang-Jaccard J., Xu W. et al., (2023)
A deep learning-based model using hybrid feature extraction approach for consumer sentiment analysis
Gagandeep Kaur, Amit Sharma (2023)
Skin-Net: a novel deep residual network for skin lesions classification using multilevel feature extraction and cross-channel correlation with detection of outlier
Yousef S. Alsahafi, Mohamed A. Kassem, Khalid M. Hosny
Read the rest of the list
here
.
Latest Tweets
View Twitter timeline
Your browser needs to have JavaScript enabled to view this timeline
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Advertisement
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/about):
About | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
About
Contact
Editorial Board
About
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Benefits of publishing with SpringerOpen
High visibility
Journal of Big Data
's open access policy allows maximum visibility of articles published in the journal as they are available to a wide, global audience.
Speed of publication
Journal of Big Data
offers a fast publication schedule whilst maintaining rigorous peer review; all articles must be submitted online, and peer review is managed fully electronically (articles are distributed in PDF form, which is automatically generated from the submitted files). Articles will be published with their final citation after acceptance, in both fully browsable web form, and as a formatted PDF; the article will then be available through
Journal of Big Data
and SpringerOpen.
Flexibility
Online publication in
Journal of Big Data
gives you the opportunity to publish large datasets, large numbers of color illustrations and moving pictures, to display data in a form that can be read directly by other software packages so as to allow readers to manipulate the data for themselves, and to create all relevant links (for example, to
PubMed
, to sequence and other databases, and to other articles).
Promotion and press coverage
Articles published in
Journal of Big Data
are included in article alerts and regular email updates.
In addition, articles published in
Journal of Big Data
may be promoted by press releases to the general or scientific press. These activities increase the exposure and number of accesses for articles published in
Journal of Big Data
.
Copyright
Authors of articles published in
Journal of Big Data
retain the copyright of their articles and are free to reproduce and disseminate their work (for further details, see the
copyright and license agreement
).
For further information about the advantages of publishing in a journal from SpringerOpen, please click
here
.
Open access
All articles published by the
Journal of Big Data
are made freely and permanently accessible online immediately upon publication, without subscription charges or registration barriers. Further information about open access can be found
here
.
As authors of articles published in the
Journal of Big Data
you are the copyright holders of your article and have granted to any third party, in advance and in perpetuity, the right to use, reproduce or disseminate your article, according to the
SpringerOpen copyright and license agreement
.
For those of you who are US government employees or are prevented from being copyright holders for similar reasons, SpringerOpen can accommodate non-standard copyright lines. Please
contact us
if further information is needed.
Article processing charges (APC)
Authors who publish open access in
Journal of Big Data
are required to pay an article processing charge (APC). The APC price will be determined from the date on which the article is accepted for publication.
The current APC, subject to VAT or local taxes where applicable, is: £1340.00/$2090.00/€1790.00
Visit
our open access support portal
and
our Journal Pricing FAQs
for further information.
Open access funding
Visit Springer Nature’s
open access funding & support services
for information about research funders and institutions that provide funding for APCs.
Springer Nature offers agreements that enable institutions to cover open access publishing costs. Learn more about our
open access agreements
to check your eligibility and discover whether this journal is included.
Springer Nature offers APC waivers and discounts for articles published in our fully open access journals whose corresponding authors are based in the world’s lowest income countries (see
our APC waivers and discounts policy
for further information). Requests for APC waivers and discounts from other authors will be considered on a case-by-case basis, and may be granted in cases of financial need (see
our open access policies for journals
for more information). All applications for discretionary APC waivers and discounts should be made at the point of manuscript submission; requests made during the review process or after acceptance are unable to be considered.
Indexing services
All articles published in
Journal of Big Data
are included in:
ESCI
Scopus
DLBP
DOAJ
ProQuest
EBSCO Discover Service
The full text of all articles is deposited in digital archives around the world to guarantee long-term digital preservation. You can also access all articles published by SpringerOpen on
SpringerLink
.
Peer-review policy
Peer review is the system used to assess the quality of a manuscript before it is published. Independent researchers in the relevant research area assess submitted manuscripts for originality, validity and significance to help Editors determine whether the manuscript should be published in their journal. You can read more about the peer-review process
here
.
Journal of Big Data
operates a single-blind peer-review system, where the reviewers are aware of the names and affiliations of the authors, but the reviewer reports provided to authors are anonymous.
The benefit of single-blind peer review is that it is the traditional model of peer review that many reviewers are comfortable with, and it facilitates a dispassionate critique of a manuscript.
Submitted manuscripts will generally be reviewed by two or more experts who will be asked to evaluate whether the manuscript is scientifically sound and coherent, whether it duplicates already published work, and whether or not the manuscript is sufficiently clear for publication. The Editors will reach a decision based on these reports and, where necessary, they will consult with members of the Editorial Board.
Editorial policies
All manuscripts submitted to
Journal of Big Data
should adhere to SpringerOpen's
editorial policies
.
Once your article is accepted, it will be processed by production and published shortly afterwards. In some cases, articles may be held for a short period of time prior to publication. If you have any concerns or particular requirements please contact the Journal.
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Citing articles in
Journal of Big Data
Articles in
Journal of Big Data
should be cited in the same way as articles in a traditional journal. Because articles are not printed, they do not have page numbers; instead, they are given a unique article number.
Article citations follow this format:
Authors: Title.
J Big Data
[year], [volume number]:[article number].
e.g. Roberts LD, Hassall DG, Winegar DA, Haselden JN, Nicholls AW, Griffin JL: Increased hepatic oxidative metabolism distinguishes the action of Peroxisome Proliferator-Activated Receptor delta from Peroxisome Proliferator-Activated Receptor gamma in the Ob/Ob mouse.
J Big Data
2009,
1
:115.
refers to article 115 from Volume 1 of the journal.
Appeals and complaints
Authors who wish to appeal a rejection or make a complaint should follow the procedure outlined in the
BMC Editorial Policies
.
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles):
Articles | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Articles
Collections
Articles
Search by keyword
Search by citation
for results from
All volumes
Volume 11 (2024)
Volume 10 (2023)
Volume 9 (2022)
Volume 8 (2021)
Volume 7 (2020)
Volume 6 (2019)
Volume 5 (2018)
Volume 4 (2017)
Volume 3 (2016)
Volume 2 (2015)
Volume 1 (2014)
Search
Show results from
Select a volume
Volume 11 (2024)
Volume 10 (2023)
Volume 9 (2022)
Volume 8 (2021)
Volume 7 (2020)
Volume 6 (2019)
Volume 5 (2018)
Volume 4 (2017)
Volume 3 (2016)
Volume 2 (2015)
Volume 1 (2014)
Search
Page 1 of 20
Sort by
Relevance
Newest first
Oldest first
Submit
Optimizing poultry audio signal classification with deep learning and burn layer fusion
This study introduces a novel deep learning-based approach for classifying poultry audio signals, incorporating a custom Burn Layer to enhance model robustness. The methodology integrates digital audio signal ...
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Citation:
Journal of Big Data
2024
11
:135
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
In late 2023, the United Nations conference on climate change (COP28), which was held in Dubai, encouraged a quick move from fossil fuels to renewable energy. Solar energy is one of the most promising forms of...
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Citation:
Journal of Big Data
2024
11
:134
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
The frequent usage of computer networks and the Internet has made computer networks vulnerable to numerous attacks, highlighting the critical need to enhance the precision of security mechanisms. One of the mo...
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Citation:
Journal of Big Data
2024
11
:133
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
The tumor microenvironment (TME) provides a region for intricate interactions within or between immune and non-immune cells. We aimed to reveal the tissue architecture and comprehensive landscape of cells with...
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Citation:
Journal of Big Data
2024
11
:132
Content type:
Research
Published on:
17 September 2024
View
Full Text
View
PDF
Development and evaluation of a deep learning model for automatic segmentation of non-perfusion area in fundus fluorescein angiography
Diabetic retinopathy (DR) is the most prevalent cause of preventable vision loss worldwide, imposing a significant economic and medical burden on society today, of which early identification is the cornerstone...
Authors:
Wei Feng, Bingjie Wang, Dan Song, Mengda Li, Anming Chen, Jing Wang, Siyong Lin, Yiran Zhao, Bin Wang, Zongyuan Ge, Shuyi Xu and Yuntao Hu
Citation:
Journal of Big Data
2024
11
:131
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Evolutionary computation-based self-supervised learning for image processing: a big data-driven approach to feature extraction and fusion for multispectral object detection
The image object recognition and detection technology are widely used in many scenarios. In recent years, big data has become increasingly abundant, and big data-driven artificial intelligence models have attr...
Authors:
Xiaoyang Shen, Haibin Li, Achyut Shankar, Wattana Viriyasitavat and Vinay Chamola
Citation:
Journal of Big Data
2024
11
:130
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Leveraging large-scale genetic data to assess the causal impact of COVID-19 on multisystemic diseases
The long-term impacts of COVID-19 on human health are a major concern, yet comprehensive evaluations of its effects on various health conditions are lacking.
Authors:
Xiangyang Zhang, Zhaohui Jiang, Jiayao Ma, Yaru Qi, Yin Li, Yan Zhang, Yihan Liu, Chaochao Wei, Yihong Chen, Ping Liu, Yinghui Peng, Jun Tan, Ying Han, Shan Zeng, Changjing Cai and Hong Shen
Citation:
Journal of Big Data
2024
11
:129
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
This article presents an investment recommender system based on an Adaptive Neuro-Fuzzy Inference System (ANFIS) and pre-trained weights from a Multimodal Neural Network (MNN). The model is designed to support...
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Citation:
Journal of Big Data
2024
11
:128
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Inhibitory neuron links the causal relationship from air pollution to psychiatric disorders: a large multi-omics analysis
Psychiatric disorders are severe health challenges that exert a heavy public burden. Air pollution has been widely reported as related to psychiatric disorder risk, but their casual association and pathologica...
Authors:
Xisong Liang, Jie Wen, Chunrun Qu, Nan Zhang, Ziyu Dai, Hao Zhang, Peng Luo, Ming Meng, Zhixiong Liu, Fan Fan and Quan Cheng
Citation:
Journal of Big Data
2024
11
:127
Content type:
Research
Published on:
11 September 2024
View
Full Text
View
PDF
Enhancing oil palm segmentation model with GAN-based augmentation
In digital agriculture, accurate crop detection is fundamental to developing automated systems for efficient plantation management. For oil palm, the main challenge lies in developing robust models that perfor...
Authors:
Qi Bin Kwong, Yee Thung Kon, Wan Rusydiah W. Rusik, Mohd Nor Azizi Shabudin, Shahirah Shazana A. Rahman, Harikrishna Kulaveerasingam and David Ross Appleton
Citation:
Journal of Big Data
2024
11
:126
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
AI sees beyond humans: automated diagnosis of myopia based on peripheral refraction map using interpretable deep learning
The question of whether artificial intelligence (AI) can surpass human capabilities is crucial in the application of AI in clinical medicine. To explore this, an interpretable deep learning (DL) model was deve...
Authors:
Yong Tang, Zhenghua Lin, Linjing Zhou, Weijia Wang, Longbo Wen, Yongli Zhou, Zongyuan Ge, Zhao Chen, Weiwei Dai, Zhikuan Yang, He Tang and Weizhong Lan
Citation:
Journal of Big Data
2024
11
:125
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
Modeling the impact of BDA-AI on sustainable innovation ambidexterity and environmental performance
Data has evolved into one of the principal resources for contemporary businesses. Moreover, corporations have undergone digitalization; consequently, their supply chains generate substantial amounts of data. T...
Authors:
Chin-Tsu Chen, Asif Khan and Shih-Chih Chen
Citation:
Journal of Big Data
2024
11
:124
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
Efficient microservices offloading for cost optimization in diverse MEC cloud networks
In recent years, mobile applications have proliferated across domains such as E-banking, Augmented Reality, E-Transportation, and E-Healthcare. These applications are often built using microservices, an archit...
Authors:
Abdul Rasheed Mahesar, Xiaoping Li and Dileep Kumar Sajnani
Citation:
Journal of Big Data
2024
11
:123
Content type:
Research
Published on:
4 September 2024
View
Full Text
View
PDF
Predicting startup success using two bias-free machine learning: resolving data imbalance using generative adversarial networks
The success of newly established companies holds significant implications for community development and economic growth. However, startups often grapple with heightened vulnerability to market volatility, whic...
Authors:
Jungryeol Park, Saesol Choi and Yituo Feng
Citation:
Journal of Big Data
2024
11
:122
Content type:
Research
Published on:
3 September 2024
View
Full Text
View
PDF
CTGAN-ENN: a tabular GAN-based hybrid sampling method for imbalanced and overlapped data in customer churn prediction
Class imbalance is one of many problems of customer churn datasets. One of the common problems is class overlap, where the data have a similar instance between classes. The prediction task of customer churn be...
Authors:
I Nyoman Mahayasa Adiputra and Paweena Wanchai
Citation:
Journal of Big Data
2024
11
:121
Content type:
Research
Published on:
2 September 2024
View
Full Text
View
PDF
Cartographies of warfare in the Indian subcontinent: Contextualizing archaeological and historical analysis through big data approaches
Some of the most notable human behavioral palimpsests result from warfare and its durable traces in the form of defensive architecture and strategic infrastructure. For premodern periods, this architecture is ...
Authors:
Monica L. Smith and Connor Newton
Citation:
Journal of Big Data
2024
11
:120
Content type:
Case Study
Published on:
29 August 2024
View
Full Text
View
PDF
Automated subway touch button detection using image process
Subway button detection is paramount for passenger safety, yet the occurrence of inadvertent touches poses operational threats. Camera-based detection is indispensable for identifying touch occurrences, ascert...
Authors:
Junfeng An, Mengmeng Lu, Gang Li, Jiqiang Liu and Chongqing Wang
Citation:
Journal of Big Data
2024
11
:119
Content type:
Research
Published on:
29 August 2024
View
Full Text
View
PDF
Cybersecurity vulnerabilities and solutions in Ethiopian university websites
This study investigates the causes and countermeasures of cybercrime vulnerabilities, specifically focusing on selected 16 Ethiopian university websites. This study uses a cybersecurity awareness survey, and a...
Authors:
Ali Yimam Eshetu, Endris Abdu Mohammed and Ayodeji Olalekan Salau
Citation:
Journal of Big Data
2024
11
:118
Content type:
Research
Published on:
23 August 2024
View
Full Text
View
PDF
Crude oil price forecasting using K-means clustering and LSTM model enhanced by dense-sparse-dense strategy
Crude oil is an essential energy source that affects international trade, transportation, and manufacturing, highlighting its importance to the economy. Its future price prediction affects consumer prices and ...
Authors:
Alireza Jahandoost, Farhad Abedinzadeh Torghabeh, Seyyed Abed Hosseini and Mahboobeh Houshmand
Citation:
Journal of Big Data
2024
11
:117
Content type:
Research
Published on:
17 August 2024
View
Full Text
View
PDF
Rs-net: Residual Sharp U-Net architecture for pavement crack segmentation and severity assessment
U-net, a fully convolutional network-based image segmentation method, has demonstrated widespread adaptability in the crack segmentation task. The combination of the semantically dissimilar features of the enc...
Authors:
Luqman Ali, Hamad AlJassmi, Mohammed Swavaf, Wasif Khan and Fady Alnajjar
Citation:
Journal of Big Data
2024
11
:116
Content type:
Research
Published on:
17 August 2024
View
Full Text
View
PDF
Internet of things and ensemble learning-based mental and physical fatigue monitoring for smart construction sites
The construction industry substantially contributes to the economic growth of a country. However, it records a large number of workplace injuries and fatalities annually due to its hesitant adoption of automat...
Authors:
Bubryur Kim, K. R. Sri Preethaa, Sujeen Song, R. R. Lukacs, Jinwoo An, Zengshun Chen, Euijung An and Sungho Kim
Citation:
Journal of Big Data
2024
11
:115
Content type:
Research
Published on:
16 August 2024
View
Full Text
View
PDF
Toward a globally lunar calendar: a machine learning-driven approach for crescent moon visibility prediction
This paper presents a comprehensive approach to harmonizing lunar calendars across different global regions, addressing the long-standing challenge of variations in new crescent Moon sightings that mark the be...
Authors:
Samia Loucif, Murad Al-Rajab, Raed Abu Zitar and Mahmoud Rezk
Citation:
Journal of Big Data
2024
11
:114
Content type:
Research
Published on:
12 August 2024
View
Full Text
View
PDF
Enhancing K-nearest neighbor algorithm: a comprehensive review and performance analysis of modifications
The k-Nearest Neighbors (kNN) method, established in 1951, has since evolved into a pivotal tool in data mining, recommendation systems, and Internet of Things (IoT), among other areas. This paper presents a c...
Authors:
Rajib Kumar Halder, Mohammed Nasir Uddin, Md. Ashraf Uddin, Sunil Aryal and Ansam Khraisat
Citation:
Journal of Big Data
2024
11
:113
Content type:
Survey
Published on:
11 August 2024
View
Full Text
View
PDF
Deep SqueezeNet learning model for diagnosis and prediction of maize leaf diseases
The maize leaf diseases create severe yield reductions and critical problems. The maize leaf disease should be discovered early, perfectly identified, and precisely diagnosed to make greater yield. This work s...
Authors:
Prasannavenkatesan Theerthagiri, A. Usha Ruby, J. George Chellin Chandran, Tanvir Habib Sardar and Ahamed Shafeeq B. M.
Citation:
Journal of Big Data
2024
11
:112
Content type:
Research
Published on:
10 August 2024
View
Full Text
View
PDF
An aspect sentiment analysis model with Aspect Gated Convolution and Dual-Feature Filtering layers
Aspect level sentiment analysis is a basic task to determine the sentiment bias based on the contextual information near the aspect words. Some sentences contain many confusing feature words due to incomplete ...
Authors:
Hongfang Gong and Siyu Zhang
Citation:
Journal of Big Data
2024
11
:111
Content type:
Methodology
Published on:
9 August 2024
View
Full Text
View
PDF
Context-aware prediction of active and passive user engagement: Evidence from a large online social platform
The success of online social platforms hinges on their ability to predict and understand user behavior at scale. Here, we present data suggesting that context-aware modeling approaches may offer a holistic yet...
Authors:
Heinrich Peters, Yozen Liu, Francesco Barbieri, Raiyan Abdul Baten, Sandra C. Matz and Maarten W. Bos
Citation:
Journal of Big Data
2024
11
:110
Content type:
Research
Published on:
8 August 2024
View
Full Text
View
PDF
Analysis of Graeco-Latin square designs in the presence of uncertain data
This paper addresses the Graeco-Latin square design (GLSD) under neutrosophic statistics. In this work, we propose a novel approach for analyzing Graeco-Latin square designs using uncertain observations.
Authors:
Abdulrahman AlAita, Muhammad Aslam, Khaled Al Sultan and Muhammad Saleem
Citation:
Journal of Big Data
2024
11
:109
Content type:
Research
Published on:
7 August 2024
View
Full Text
View
PDF
Memetic multilabel feature selection using pruned refinement process
With the growing complexity of data structures, which include high-dimensional and multilabel datasets, the significance of feature selection has become more emphasized. Multilabel feature selection endeavors ...
Authors:
Wangduk Seo, Jaegyun Park, Sanghyuck Lee, A-Seong Moon, Dae-Won Kim and Jaesung Lee
Citation:
Journal of Big Data
2024
11
:108
Content type:
Research
Published on:
6 August 2024
View
Full Text
View
PDF
Sentiment-based predictive models for online purchases in the era of marketing 5.0: a systematic review
The convergence of artificial intelligence (AI), big data (DB), and Internet of Things (IoT) in Society 5.0, has given rise to Marketing 5.0, revolutionizing personalized customer experiences. In this study, a...
Authors:
Veerajay Gooljar, Tomayess Issa, Sarita Hardin-Ramanan and Bilal Abu-Salih
Citation:
Journal of Big Data
2024
11
:107
Content type:
Survey
Published on:
5 August 2024
View
Full Text
View
PDF
Unlocking the potential of Naive Bayes for spatio temporal classification: a novel approach to feature expansion
Prediction processes in areas ranging from climate and disease spread to disasters and air pollution rely heavily on spatial–temporal data. Understanding and forecasting the distribution patterns of disease ca...
Authors:
Sri Suryani Prasetiyowati and Yuliant Sibaroni
Citation:
Journal of Big Data
2024
11
:106
Content type:
Research
Published on:
5 August 2024
View
Full Text
View
PDF
Advancing cybersecurity: a comprehensive review of AI-driven detection techniques
As the number and cleverness of cyber-attacks keep increasing rapidly, it's more important than ever to have good ways to detect and prevent them. Recognizing cyber threats quickly and accurately is crucial be...
Authors:
Aya H. Salem, Safaa M. Azzam, O. E. Emam and Amr A. Abohany
Citation:
Journal of Big Data
2024
11
:105
Content type:
Survey
Published on:
4 August 2024
View
Full Text
View
PDF
Interpolation-split: a data-centric deep learning approach with big interpolated data to boost airway segmentation performance
The morphology and distribution of airway tree abnormalities enable diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critic...
Authors:
Wing Keung Cheung, Ashkan Pakzad, Nesrin Mogulkoc, Sarah Helen Needleman, Bojidar Rangelov, Eyjolfur Gudmundsson, An Zhao, Mariam Abbas, Davina McLaverty, Dimitrios Asimakopoulos, Robert Chapman, Recep Savas, Sam M. Janes, Yipeng Hu, Daniel C. Alexander, John R. Hurst…
Citation:
Journal of Big Data
2024
11
:104
Content type:
Research
Published on:
4 August 2024
View
Full Text
View
PDF
DiabSense: early diagnosis of non-insulin-dependent diabetes mellitus using smartphone-based human activity recognition and diabetic retinopathy analysis with Graph Neural Network
Non-Insulin-Dependent Diabetes Mellitus (NIDDM) is a chronic health condition caused by high blood sugar levels, and if not treated early, it can lead to serious complications i.e. blindness. Human Activity Re...
Authors:
Md Nuho Ul Alam, Ibrahim Hasnine, Erfanul Hoque Bahadur, Abdul Kadar Muhammad Masum, Mercedes Briones Urbano, Manuel Masias Vergara, Jia Uddin, Imran Ashraf and Md. Abdus Samad
Citation:
Journal of Big Data
2024
11
:103
Content type:
Research
Published on:
3 August 2024
View
Full Text
View
PDF
An adaptive composite time series forecasting model for short-term traffic flow
Short-term traffic flow forecasting is a hot issue in the field of intelligent transportation. The research field of traffic forecasting has evolved greatly in past decades. With the rapid development of deep ...
Authors:
Qitan Shao, Xinglin Piao, Xiangyu Yao, Yuqiu Kong, Yongli Hu, Baocai Yin and Yong Zhang
Citation:
Journal of Big Data
2024
11
:102
Content type:
Methodology
Published on:
3 August 2024
View
Full Text
View
PDF
Fitcam: detecting and counting repetitive exercises with deep learning
Physical fitness is one of the most important traits a person could have for health longevity. Conducting regular exercise is fundamental to maintaining physical fitness, but with the caveat of occurring injur...
Authors:
Ferdinandz Japhne, Kevin Janada, Agustinus Theodorus and Andry Chowanda
Citation:
Journal of Big Data
2024
11
:101
Content type:
Research
Published on:
3 August 2024
View
Full Text
View
PDF
Tc-llama 2: fine-tuning LLM for technology and commercialization applications
This paper introduces TC-Llama 2, a novel application of large language models (LLMs) in the technology-commercialization field. Traditional methods in this field, reliant on statistical learning and expert kn...
Authors:
Jeyoon Yeom, Hakyung Lee, Hoyoon Byun, Yewon Kim, Jeongeun Byun, Yunjeong Choi, Sungjin Kim and Kyungwoo Song
Citation:
Journal of Big Data
2024
11
:100
Content type:
Research
Published on:
2 August 2024
View
Full Text
View
PDF
An ensemble machine learning model for predicting one-year mortality in elderly coronary heart disease patients with anemia
This study was designed to develop and validate a robust predictive model for one-year mortality in elderly coronary heart disease (CHD) patients with anemia using machine learning methods.
Authors:
Longcan Cheng, Yan Nie, Hongxia Wen, Yan Li, Yali Zhao, Qian Zhang, Mingxing Lei and Shihui Fu
Citation:
Journal of Big Data
2024
11
:99
Content type:
Research
Published on:
24 July 2024
View
Full Text
View
PDF
Predictive modelling of MapReduce job performance in cloud environments using machine learning techniques
Within the Hadoop ecosystem, MapReduce stands as a cornerstone for managing, processing, and mining large-scale datasets. Yet, the absence of efficient solutions for precise estimation of job execution times p...
Authors:
Mohammed Bergui, Soufiane Hourri, Said Najah and Nikola S. Nikolov
Citation:
Journal of Big Data
2024
11
:98
Content type:
Research
Published on:
23 July 2024
View
Full Text
View
PDF
Hate speech detection in the Bengali language: a comprehensive survey
The detection of hate speech (HS) in online platforms has become extremely important for maintaining a safe and inclusive environment. While significant progress has been made in English-language HS detection,...
Authors:
Abdullah Al Maruf, Ahmad Jainul Abidin, Md. Mahmudul Haque, Zakaria Masud Jiyad, Aditi Golder, Raaid Alubady and Zeyar Aung
Citation:
Journal of Big Data
2024
11
:97
Content type:
Survey
Published on:
23 July 2024
View
Full Text
View
PDF
Introducing Mplots: scaling time series recurrence plots to massive datasets
Time series similarity matrices (informally, recurrence plots or dot-plots), are useful tools for time series data mining. They can be used to guide data exploration, and various useful features can be derived...
Authors:
Maryam Shahcheraghi, Ryan Mercer, João Manuel de Almeida Rodrigues, Audrey Der, Hugo Filipe Silveira Gamboa, Zachary Zimmerman, Kerry Mauck and Eamonn Keogh
Citation:
Journal of Big Data
2024
11
:96
Content type:
Research
Published on:
20 July 2024
View
Full Text
View
PDF
Text summarization based on semantic graphs: an abstract meaning representation graph-to-text deep learning approach
Nowadays, due to the constantly growing amount of textual information, automatic text summarization constitutes an important research area in natural language processing. In this work, we present a novel frame...
Authors:
Panagiotis Kouris, Georgios Alexandridis and Andreas Stafylopatis
Citation:
Journal of Big Data
2024
11
:95
Content type:
Research
Published on:
14 July 2024
View
Full Text
View
PDF
Examining ALS: reformed PCA and random forest for effective detection of ALS
ALS (Amyotrophic Lateral Sclerosis) is a fatal neurodegenerative disease of the human motor system. It is a group of progressive diseases that affects the nerve cells in the brain and spinal cord that control ...
Authors:
Abdullah Alqahtani, Shtwai Alsubai, Mohemmed Sha and Ashit Kumar Dutta
Citation:
Journal of Big Data
2024
11
:94
Content type:
Research
Published on:
10 July 2024
View
Full Text
View
PDF
Emotion AWARE: an artificial intelligence framework for adaptable, robust, explainable, and multi-granular emotion analysis
Emotions are fundamental to human behaviour. How we feel, individually and collectively, determines how humanity evolves and advances into our shared future. The rapid digitalisation of our personal, social an...
Authors:
Gihan Gamage, Daswin De Silva, Nishan Mills, Damminda Alahakoon and Milos Manic
Citation:
Journal of Big Data
2024
11
:93
Content type:
Methodology
Published on:
10 July 2024
View
Full Text
View
PDF
Exploring AI-driven approaches for unstructured document analysis and future horizons
In the current industrial landscape, a significant number of sectors are grappling with the challenges posed by unstructured data, which incurs financial losses amounting to millions annually. If harnessed eff...
Authors:
Supriya V. Mahadevkar, Shruti Patil, Ketan Kotecha, Lim Way Soong and Tanupriya Choudhury
Citation:
Journal of Big Data
2024
11
:92
Content type:
Survey
Published on:
5 July 2024
View
Full Text
View
PDF
New custom rating for improving recommendation system performance
Recommendation system is currently attracting the interest of many explorers. Various new businesses have surfaced with the rise of online marketing (E-Commerce) in response to Covid-19 pandemic. This phenomen...
Authors:
Tora Fahrudin and Dedy Rahman Wijaya
Citation:
Journal of Big Data
2024
11
:91
Content type:
Research
Published on:
2 July 2024
View
Full Text
View
PDF
Optimization-based convolutional neural model for the classification of white blood cells
White blood cells (WBCs) are one of the most significant parts of the human immune system, and they play a crucial role in diagnosing the characteristics of pathologists and blood-related diseases. The charact...
Authors:
Tulasi Gayatri Devi and Nagamma Patil
Citation:
Journal of Big Data
2024
11
:90
Content type:
Research
Published on:
26 June 2024
View
Full Text
View
PDF
Advanced RIME architecture for global optimization and feature selection
The article introduces an innovative approach to global optimization and feature selection (FS) using the RIME algorithm, inspired by RIME-ice formation. The RIME algorithm employs a soft-RIME search strategy ...
Authors:
Ruba Abu Khurma, Malik Braik, Abdullah Alzaqebah, Krishna Gopal Dhal, Robertas Damaševičius and Bilal Abu-Salih
Citation:
Journal of Big Data
2024
11
:89
Content type:
Research
Published on:
18 June 2024
View
Full Text
View
PDF
Feature reduction for hepatocellular carcinoma prediction using machine learning algorithms
Hepatocellular carcinoma (HCC) is a highly prevalent form of liver cancer that necessitates accurate prediction models for early diagnosis and effective treatment. Machine learning algorithms have demonstrated...
Authors:
Ghada Mostafa, Hamdi Mahmoud, Tarek Abd El-Hafeez and Mohamed E. ElAraby
Citation:
Journal of Big Data
2024
11
:88
Content type:
Research
Published on:
18 June 2024
View
Full Text
View
PDF
Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering
The classification of imbalanced datasets is a prominent task in text mining and machine learning. The number of samples in each class is not uniformly distributed; one class contains a large number of samples...
Authors:
Muhammad Mujahid, EROL Kına, Furqan Rustam, Monica Gracia Villar, Eduardo Silva Alvarado, Isabel De La Torre Diez and Imran Ashraf
Citation:
Journal of Big Data
2024
11
:87
Content type:
Research
Published on:
17 June 2024
View
Full Text
View
PDF
Advancing machine learning with OCR2SEQ: an innovative approach to multi-modal data augmentation
OCR2SEQ represents an innovative advancement in Optical Character Recognition (OCR) technology, leveraging a multi-modal generative augmentation strategy to overcome traditional limitations in OCR systems. Thi...
Authors:
Michael Lowe, Joseph D. Prusa, Joffrey L. Leevy and Taghi M. Khoshgoftaar
Citation:
Journal of Big Data
2024
11
:86
Content type:
Research
Published on:
13 June 2024
View
Full Text
View
PDF
Previous
page
1
2
3
4
5
…
20
Next
page
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/submission-guidelines):
Submission guidelines | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submission Guidelines
Aims and scope
Fees and funding
Language editing services
Copyright
Preparing your manuscript
Prepare supporting information
Conditions of publication
Editorial policies
Peer-review policy
Promoting your publication
Submission guidelines
Our 3-step submission process
Before you submit
Before you submit, we recommend familiarizing yourself with the following.
Make sure you are submitting to the most suitable journal


    -
Aims and scope
Understand the costs and funding options


    -
Fees and funding
Make sure your manuscript is accurate and readable


    -
Language editing
Understand the copyright agreement


    -
Copyright
Ready to submit
To give your manuscript the best chance of publication, follow these editorial policies and formatting guidelines.
Journal of Big Data
publishes the following article types:
Research
Case Studies
Methodology
Brief Report
Survey
Click the relevant link to find style and formatting information for the article you are going to submit.
General formatting rules for all article types


    -
Prepare your manuscript
Make sure your submission is complete


    -
Prepare supporting information
Copyright and license agreement


    -
Agree to conditions
Read and agree to our Editorial Policies


    -
Editorial Policies
Submit and promote
After acceptance, we provide support so your article gains maximum impact in the scientific community and beyond.
Who decides whether my work will be accepted?


    -
Peer-review policy
Spreading the word


    -
Promoting your publication
Submit manuscript
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/cip-research):
Computationally Intensive Problems in General Math and Engineering: Basic Research and Development | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Computationally Intensive Problems in General Math and Engineering: Basic Research and Development
This two-part special issue for the
Journal of Big Data
covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
This special issue is separated into two distinct collections: part one covers basic research and development for problems in general engineering, and part two covers applicative research and education.
View part two here.
Collection Articles
About the editor
Featured Nobel Laureate Contributors
Automated segmentation of choroidal neovascularization on optical coherence tomography angiography images of neovascular age-related macular degeneration patients based on deep learning
Optical coherence tomography angiography (OCTA) has been a frequently used diagnostic method in neovascular age-related macular degeneration (nAMD) because it is non-invasive and provides a comprehensive view ...
Authors:
Wei Feng, Meihan Duan, Bingjie Wang, Yu Du, Yiran Zhao, Bin Wang, Lin Zhao, Zongyuan Ge and Yuntao Hu
Citation:
Journal of Big Data
2023
10
:111
Content type:
Research
Published on:
23 June 2023
View
Full Text
View
PDF
An approach to automatic classification of hate speech in sports domain on social media
Hate Speech encompasses different forms of trolling, bullying, harassment, and threats directed against specific individuals or groups. This phenomena is mainly expressed on Social Networks. For sports players...
Authors:
Staša Vujičić Stanković and Miljana Mladenović
Citation:
Journal of Big Data
2023
10
:109
Content type:
Research
Published on:
22 June 2023
View
Full Text
View
PDF
A new deep learning architecture with inductive bias balance for transformer oil temperature forecasting
Ensuring the optimal performance of power transformers is a laborious task in which the insulation system plays a vital role in decreasing their deterioration. The insulation system uses insulating oil to cont...
Authors:
Manuel J. Jiménez-Navarro, María Martínez-Ballesteros, Francisco Martínez-Álvarez and Gualberto Asencio-Cortés
Citation:
Journal of Big Data
2023
10
:80
Content type:
Methodology
Published on:
28 May 2023
View
Full Text
View
PDF
Gaussian transformation enhanced semi-supervised learning for sleep stage classification
Sleep disorders are significant health concerns affecting a large population. Related clinical studies face the deficiency in sleep data and challenges in data analysis, which requires enormous human expertise...
Authors:
Yifan Guo, Helen X. Mao, Jijun Yin and Zhi-Hong Mao
Citation:
Journal of Big Data
2023
10
:79
Content type:
Methodology
Published on:
27 May 2023
View
Full Text
View
PDF
Main memory controller with multiple media technologies for big data workloads
SRAM and DRAM memory technologies have been dominant in the implementations of memory subsystems. In recent years, and mainly driven by the huge memory demands of big data applications, NVRAM technology has em...
Authors:
Miguel A. Avargues, Manel Lurbe, Salvador Petit, Maria E. Gomez, Rui Yang, Xiaoping Zhu, Guanhao Wang and Julio Sahuquillo
Citation:
Journal of Big Data
2023
10
:75
Content type:
Research
Published on:
22 May 2023
View
Full Text
View
PDF
Research in computing-intensive simulations for nature-oriented civil-engineering and related scientific fields, using machine learning and big data: an overview of open problems
This article presents a taxonomy and represents a repository of open problems in computing for numerically and logically intensive problems in a number of disciplines that have to synergize for the best perfor...
Authors:
Zoran Babović, Branislav Bajat, Vladan Đokić, Filip Đorđević, Dražen Drašković, Nenad Filipović, Borko Furht, Nikola Gačić, Igor Ikodinović, Marija Ilić, Ayhan Irfanoglu, Branislav Jelenković, Aleksandar Kartelj, Gerhard Klimeck, Nenad Korolija, Miloš Kotlar…
Citation:
Journal of Big Data
2023
10
:73
Content type:
Research
Published on:
22 May 2023
View
Full Text
View
PDF
RILS-ROLS: robust symbolic regression via iterated local search and ordinary least squares
In this paper, we solve the well-known symbolic regression problem that has been intensively studied and has a wide range of applications. To solve it, we propose an efficient metaheuristic-based approach, cal...
Authors:
Aleksandar Kartelj and Marko Djukanović
Citation:
Journal of Big Data
2023
10
:71
Content type:
Research
Published on:
22 May 2023
View
Full Text
View
PDF
Practical ANN prediction models for the axial capacity of square CFST columns
In this study, two machine-learning algorithms based on the artificial neural network (ANN) model are proposed to estimate the ultimate compressive strength of square concrete-filled steel tubular columns. The...
Authors:
Filip Đorđević and Svetlana M. Kostić
Citation:
Journal of Big Data
2023
10
:67
Content type:
Research
Published on:
17 May 2023
View
Full Text
View
PDF
Unsupervised outlier detection for time-series data of indoor air quality using LSTM autoencoder with ensemble method
The proposed framework consists of three modules as an outlier detection method for indoor air quality data. We first use a long short-term memory autoencoder (LSTM-AE) based reconstruction error detector, whi...
Authors:
Junhyeok Park, Youngsuk Seo and Jaehyuk Cho
Citation:
Journal of Big Data
2023
10
:66
Content type:
Research
Published on:
17 May 2023
View
Full Text
View
PDF
Scenic routing navigation using property valuation
Extensive prior work has provided methods for the optimization of routing based on weights assigned to travel duration, and/or travel cost, and/or the distance traveled. Routing can be in various modalities, s...
Authors:
Naphtali Rishe, M. Hadi Amini and Malek Adjouadi
Citation:
Journal of Big Data
2023
10
:57
Content type:
Methodology
Published on:
4 May 2023
View
Full Text
View
PDF
Governance and sustainability of distributed continuum systems: a big data approach
Distributed computing continuum systems (DCCS) make use of a vast number of computing devices to process data generated by edge devices such as the Internet of Things and sensor nodes. Besides performing compu...
Authors:
Praveen Kumar Donta, Boris Sedlak, Victor Casamayor Pujol and Schahram Dustdar
Citation:
Journal of Big Data
2023
10
:53
Content type:
Methodology
Published on:
28 April 2023
View
Full Text
View
PDF
A practical Alzheimer’s disease classifier via brain imaging-based deep learning on 85,721 samples
Beyond detecting brain lesions or tumors, comparatively little success has been attained in identifying brain disorders such as Alzheimer’s disease (AD), based on magnetic resonance imaging (MRI). Many machine...
Authors:
Bin Lu, Hui-Xian Li, Zhi-Kai Chang, Le Li, Ning-Xuan Chen, Zhi-Chen Zhu, Hui-Xia Zhou, Xue-Ying Li, Yu-Wei Wang, Shi-Xian Cui, Zhao-Yu Deng, Zhen Fan, Hong Yang, Xiao Chen, Paul M. Thompson, Francisco Xavier Castellanos…
Citation:
Journal of Big Data
2022
9
:101
Content type:
Research
Published on:
13 October 2022
View
Full Text
View
PDF
Prof. Veljko Milutinovic
Fellow of the IEEE and of the Academy of Europe, Indiana University, Bloomington, IND, USA, Adjunct Professor, University of Belgrade, SRB, EUR, Visiting Professor
Prof. Veljko Milutinovic (1951) received his PhD from the University of Belgrade in Serbia, spent about a decade on various faculty positions in the USA (mostly at Purdue University and more recently at the University of Indiana in Bloomington), and was a co-designer of the DARPAs pioneering GaAs RISC microprocessor on 200MHz (about a decade before the first commercial effort on that same speed) and was a co-designer also of the related GaAs Systolic Array (with 4096 GaAs microprocessors). Later, for almost three decades, he taught and conducted research at the University of Belgrade in Serbia, for departments of EE, MATH, BA, and PHYS/CHEM. His research is mostly in datamining algorithms and dataflow computing, with the emphasis on mapping of data analytics algorithms onto fast energy efficient architectures. Most of his research was done in cooperation with industry (Intel, Fairchild, Honeywell, Maxeler, HP, IBM, NCR, RCA, etc... ). For 10 of his books, forewords were written by 10 different Nobel Laureates with whom he cooperated on his past industry sponsored projects. He published 40 books (mostly in the USA), he has over 100 papers in SCI journals (mostly in IEEE and ACM journals), and he presented invited talks at over 400 destinations worldwide. He has well over 1000 Thomson-Reuters WoS citations, well over 1000 Elsevier SCOPUS citations, and about 4000 Google Scholar citations. His Google Scholar h index is equal to 36. He is a Life Fellow of the IEEE since 2003 and a Member of The Academy of Europe since 2011. He is a member of the Serbian National Academy of Engineering and a Foreign Member of the Montenegro National Academy of Sciences and Arts.
Jean-Marie Lehn
University of Strasbourg, France
Jean-Marie Lehn received the Nobel Prize in Chemistry in 1987, together with Donald Cram and Charles Pedersen for his synthesis of cryptands. Lehn was an early innovator in the field of supermolecular chemistry, i.e. the chemistry of host-guest molecular assemblies created by intermolecular interactions, and continues to innovate in this field.
Konstantin Novoselov
University of Manchester, UK
Konstantin Novoselov received the Nobel Prize in Physics in 2010, together with Andre Geim, for their groundbreaking experiments on graphene, a two-dimensional material with remarkable properties such as high electrical conductivity, mechanical strength, and transparency. Novoselov and Geim's discovery of graphene was published in 2004, and it opened up new possibilities for the development of innovative technologies in various fields.
Dan Shechtman
Technion, Israel
Dan Shechtman received the Nobel Prize in Chemistry in 2011, for the discovery of quasicrystals, a type of solid material with a highly ordered structure that was previously thought to be impossible. However, further investigation revealed that the pattern was due to a new type of crystal structure, which did not fit the conventional rules of crystallography. This discovery eventually led to a new field of research in materials science.
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/cip-research):
Computationally Intensive Problems in General Math and Engineering: Basic Research and Development | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Computationally Intensive Problems in General Math and Engineering: Basic Research and Development
This two-part special issue for the
Journal of Big Data
covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
This special issue is separated into two distinct collections: part one covers basic research and development for problems in general engineering, and part two covers applicative research and education.
View part two here.
Collection Articles
About the editor
Featured Nobel Laureate Contributors
Automated segmentation of choroidal neovascularization on optical coherence tomography angiography images of neovascular age-related macular degeneration patients based on deep learning
Optical coherence tomography angiography (OCTA) has been a frequently used diagnostic method in neovascular age-related macular degeneration (nAMD) because it is non-invasive and provides a comprehensive view ...
Authors:
Wei Feng, Meihan Duan, Bingjie Wang, Yu Du, Yiran Zhao, Bin Wang, Lin Zhao, Zongyuan Ge and Yuntao Hu
Citation:
Journal of Big Data
2023
10
:111
Content type:
Research
Published on:
23 June 2023
View
Full Text
View
PDF
An approach to automatic classification of hate speech in sports domain on social media
Hate Speech encompasses different forms of trolling, bullying, harassment, and threats directed against specific individuals or groups. This phenomena is mainly expressed on Social Networks. For sports players...
Authors:
Staša Vujičić Stanković and Miljana Mladenović
Citation:
Journal of Big Data
2023
10
:109
Content type:
Research
Published on:
22 June 2023
View
Full Text
View
PDF
A new deep learning architecture with inductive bias balance for transformer oil temperature forecasting
Ensuring the optimal performance of power transformers is a laborious task in which the insulation system plays a vital role in decreasing their deterioration. The insulation system uses insulating oil to cont...
Authors:
Manuel J. Jiménez-Navarro, María Martínez-Ballesteros, Francisco Martínez-Álvarez and Gualberto Asencio-Cortés
Citation:
Journal of Big Data
2023
10
:80
Content type:
Methodology
Published on:
28 May 2023
View
Full Text
View
PDF
Gaussian transformation enhanced semi-supervised learning for sleep stage classification
Sleep disorders are significant health concerns affecting a large population. Related clinical studies face the deficiency in sleep data and challenges in data analysis, which requires enormous human expertise...
Authors:
Yifan Guo, Helen X. Mao, Jijun Yin and Zhi-Hong Mao
Citation:
Journal of Big Data
2023
10
:79
Content type:
Methodology
Published on:
27 May 2023
View
Full Text
View
PDF
Main memory controller with multiple media technologies for big data workloads
SRAM and DRAM memory technologies have been dominant in the implementations of memory subsystems. In recent years, and mainly driven by the huge memory demands of big data applications, NVRAM technology has em...
Authors:
Miguel A. Avargues, Manel Lurbe, Salvador Petit, Maria E. Gomez, Rui Yang, Xiaoping Zhu, Guanhao Wang and Julio Sahuquillo
Citation:
Journal of Big Data
2023
10
:75
Content type:
Research
Published on:
22 May 2023
View
Full Text
View
PDF
Research in computing-intensive simulations for nature-oriented civil-engineering and related scientific fields, using machine learning and big data: an overview of open problems
This article presents a taxonomy and represents a repository of open problems in computing for numerically and logically intensive problems in a number of disciplines that have to synergize for the best perfor...
Authors:
Zoran Babović, Branislav Bajat, Vladan Đokić, Filip Đorđević, Dražen Drašković, Nenad Filipović, Borko Furht, Nikola Gačić, Igor Ikodinović, Marija Ilić, Ayhan Irfanoglu, Branislav Jelenković, Aleksandar Kartelj, Gerhard Klimeck, Nenad Korolija, Miloš Kotlar…
Citation:
Journal of Big Data
2023
10
:73
Content type:
Research
Published on:
22 May 2023
View
Full Text
View
PDF
RILS-ROLS: robust symbolic regression via iterated local search and ordinary least squares
In this paper, we solve the well-known symbolic regression problem that has been intensively studied and has a wide range of applications. To solve it, we propose an efficient metaheuristic-based approach, cal...
Authors:
Aleksandar Kartelj and Marko Djukanović
Citation:
Journal of Big Data
2023
10
:71
Content type:
Research
Published on:
22 May 2023
View
Full Text
View
PDF
Practical ANN prediction models for the axial capacity of square CFST columns
In this study, two machine-learning algorithms based on the artificial neural network (ANN) model are proposed to estimate the ultimate compressive strength of square concrete-filled steel tubular columns. The...
Authors:
Filip Đorđević and Svetlana M. Kostić
Citation:
Journal of Big Data
2023
10
:67
Content type:
Research
Published on:
17 May 2023
View
Full Text
View
PDF
Unsupervised outlier detection for time-series data of indoor air quality using LSTM autoencoder with ensemble method
The proposed framework consists of three modules as an outlier detection method for indoor air quality data. We first use a long short-term memory autoencoder (LSTM-AE) based reconstruction error detector, whi...
Authors:
Junhyeok Park, Youngsuk Seo and Jaehyuk Cho
Citation:
Journal of Big Data
2023
10
:66
Content type:
Research
Published on:
17 May 2023
View
Full Text
View
PDF
Scenic routing navigation using property valuation
Extensive prior work has provided methods for the optimization of routing based on weights assigned to travel duration, and/or travel cost, and/or the distance traveled. Routing can be in various modalities, s...
Authors:
Naphtali Rishe, M. Hadi Amini and Malek Adjouadi
Citation:
Journal of Big Data
2023
10
:57
Content type:
Methodology
Published on:
4 May 2023
View
Full Text
View
PDF
Governance and sustainability of distributed continuum systems: a big data approach
Distributed computing continuum systems (DCCS) make use of a vast number of computing devices to process data generated by edge devices such as the Internet of Things and sensor nodes. Besides performing compu...
Authors:
Praveen Kumar Donta, Boris Sedlak, Victor Casamayor Pujol and Schahram Dustdar
Citation:
Journal of Big Data
2023
10
:53
Content type:
Methodology
Published on:
28 April 2023
View
Full Text
View
PDF
A practical Alzheimer’s disease classifier via brain imaging-based deep learning on 85,721 samples
Beyond detecting brain lesions or tumors, comparatively little success has been attained in identifying brain disorders such as Alzheimer’s disease (AD), based on magnetic resonance imaging (MRI). Many machine...
Authors:
Bin Lu, Hui-Xian Li, Zhi-Kai Chang, Le Li, Ning-Xuan Chen, Zhi-Chen Zhu, Hui-Xia Zhou, Xue-Ying Li, Yu-Wei Wang, Shi-Xian Cui, Zhao-Yu Deng, Zhen Fan, Hong Yang, Xiao Chen, Paul M. Thompson, Francisco Xavier Castellanos…
Citation:
Journal of Big Data
2022
9
:101
Content type:
Research
Published on:
13 October 2022
View
Full Text
View
PDF
Prof. Veljko Milutinovic
Fellow of the IEEE and of the Academy of Europe, Indiana University, Bloomington, IND, USA, Adjunct Professor, University of Belgrade, SRB, EUR, Visiting Professor
Prof. Veljko Milutinovic (1951) received his PhD from the University of Belgrade in Serbia, spent about a decade on various faculty positions in the USA (mostly at Purdue University and more recently at the University of Indiana in Bloomington), and was a co-designer of the DARPAs pioneering GaAs RISC microprocessor on 200MHz (about a decade before the first commercial effort on that same speed) and was a co-designer also of the related GaAs Systolic Array (with 4096 GaAs microprocessors). Later, for almost three decades, he taught and conducted research at the University of Belgrade in Serbia, for departments of EE, MATH, BA, and PHYS/CHEM. His research is mostly in datamining algorithms and dataflow computing, with the emphasis on mapping of data analytics algorithms onto fast energy efficient architectures. Most of his research was done in cooperation with industry (Intel, Fairchild, Honeywell, Maxeler, HP, IBM, NCR, RCA, etc... ). For 10 of his books, forewords were written by 10 different Nobel Laureates with whom he cooperated on his past industry sponsored projects. He published 40 books (mostly in the USA), he has over 100 papers in SCI journals (mostly in IEEE and ACM journals), and he presented invited talks at over 400 destinations worldwide. He has well over 1000 Thomson-Reuters WoS citations, well over 1000 Elsevier SCOPUS citations, and about 4000 Google Scholar citations. His Google Scholar h index is equal to 36. He is a Life Fellow of the IEEE since 2003 and a Member of The Academy of Europe since 2011. He is a member of the Serbian National Academy of Engineering and a Foreign Member of the Montenegro National Academy of Sciences and Arts.
Jean-Marie Lehn
University of Strasbourg, France
Jean-Marie Lehn received the Nobel Prize in Chemistry in 1987, together with Donald Cram and Charles Pedersen for his synthesis of cryptands. Lehn was an early innovator in the field of supermolecular chemistry, i.e. the chemistry of host-guest molecular assemblies created by intermolecular interactions, and continues to innovate in this field.
Konstantin Novoselov
University of Manchester, UK
Konstantin Novoselov received the Nobel Prize in Physics in 2010, together with Andre Geim, for their groundbreaking experiments on graphene, a two-dimensional material with remarkable properties such as high electrical conductivity, mechanical strength, and transparency. Novoselov and Geim's discovery of graphene was published in 2004, and it opened up new possibilities for the development of innovative technologies in various fields.
Dan Shechtman
Technion, Israel
Dan Shechtman received the Nobel Prize in Chemistry in 2011, for the discovery of quasicrystals, a type of solid material with a highly ordered structure that was previously thought to be impossible. However, further investigation revealed that the pattern was due to a new type of crystal structure, which did not fit the conventional rules of crystallography. This discovery eventually led to a new field of research in materials science.
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/collections):
Collections | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Articles
Collections
Collections
2024
Special Issue
Emergent architectures and technologies for big data management and analysis
Edited by: Robert Wrembel, Andrea Kő, Philippe Cudré-Mauroux
Big Data and Artificial Intelligence in Emerging Scientific Field
Edited by: Veljko Milutinovic
Big Data and Artificial Intelligence in Emerging Engineering Fields
Edited by: Veljko Milutinovic
2023
Special Issue
Green and Sustainable AI
Edited by: Paolo Trunfio, Loris Belcastro, Themis Palpanas
Big Data in Human Behaviour Research: A contextual turn?
Edited by: Jun Liu, Xianwen Kuang, Simon Schweighofer
Computationally Intensive Problems in General Math and Engineering
Edited by: Veljko Milutinovic
Special Issue
Advanced Bio-Inspired Deep Learning Algorithms for Multi-Modal Perceptual Big Data Analysis in the Car-Driver Assistance Systems
Edited by: Eng. Francesco Rundo, Sabrina Conoci, Sebastiano Battiato, Arcangelo Merla
2018
Special Issue
Advanced Soft Computing Methodologies and Applications in Social Media Big Data Analytics
Edited by: Zhiyong Z. Zhang, Kun Hua, Arun Kumar Sangaiah
First published: 25 January 2018
2015
Special Issue
Knowledge-Based Big Data Management in Cloud Computing Environments
Edited by: Zongmin Ma, Fu Zhang, Wen-Chen Hu
First published: 16 July 2015
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/special-issue-guidelines):
Special Issue Proposal Guidelines | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Special Issue Proposal Guidelines
The Journal of Big Data welcomes Special Issues (SI) on timely topics related to the ﬁeld at large.
The objective of Special Issues is to bring together recent and high-quality works in a research
domain, to promote key advances in speciﬁc research areas covered by the journal, and to provide
overviews of the state-of-the-art in emerging domains.
Preparing a proposal
All Guest Editors who wish to organize a Special Issue must send a proposal to Drs. Borivoje Furht
bfurht@fau.edu
and Taghi
Khoshgoftaar
khoshgof@fau.edu
with the following
requirements:
• Title
• Short description
• List of topics of interest
• A few sentences explaining the importance of the topic and relevance to the journal’s
aims
and scope
• Manuscript submission deadline
• Guest Editor details: Name, Email, Affiliation, Bio and Short CV (incl. list of at most 5
publications related to the SI proposal and links to their institutional webpages)
• Nomination of the Lead Guest Editor
• Draft call for papers (if applicable)
General Notes
This Journal adheres to the standard
Peer Review Policy, Process and Guidance
as outlined by
Springer under
Editorial Policies
.
After acceptance of the proposed topic, Guest Editors will manage the peer review process of the
special issue and must obtain a minimum of 2 reviews for each paper. Guest Editors should be well
established experts in the domain of the topic or closely related ﬁelds. The Editors-in-Chief are
responsible for the ﬁnal content published in the journal.
We require our Guest Editors to familiarize themselves with the editorial and publication policies of
the journal and our Springer Nature Code of Conduct before they undertake their SI. Please ﬁnd
links to these below:
Submission guidelines
JoBD Peer-Review Policy
Code of Conduct
For SIs originating from conferences or workshops, papers are expected to be developed and
extended by 60% with new material. The title and abstract should be updated.
A complete SI will contain a minimum of 5 published articles.
Article Processing Charges
Authors who publish open access in Journal of Big Data are required to pay an article processing
charge (APC). The APC price will be determined from the date on which the article is accepted for
publication. Current APC information is available
here
.
Visit our
open access support portal
, our
Journal Pricing FAQs
and
open access funding & support
services
for further information.
Open Access Funding
Springer Nature offers agreements that enable institutions to cover open access publishing costs.
Authors can learn more about our
open access agreements
to check their eligibility and discover
whether this journal is included.
Springer Nature offers APC waivers and discounts for articles published in our fully open access
journals whose corresponding authors are based in the world’s lowest income countries (see
our
APC waivers and discounts policy
for further information). Requests for APC waivers and
discounts from other authors will be considered on a case-by-case basis, and may be granted in
cases of ﬁnancial need (see our
open access policies for journals
for more information). All
applications for discretionary APC waivers and discounts should be made by authors at the point of
manuscript submission within the submission system. Requests made during the review process or
after acceptance cannot be considered.
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/#tab-1):
Home page | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Featured Collections on Computationally Intensive Problems in General Math and Engineering
This two-part special issue covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
Open Special Issues
Customization and fine-tuning of machine learning models
Submission Deadline: 15 December 2024
Advancements on Automated Data Platform Management, Orchestration, and Optimization
Submission Deadline: 30 September 2024
Emergent architectures and technologies for big data management and analysis
Submission Deadline: 1 October 2024
View our collection of open and closed special issues
Read our guidelines for special issue proposals
here
.
Articles
Recent
Most accessed
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Content type:
Research
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Content type:
Research
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Content type:
Research
18 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Content type:
Research
17 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Content type:
Research
12 September 2024
Most recent articles RSS
View all articles
A survey on Image Data Augmentation for Deep Learning
Authors:
Connor Shorten and Taghi M. Khoshgoftaar
Content type:
Survey paper
6 July 2019
Big data in healthcare: management, analysis and future prospects
Authors:
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
Content type:
Survey paper
19 June 2019
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Authors:
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
Content type:
Survey Paper
31 March 2021
Deep learning applications and challenges in big data analytics
Authors:
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
Content type:
Research
24 February 2015
Short-term stock market price trend prediction using a comprehensive deep learning system
Authors:
Jingyi Shen and M. Omair Shafiq
Content type:
Research
28 August 2020
Most accessed articles RSS
View all articles
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Top 10 Most Cited Articles 2023
A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications
Alzubaidi L., Bai J., Al-Sabaawi A. et al., (2023)
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset
Yin Y., Jang-Jaccard J., Xu W. et al., (2023)
A deep learning-based model using hybrid feature extraction approach for consumer sentiment analysis
Gagandeep Kaur, Amit Sharma (2023)
Skin-Net: a novel deep residual network for skin lesions classification using multilevel feature extraction and cross-channel correlation with detection of outlier
Yousef S. Alsahafi, Mohamed A. Kassem, Khalid M. Hosny
Read the rest of the list
here
.
Latest Tweets
View Twitter timeline
Your browser needs to have JavaScript enabled to view this timeline
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Advertisement
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/#tab-2):
Home page | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Featured Collections on Computationally Intensive Problems in General Math and Engineering
This two-part special issue covers computationally intensive problems in engineering and focuses on mathematical mechanisms of interest for emerging problems such as Partial Difference Equations, Tensor Calculus, Mathematical Logic, and Algorithmic Enhancements based on Artificial Intelligence. Applications of the research highlighted in the collection include, but are not limited to: Earthquake Engineering, Spatial Data Analysis, Geo Computation, Geophysics, Genomics and Simulations for Nature Based Construction, and Aerospace Engineering. Featured lead articles are co-authored by three esteemed Nobel laureates: Jean-Marie Lehn, Konstantin Novoselov, and Dan Shechtman.
Open Special Issues
Customization and fine-tuning of machine learning models
Submission Deadline: 15 December 2024
Advancements on Automated Data Platform Management, Orchestration, and Optimization
Submission Deadline: 30 September 2024
Emergent architectures and technologies for big data management and analysis
Submission Deadline: 1 October 2024
View our collection of open and closed special issues
Read our guidelines for special issue proposals
here
.
Articles
Recent
Most accessed
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Content type:
Research
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Content type:
Research
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Content type:
Research
18 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Content type:
Research
17 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Content type:
Research
12 September 2024
Most recent articles RSS
View all articles
A survey on Image Data Augmentation for Deep Learning
Authors:
Connor Shorten and Taghi M. Khoshgoftaar
Content type:
Survey paper
6 July 2019
Big data in healthcare: management, analysis and future prospects
Authors:
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
Content type:
Survey paper
19 June 2019
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Authors:
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
Content type:
Survey Paper
31 March 2021
Deep learning applications and challenges in big data analytics
Authors:
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
Content type:
Research
24 February 2015
Short-term stock market price trend prediction using a comprehensive deep learning system
Authors:
Jingyi Shen and M. Omair Shafiq
Content type:
Research
28 August 2020
Most accessed articles RSS
View all articles
Aims and scope
The
Journal of Big Data
publishes open-access original research on data science and data analytics. Deep learning algorithms and all applications of big data are welcomed. Survey papers and case studies are also considered.
The journal examines the challenges facing big data today and going forward including, but not limited to: data capture and storage; search, sharing, and analytics; big data technologies; data visualization; architectures for massively parallel processing; data mining tools and techniques; machine learning algorithms for big data; cloud computing platforms; distributed file systems and databases; and scalable storage systems. Academic researchers and practitioners will find the
Journal of Big Data
to be a seminal source of innovative material.
Top 10 Most Cited Articles 2023
A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications
Alzubaidi L., Bai J., Al-Sabaawi A. et al., (2023)
IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset
Yin Y., Jang-Jaccard J., Xu W. et al., (2023)
A deep learning-based model using hybrid feature extraction approach for consumer sentiment analysis
Gagandeep Kaur, Amit Sharma (2023)
Skin-Net: a novel deep residual network for skin lesions classification using multilevel feature extraction and cross-channel correlation with detection of outlier
Yousef S. Alsahafi, Mohamed A. Kassem, Khalid M. Hosny
Read the rest of the list
here
.
Latest Tweets
View Twitter timeline
Your browser needs to have JavaScript enabled to view this timeline
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Advertisement
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00985-8):
Optimizing poultry audio signal classification with deep learning and burn layer fusion | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
18 September 2024
Optimizing poultry audio signal classification with deep learning and burn layer fusion
Esraa Hassan
1
,
Samar Elbedwehy
2
,
Mahmoud Y. Shams
1
,
Tarek Abd El-Hafeez
3
,
4
&
…
Nora El-Rashidy
1
Show authors
Journal of Big Data
volume
11
, Article number:
135
(
2024
)
Cite this article
57
Accesses
Metrics
details
Abstract
This study introduces a novel deep learning-based approach for classifying poultry audio signals, incorporating a custom Burn Layer to enhance model robustness. The methodology integrates digital audio signal processing, convolutional neural networks (CNNs), and the innovative Burn Layer, which injects controlled random noise during training to reinforce the model's resilience to input signal variations. The proposed architecture is streamlined, with convolutional blocks, densely connected layers, dropout, and an additional Burn Layer to fortify robustness. The model demonstrates efficiency by reducing trainable parameters to 191,235, compared to traditional architectures with over 1.7 million parameters. The proposed model utilizes a Burn Layer with burn intensity as a parameter and an Adamax optimizer to optimize and address the overfitting problem. Thorough evaluation using six standard classification metrics showcases the model's superior performance, achieving exceptional sensitivity (96.77%), specificity (100.00%), precision (100.00%), negative predictive value (NPV) (95.00%), accuracy (98.55%), F1 score (98.36%), and Matthew’s correlation coefficient (MCC) (95.88%). This research contributes valuable insights into the fields of audio signal processing, animal health monitoring, and robust deep-learning classification systems. The proposed model presents a systematic approach for developing and evaluating a deep learning-based poultry audio classification system. It processes raw audio data and labels to generate digital representations, utilizes a Burn Layer for training variability, and constructs a CNN model with convolutional blocks, pooling, and dense layers. The model is optimized using the Adamax algorithm and trained with data augmentation and early-stopping techniques. Rigorous assessment on a test dataset using standard metrics demonstrates the model's robustness and efficiency, with the potential to significantly advance animal health monitoring and disease detection through audio signal analysis.
Introduction
Recent advances in technology enable the exploitation of animals' acoustic communication for automatic health monitoring, offering real-time, objective, and cost-effective alternatives to manual inspection methods [
1
,
2
,
3
].
Despite the increasing interest in exploiting animal vocalizations for automated health monitoring purposes, few openly available datasets support the development of such systems [
4
]. Specifically, for poultry production, only limited resources documenting healthy versus diseased states' vocal patterns are public [
5
,
6
,
7
]. This study utilizes a labeled audio dataset of healthy and unhealthy chicken vocalizations from various African poultry breeds. The dataset contains 346 WAV recordings categorized as healthy, noisy, or unhealthy. Each category contains 139, 86, and 121 instances, respectively, lasting between 5 and 60 s. Selected sound segments in the unhealthy folder include chicken cough, snoring, and rale sounds, representing symptoms of respiratory distress. At the same time, the Noise folder contains background noises and poultry bird activities such as feeding and pecking one another.
Analyzing animal vocalizations involves extracting relevant features from raw audio data. Various representations capture different aspects of audio signals, e.g., Mel-frequency cepstral coefficients (MFCC), chromograms, and autocorrelations. Subsequent processing typically entails dimensionality reduction and normalization techniques to feed downstream machine learning algorithms [
8
].
Deep neural networks constitute popular choices for pattern recognition problems requiring end-to-end mapping between raw sensor data and desired outputs [
9
]. Several architectures achieve excellent performances on various image, video, audio, and text modalities [
10
,
11
]. Among those, ResNet (short for residual network) gained popularity due to its skip connections mitigating vanishing gradient issues encountered during optimization [
12
]. Extensions of plain ResNets include bottleneck structures and dilated convolutions, boosting representational capacities while controlling computational costs [
13
].
However, despite their proven abilities, deep learning models remain sensitive to hyperparameter tuning and prone to memorizing random artifacts rather than genuine underlying patterns [
14
,
15
]. Regularization schemes counteract overfitting tendencies, inducing prior distributions over model parameters and introducing stochasticity during optimization [
16
]. Examples include weight decay, dropouts, and adversarial perturbations added to input data. Recently, the latter idea inspired the introduction of so-called "Burn-in" layers, injecting structured noise directly inside neural networks to encourage more stable training and enhanced generalization properties [
17
].
Considering the scarcity of openly available datasets depicting healthy vs. unhealthy chicken vocalizations, this study proposes an original architecture harnessing the above-mentioned techniques to discriminate between the considered classes [
18
]. Starting from raw waveform representations, mel spectrograms serve as intermediate visual descriptors capturing spectral patterns. Then, two parallel pipelines process local vs. global features separately [
19
]. Local details pass through consecutive convolutional layers, while global trends proceed straight to the final merge point. Before merging, both paths apply separate temporal pooling stages to reduce dimensionality. Lastly, fully connected layers supported by dropouts produce probabilistic estimates reflecting the likelihood of observing either healthy or unhealthy cases [
20
].
This research compares the proposed architecture to baselines established on plain ResNets and simple combinations of convolutional and recurrent modules. Quantitative assessments rely on sensitivity, specificity, positive/negative predictive values, accuracy, F1 scores, and Matthew’s correlation coefficient. Visual inspections complement numerical analyses by scrutinizing learning curves and plotting error evolution across iterations. Obtained results shed light on the relative strengths and weaknesses of competing designs, guiding future improvements in automatic poultry health monitoring systems.
Overall, this work addresses the pressing need for intelligent and autonomous monitoring systems in agriculture, particularly poultry farming. By capitalizing on readily deployable sensors collecting vast amounts of multimedia data, the envisaged framework shall alert farmers about anomalous situations threatening their businesses' sustainability. Therefore, immediate actions tackling emerging threats become possible, limiting financial damages and safeguarding food security.
Problem statement
Reliable and efficient monitoring of poultry health is crucial for the poultry industry. This study addresses the challenges in developing accurate and robust poultry sound classification models. Environmental factors can distort audio signals, making it difficult to extract relevant features for classification. Additionally, training stability issues in deep learning models can lead to unreliable predictions [
21
]. To enhance robustness, this study integrates a Burn Layer into ResNet-based architectures for improved training stability.
A dataset comprising 346 audio signal files is used, categorized as healthy, noisy, and unhealthy. Each file represents a distinct time frame, with carefully selected sound segments. The "noisy" category includes background noises from vehicles and human voices, as well as poultry bird activities. The "unhealthy" category contains sounds indicative of respiratory issues. All files are stored in.wav audio format [
18
].
Two architectures are implemented: a traditional architecture and a proposed work architecture. Both architectures feature convolutional and pooling layers, batch normalization, and activation functions. The proposed work architecture includes a Burn Layer, which adds Gaussian noise during training to improve robustness. It also incorporates a second input layer and a global average pooling layer.
The study evaluates the performance of these architectures to demonstrate the effectiveness of the Burn Layer in improving robustness and stability in poultry sound classification models. The aim is to establish a foundation for accurate and dependable poultry health monitoring systems, benefiting the poultry industry and animal welfare standards.
Research objective
This paper aims to enhance the robustness of deep learning models for classifying poultry multimedia data by introducing novel modifications to existing architectures. The Burn Layer introduces controlled noise during training, significantly improving the model's stability and generalization by exposing it to noisy inputs. This innovative technique sets it apart from traditional regularization methods and offers a novel solution to the challenges of training robust deep learning models. The proposed architecture not only reduces the number of trainable parameters to 191,235 but also achieves outstanding classification metrics. This approach has significant potential applications in animal health monitoring and disease detection through audio signal analysis.
Contributions
This paper proposes modifications to existing deep learning architectures for enhanced robustness in classifying poultry multimedia data. Specifically, it makes the following key contributions:
1.
Introduction of a Burn Layer that randomly perturbs input data during training. This improves model stability and generalization by exposing it to noisy inputs.
2.
Development of an end-to-end pipeline for audio-based poultry health status detection using deep learning. Previous work in this domain utilized traditional machine-learning techniques.
3.
Comparison of performance between a standard CNN, ResNet with Burn Layer, and the proposed model integrating Burn Layers. The proposed model achieves superior results with 98.55% accuracy.
4.
Detailed analysis of model optimization steps including data preprocessing, augmentation, training with early stopping, and learning rate scheduling.
5.
Evaluation of models on important metrics like sensitivity, specificity, precision, etc. to provide a holistic view of classification capability.
This paper advances the field of poultry disease detection by developing an accurate deep learning-based approach and customized architecture incorporating techniques to enhance robustness. The openly available dataset also enables further research.
The remainder of this paper is structured as follows: “
Related work
” section provides a review of related work. “
Preliminaries
” section outlines the preliminaries which include the most common methodologies Burn Layer, ResNet, ResNext, DenseNet, and Wide ResNet. “
Proposed work
” section presents the proposed work; the results and analysis of the proposed ensemble model are presented in “
Result and experimental
” section. Finally, “
Discussions and limitations
” section concludes the paper, highlighting future research directions.
Related work
Previous approaches
Monitoring poultry traits is crucial for assessing environmental health conditions and making informed decisions [
22
,
23
]. These aggregated pieces of information are utilized to ensure the welfare of poultry and make appropriate management choices [
24
]. Research focusing on changes in physiological traits can be employed to predict variations in vocalization patterns and detect various diseases [
25
,
26
]. Poultry, being a homeothermic animal, generates and disperses heat to maintain a constant body temperature [
18
,
27
]. Several methods have been developed to monitor the health status of poultry, one of which involves monitoring their body temperature [
18
,
29
]. Fluctuations in body temperature can indicate stress or pathological conditions. Therefore, temperature monitoring plays a significant role in determining the health status of poultry. Infrared thermoregulation (IRT) is a technique commonly used to measure poultry temperature [
28
,
29
]. Additionally, visual observation is employed to identify sick chickens [
27
,
30
,
31
,
32
,
33
,
34
]. Recent studies suggest that vocalization and abnormal sounds emitted by sick chickens can serve as significant indicators of their health status [
35
,
36
]. In one study, Qunitee et al. [
37
] developed a hybrid model for chicken monitoring using a decision tree (DT). The system was based on visual input from 15 chickens monitored over 72 h, achieving a classification accuracy of 84.8%. When audio inputs were considered, the classification accuracy improved to 86.1%. Another study [
38
] developed a detection system aiming to classify chick calls based on Deep Learning models. The study explored three different chick breeds, analyzing zero-crossing rate and short-time crossing rate to identify the endpoints of chick calls in the audio signals. The results showed that the ResNet model achieved the highest accuracy of 83%, while the gated recurrent network (GRU) achieved an accuracy of 90%.
Challenges
Despite these advancements, several challenges persist in effectively monitoring poultry health through audio signals. The complexity of accurately detecting and classifying various health indicators such as stress, diseases, and other pathological conditions through sound remains high. Noise in the environment often interferes with the accuracy of the models, making it difficult to distinguish between normal and abnormal sounds. Additionally, the diversity in vocalization patterns among different breeds and individual chickens adds another layer of complexity. This necessitates the development of sophisticated models that can generalize well across different conditions and environments.
Proposed solutions
To address these challenges, various innovative approaches have been proposed. For instance, a study [
39
] utilized a ResNet model to classify Newcastle disease among poultry, which affects both health and production. The study utilized audio signals from 35 chickens and implemented multi-window spectral filtering and high-filtering techniques to reduce the impact of noise. The processed model achieved an average accuracy of 91.06% for infected and healthy chicken classes. In another study [
40
], an audio-based system was developed to detect chicken stress during their first weeks of life. The system monitored the birds' sounds, identified stress, and improved any conditions that may have arisen. The study concluded that pre-recorded audio signals could be used with different classifiers and at different frame levels. Using four classifiers at a 1000 ms frame level, accuracies ranged from 63 to 83%. Additionally, authors in [
41
] proposed an audio-based system to detect various types of vocalization in chickens, including chirping, peeping, and begging. The system analyzed the audio signals, applied feature engineering, and utilized joint-time–frequency scattering (JTFS) for feature extraction to accurately identify the different types of vocalizations. Another innovative solution [
42
] involved developing a system capable of detecting chicken sneezing in noisy environments. The researchers built a model based on audio data for sneeze and non-sneeze classification, aggregating 763 audio segments from 51 chickens. The system achieved a performance of 88.4% and 66.7% in terms of sensitivity and precision, respectively. Furthermore, a study focused on the early detection of influenza in chickens by analyzing audio signals and extracting sound features using Mel-frequency cepstral coefficients (MFCC) to classify healthy and infected chickens [
27
]. The model achieved accuracy ranging from 84 to 90%. These studies collectively demonstrate the potential of audio signals to provide significant indications about the health status of poultry. Table
1
provides a summary of the current related work.
Table 1 Summarization of literature
Full size table
Preliminaries
The burn layer
The Burn Layer acts as a custom layer in a neural network that introduces a form of noise during training [
44
]. Initialization involves setting parameters, and controlling the intensity of the noise introduced by the layer, to a default value of 0.2 [
45
]. During the forward pass, when the layer is called with inputs, it checks if the model is in training mode [
46
,
47
,
48
]. By adding a special layer that enforces a selective burning process during training. The idea is to make the model more robust by training it on slightly perturbed data, akin to dynamic data augmentation. The burn intensity parameter allows control over the strength of this effect.
To represent the Burn Layer mathematically, given an input tensor
X
of shape
(N, T, C)
, the Burn Layer operation can be expressed as in Eq. (
1
)
$$Burn\,Layer\left( X \right) \, = \, X \, + \, burn\_intensity \, \times \, Z$$
(1)
where
Z
is a tensor of the same shape as
X
, containing random noise sampled from a normal distribution
N (0,1)
and scaled by the burn intensity. This operation adds random noise to the input tensor
X
, scaled by the burn intensity. The outputs include a digital audio signal represented as a sequence of samples. Steps involve calculating the total number of samples, initializing the digital audio signal array, sampling the continuous audio signal, quantizing the sampled values, normalizing the digital audio signal by converting quantized samples to a suitable digital representation (e.g., 16-bit integer), and scaling the values to fit within the dynamic range of the chosen representation. Finally, the digital audio signal array
D
is returned [
45
]. The mathematical formulation of the Burn Layer operates as follows:
i)
For a given layer L in the neural network, let h
L
represent the output of the neurons in that layer. The Burn Layer modifies these outputs as in Eq. (
2
).
$${\text{h}}_{\text{L}}^{\prime} = h_{L{ }} \odot { }m_L$$
(2)
where
\({m}_{L}\)
is a mask vector of the same dimension as
\({h}_{L}\)
, and
⊙
denotes the element-wise multiplication.
ii)
The mask
m
L
is updated based on a burning function
f(
h
L
)
that determines the likelihood of each neuron being burned as shown in Eq. (
3
).
$${m}_{L}={m}_{L}\odot Burn({h}_{L }, \theta )$$
(3)
where
\(Burn({h}_{L}, \theta )\)
is a function parameterized by
\(\theta\)
that progressively zeros out neurons as in Eq. (
4
).
$$Burn\left( {h_{L} ,\theta } \right) = \left\{ {\begin{array}{*{20}c} 0 & {if\left| {h_{L} } \right| < 0} \\ 1 & {otherwise} \\ \end{array} } \right.{\text{ }}$$
(4)
where
\(\theta\)
is a threshold parameter that determines the sensitivity of the burning process.
iii)
The overall loss function
\(\mathcal{L}\)
of the neural network might also incorporate a burning penalty to promote the burning of non-essential neurons as in Eq. (
5
).
$$\mathcal{L}={\mathcal{L}}_{task}+\lambda \sum_{L}\left||m\right||$$
(5)
where
\({\mathcal{L}}_{task}\)
is the original task-cross-entropy for classification), and λ is a regularization parameter controlling the strength of the burning penalty [
49
].
The comparison between the Burn Layer with traditional regularization techniques is investigated as follows:
i)
Dropout
: Dropout randomly sets a fraction of input units to zero during training, effectively reducing the network capacity and preventing overfitting. The Burn Layer, on the other hand, adds controlled noise to the input, maintaining the network's full capacity but training it on slightly perturbed data as in Eq. (
6
).
$${\text{h}}_{\text{L}}^{\prime} = h_{L{ }} \odot { }d_L$$
(6)
where
\({d}_{L}\)
is a binary mask with each element being zero with probability p. Dropout aims to prevent co-adaptation of neurons by randomly omitting them during training. In contrast, the Burn Layer deterministically removes neurons based on their utility.
ii)
Weight Decay (L2 Regularization)
: This technique penalizes large weights by adding a regularization term to the loss function as shown in Eq. (
7
).
$$\mathcal{L}={\mathcal{L}}_{task}+\lambda \sum_{w}{w}^{2}$$
(7)
The Burn Layer does not directly influence the weights but rather the input data, ensuring the model learns to be robust to variations.
iii)
Data Augmentation
: Traditional data augmentation techniques create multiple modified copies of the training data. The Burn Layer dynamically perturbs the data during training, providing a similar effect but without the need for explicitly generating augmented datasets.
iv)
Batch Normalization
: Batch normalization standardizes the outputs of neurons to have zero mean and unit variance, followed by a learnable scaling and.
v)
shifting as shown in Eq. (
8
).
$$h_L^{\prime} = \gamma \frac{{h_L - {\upmu }}}{\sigma }h_{L{ }} + {\upbeta }$$
(8)
where
\(\upmu\)
and
\(\sigma\)
are the mean and standard deviation of the batch,
\(\gamma\)
and
\(\upbeta\)
are learnable parameters. Batch normalization primarily addresses internal covariate shift and accelerates training, while the Burn Layer focuses on pruning non-contributive neurons [
50
].
The ResNet model
Residual Network (ResNet) is a deep learning model tailored for computer vision tasks, particularly designed to accommodate hundreds or thousands of convolutional layers [
12
]. Traditional CNN architectures struggled to scale to such depths due to the "vanishing gradient" issue, where gradients diminish with increasing layer depth, leading to suboptimal performance [
51
]. ResNet addresses this by introducing "skip connections," which allow the reuse of previous layer activations, thus mitigating the vanishing gradient problem [
52
].
By stacking multiple identity mappings and skipping layers during initial training, ResNet compresses the network into fewer layers, accelerating training [
53
]. Subsequently, during retraining, the skipped layers are expanded, enabling the network to explore more complex features of the input image. ResNet models typically skip two or three layers at a time, incorporating nonlinearity and batch normalization between them. Advanced versions, like HighwayNets, can dynamically determine skip weights to further optimize performance. Residual blocks form the core of the ResNet architecture, differing from older architectures like VGG16 [
54
], which relied on stacking convolutional layers with batch normalization and nonlinear activation layers [
55
]. While effective for a limited number of layers, subsequent research revealed the potential for improved performance with increased layer depth. ResNet's simple yet effective approach of adding intermediate inputs to convolution blocks allows for deeper exploration of feature spaces, making it a powerful tool for computer vision tasks [
56
].
Figure
1
illustrates a typical residual block, which can be represented in Python code as output equal to
F(x)
+
x
, where
x
is the input to the block and the output from the previous layer, and
F(x)
represents the operations within the residual block. This technique, known as skip connections, facilitates smoother gradient flow during backpropagation, enabling networks to scale to significant depths, such as 50, 100, or even 150 layers, without suffering from the vanishing gradient problem. Importantly, skipping connections incurs no additional computational overhead [
57
]. This approach has gained widespread popularity and has been adopted in various neural network architectures beyond CNNs, including UNet and Recurrent Neural Networks (RNNs) [
58
].
Fig. 1
The building block of the residual learning of ResNet architecture
Full size image
Optimization algorithms with CNN
Sound recognition, a cornerstone in audio processing, encompasses tasks like audio classification and sound event detection [
59
]. Comprising various layers, each assumes a distinct role in feature extraction and learning from input audio data [
60
]. Convolutional layers lie at the heart of the architecture, applying filters to detect patterns. Activation functions introduce non-linearities, while pooling layers reduce spatial dimensions, preserving crucial features. Fully connected layers process high-level features, and dropout mitigates overfitting by randomly deactivating neurons [
61
].
Optimization algorithms, such as Adam [
62
], Nadam [
63
], and Adamax [
63
], drive the training of sound recognition models. Adam, an amalgamation of AdaGrad [
64
]and RMSProp [
65
], adapts learning rates and maintains moving averages of gradients. Nadam integrates Nesterov's accelerated gradient, optimizing convergence [
11
]. Adamax, a streamlined variant of Adam, employs the max norm of gradients. These algorithms dynamically adjust learning rates, facilitating efficient parameter updates to minimize loss and enhance the performance of sound recognition systems.
Dataset description
The dataset was collected from poultry birds that were purchased for 100 days. These birds were divided into groups at a research farm located at Bowen University. The birds have respiratory diseases that have many symptoms including cough, rale, and snoring. Some groups of birds received treatments while others did not. Afterward, the birds were separated and monitored in isolated environments [
18
]. To minimize noise interference, the microphones were placed away from the birds. Sound segments were recorded using 24-bit samples at a sampling rate of 96 kHz for 65 days.
The dataset consists of 346 audio signals, which are categorized into three folders: "noisy," "healthy," and "unhealthy." The "healthy" folder contains 139 files, the "noisy" folder contains 86 audio files, and the "unhealthy" folder contains 121 audio files. The length of each audio signal ranges from 5 to 60 units. The selected noise segments include the sounds of moving vehicles, human voices, and other background noises. The selected segments in the "unhealthy" folder consist of cough, rale, and snore sounds. All files are stored in the.wav format [
18
] found in the following Mendeley link "
https://data.mendeley.com/datasets/zp4nf2dxbh/1
".
To analyze the audio signals, both frequency and time domain analyses were conducted. The power of the signals in the "healthy" and "unhealthy" categories was compared in the frequency domain, while the noise segments exhibited higher frequency content on the y-axis compared to the x-axis. The statistical analysis of the applied dataset including mean, standard deviation, skewness, kurtosis, median, range, interquartile range, and entropy are shown in Table
2
.
Table 2 The statistical analysis of the applied poultry bird's dataset
Full size table
To investigate further, the audio signal underwent analysis in both the time and frequency domains. Comparison was made between the power spectra of two signals, one representing healthy sounds and the other unhealthy sounds, within the frequency domain. In this comparison, it was observed that the normal sound exhibited a higher frequency content precisely around the y-axis. Conversely, the anomalous sound displayed a distinct spike of approximately 0.2 radians, indicating a noticeable deviation from the expected frequency distribution.
Proposed work
Figure
2
illustrates the overall architecture of the proposed model. The primary focus of this paper is on leveraging sound analysis algorithms for classifying chicken behavior within a farm environment. The sound signals undergo preprocessing, followed by feature extraction, and the extracted features are stored in a database [
66
]. Subsequently, these features are fed into a classifier, enabling the classification of sound signals into three categories: healthy, unhealthy, and noise. This approach aims to provide a systematic method for monitoring and assessing the well-being of chickens based on their vocalizations.
Fig. 2
The general structure of the proposed sound poultry recognition system
Full size image
The proposed model presents the key steps to develop and evaluate a deep learning-based model for poultry audio classification that incorporates a Burn Layer for improved robustness. It takes in the raw training, validation, and test audio data as well as labels and pre-processes the data to create digital representations. It first defines a Burn Layer custom layer that randomly perturbs the input during training with a specified burn intensity parameter. This layer is applied to the input to expose it to noisy variations of the training samples. It then builds the classification model, starting with convolutional blocks comprising Conv1D, batch normalization, and max pooling layers. These layers extract powerful audio features from the input. A fusion layer is created by concatenating global average pooling outputs. Further, dense and dropout layers are added for classification. Crucially, another Burn Layer is integrated after the first dense layer for additional robustness. It is compiled using Adamax optimizer, categorical cross-entropy loss and trained over multiple epochs with data augmentation and early stopping. During training, validation loss is monitored to retain the best-performing weights. Once training is complete, it evaluates the trained model on the held-out test set to analyze performance metrics like accuracy, sensitivity, specificity, etc. This helps gauge how effectively the model with Burn Layers can classify poultry health from audio recordings.
Figures
3
and
4
outlines the proposed Burn Layer model. Our proposed Burn Layer model involves defining a custom layer responsible for adding controlled random noise to input data during training, thus improving robustness against fluctuations in input signals. To prepare the digital audio signal, we convert the continuous audio signal into a digital representation by determining the number of samples, initiating a zero-valued array, sampling the signal at equal intervals, transforming analog values into digital ones, and scaling them accordingly.
Fig. 3
The Block diagram of the proposed model architecture
Full size image
Fig. 4
The Algorithm steps of the proposed model architecture
Full size image
For designing the audio model architecture, we define input tensors, connect the Burn Layer to the input tensor with a specified burn intensity, stack sequential convolutional blocks—each containing a Conv1D layer, Batch Normalization layer, and MaxPooling1D layer—and configure activations and L2 regularization. Global average pooling compresses feature maps along the temporal dimension, allowing us to merge the aggregated features with another globally pooled input using a concatenation layer. We append fully connected layers—featuring a dense layer followed by a dropout layer and another Burn Layer with decreased burn intensity—culminating in the output layer activated with softmax for multi-class classification. Configuring the model appropriately includes setting the optimizer (Adamax), loss function (sparse categorical cross-entropy), and learning rate (0.001). For training, we implement early stopping, store the best model checkpoint based on validation loss, fix the learning rate, generate synthetic training data through data augmentation, divide input data into batches, and fine-tune model parameters using backpropagation. Once the model is trained, measuring its performance relies on computing the loss value and metric evaluations on held-back testing data. Our Burn Layer model provides enhanced robustness compared to conventional methods in diverse applications.
Data preprocessing
The sound samples were framed and filtered as part of the sound signal preprocessing. Longer period nonstationary sound samples were then framed by a shifting Hamming window to create a 10 to 30 s stationary signal. Table
3
details the architecture of the traditional convolutional neural network model used for comparison in the paper. The table lists each layer of the network, the output shape at each step, and the number of trainable parameters. The network takes an input of shape (None, 13, 1) representing the audio samples. It then applies a series of 1D convolutional layers interspersed with batch normalization, activation, and max pooling layers to extract features from the input audio. Three convolutional blocks are used, each containing a convolutional layer, batch normalization, and max pooling. This is followed by the global average pooling of feature maps across time. A fusion layer is created by concatenating outputs from two global average pooling layers. Finally, the network contains dense layers for classification. In total, the traditional architecture comprises 45 layers with a trainable parameter. The table provides a detailed overview of the network architecture to facilitate results comparison with the proposed model.
Table 3 The traditional architecture with a detailed description
Full size table
Table
4
outlines the architecture of the proposed model in this study. It improves upon the traditional architecture by incorporating a Burn Layer to enhance robustness. The input is first passed through the Burn Layer, which randomly perturbs the data during training. Then, similar to Table
3
, it applies convolutional and max pooling layers in three blocks to extract features. A key difference is the introduction of a second input stream that is pooled separately and concatenated with the mainstream, forming a fusion layer. The network also contains dense layers for classification. Notably, another Burn Layer is added after the first dense layer. In total, the proposed model contains 19 layers with 191,235 trainable parameters—a more efficient architecture compared to Table
3
. The Burn Layers aim to improve model stability during training by exposing it to variations in input. This table provides details of the modified architecture designed to classify poultry audio with improved robustness.
Table 4 The proposed work architecture with a detailed description
Full size table
In Fig.
5
, we observe the distribution of channels across the layers of a convolutional neural network (CNN). The number of channels in each layer is not uniform and varies depending on the layer's position relative to the output layer. Specifically, the initial layer contains 1 channel, followed by 64 channels in the second layer, and 128 channels in the third layer. The quantity of channels within a layer correlates with the number of filters applied within that layer. Filters play a pivotal role in feature extraction from input images. Consequently, layers with a higher number of channels possess an enhanced capacity to extract a more diverse range of features from the input image.
Fig. 5
The number of channels in each layer of a convolutional neural network (CNN)
Full size image
Model evaluation
Following the completion of the training phase with an 80% training and 20% testing data split for our classification model, we meticulously evaluate its performance using established metrics. These measures encompass various facets of model performance in classification tasks. Accuracy gauges overall precision by comparing correctly predicted samples to the total, offering a broad measure of correctness. Precision focuses on the model's accuracy in identifying positive instances, calculated as the ratio of true positives to the total predicted positives. Recall (or sensitivity) evaluates the model's ability to capture positive instances by comparing true positives to the total actual positives. The F1 Score, a harmonic mean of precision and recall, provides a balanced assessment, crucial for scenarios with imbalanced class distributions. Additionally, loss, derived from an average cross-entropy error during training, indicates the model's ability to predict the correct class and guides optimization. AUC (Area Under the ROC Curve) measures the model's discriminative ability between classes, calculated based on the entire area under the curve plotting True Positive Rate vs. False Positive Rate. These metrics collectively offer a comprehensive view of a classification model's effectiveness, enabling assessment across various aspects of accuracy and predictive capability. Common techniques, outlined in references and Eqs. (
9
)–(
15
), are utilized for the computation of sensitivity, specificity, precision, Negative Predictive Value (NPV), accuracy, F1-score, and Matthews Correlation Coefficient (MCC), respectively. [
11
,
67
,
68
].
$$\text{Recall}=\text{S}ensitivity =\frac{\text{TP }}{\text{TP }+\text{ FN}}$$
(9)
$$\text{Specificity}=\frac{\text{TN }}{\text{TN }+\text{ FP}}$$
(10)
$$\text{Precision}=\frac{\text{TP }}{\text{TP }+\text{ FP}}$$
(11)
$${\text{Negative}}\,{\text{Predictive}}\,{\text{Value}} = \frac{{\text{TN }}}{{{\text{TN }} + {\text{ FN }}}}$$
(12)
$$\text{Accuracy}=\frac{\text{TP }+\text{ TN}}{\text{TP }+\text{ FP }+\text{ TN }+\text{ FN}}$$
(13)
$${\text{F1 - score }} = 2* \frac{{\left( {{\text{Precision }} \times {\text{ Recall}}} \right)}}{{\left( {{\text{Precision }} + {\text{ Recall}}} \right)}}$$
(14)
$$MCC=\frac{\text{TP}\times \text{ TN }-\text{ FP}\times \text{ FN}}{\sqrt{(\text{TP }+\text{ FP})(\text{TP }+\text{ FN})(\text{TN }+\text{ FP})(\text{TN }+\text{ FN})}}$$
(15)
where TP, TN, FN, and FP are truly positive, true Negative, False Negative, and False Positive numbers respectively. n is the number of classes.
Result and experimental
To evaluate the effectiveness of our machine learning framework, we conducted experiments in this section. The experiments were performed on a computer with a 3 GHz i5 processor, 8GB main memory, and a 64-bit Windows 10 operating system. We used the Python programming language to experiment.
Firstly, we train our model using CNN and NADAM, and the resulting MFCC features are shown in Fig.
6
a, while the chromogram that shows the relation between the time and pitch size is shown in Fig.
6
b. The autocorrelation plot that shows the lags that vary from 0 to 200,000 is shown in Fig.
6
c. The learning curve of the CNN + NADAM model is shown in Fig.
7
.
Fig. 6
The resulting
a
MFCC features,
b
chromogram, and
c
autocorrelation plot of the CNN and NADAM model
Full size image
Fig. 7
The Learning curve of the CNN + NADAM model
a
training and validation loss,
b
training and validation accuracy
Full size image
Secondly, we utilized a model that undergoes training utilizing ResNet (50), Burn Layer, and NADAM, resulting in MFCC features depicted in Fig.
8
a. Additionally, Fig.
8
b illustrates the chromogram, elucidating the relationship between time and pitch size. Furthermore, Fig.
8
c showcases the autocorrelation plot, exhibiting lags ranging from 0 to 100,000. The learning curve of the ResNet (50) + Burn Layer + NADAM model is presented in Fig.
9
.
Fig. 8
The resulting
a
MFCC features,
b
chromogram, and
c
autocorrelation plot of the ResNet (50) + Burn Layer + NADAM model
Full size image
Fig. 9
The Learning curve of the ResNet (50) + Burn Layer + NADAM model
a
training and validation loss,
b
training and validation accuracy
Full size image
Third, our model is trained using the proposed algorithm which consists of using a Burn Layer with specified burn intensity and CNN with Adamax illustrated in Figs.
3
and
4
. The proposed model resulting in MFCC features is displayed in Fig.
10
a, while the chromogram illustrating the relationship between time and pitch size is depicted in Fig.
10
b. Additionally, Fig.
10
c showcases the autocorrelation plot, revealing lags ranging from 0 to 300,000. Subsequently, the learning curve of the proposed CNN + NADAM model is presented in Fig.
11
.
Fig. 10
The resulting
a
MFCC features,
b
chromogram, and
c
autocorrelation plot of the proposed model
Full size image
Fig. 11
The Learning curve of the proposed model
a
training and validation loss,
b
training and validation accuracy
Full size image
We performed experimental results using fivefold cross-validation along with standard deviation (SD) and the obtained results are shown in Table
5
as follows. The proposed model, evaluated using fivefold cross-validation, exhibits robust performance metrics with a sensitivity of 89.96% ± 0.0528, specificity of 82.58% ± 0.1124, precision of 87.49% ± 0.0996, and accuracy of 89.86% ± 0.0371. It also achieves a high F1 score of 88.4% ± 0.0706 and an MCC of 72.00% ± 0.1260, demonstrating reliable and balanced classification capabilities as shown in Table
5
.
Table 5 Performance metrics of the proposed model using fivefold cross validation
Full size table
Table
6
showcases the performance comparison of three architectures: CNN + Nadam, ResNet (50) + Burn Layer + Nadam, the proposed model using fivefold cross-validation, and the proposed model based on Burn Layer with burn intensity parameter and Adamax optimizer. The evaluation is based on six standard classification metrics: sensitivity, specificity, precision, negative predictive value, accuracy, F1 score, and Matthews’s correlation coefficient.
Table 6 The performance of three architectures CNN + Nadam, ResNet (50) + BurnLayer + Nadam, and the proposed model
Full size table
As shown in Table
6
, CNN + Nadam showed adequate yet modestly inferior results, achieving a sensitivity of 89.29% and an accuracy of 88.24%. ResNet (50) + Burn Layer + Nadam remarkably surpassed CNN + Nadam in specific areas, reaching a sensitivity of 97.56% and accuracy of 95.51%, though experiencing some decline in certain metrics. Nonetheless, the proposed work significantly outperformed the competition, boasting a sensitivity of 96.77%, specificity of 100%, precision of 100%, negative predictive value of 95.00%, accuracy of 98.55%, F1 score of 98.36%, and Matthews’s correlation coefficient of 95.88%. These exceptional figures validate the proposed work's prowess as a distinguished and competitive solution for poultry health status assessment tasks. The proposed model demonstrates 100% precision, reflecting its exceptional ability to classify positive cases accurately without any false positives. This performance was validated through fivefold cross-validation, revealing robust metrics: a sensitivity of 96.77%, specificity of 100%, and accuracy of 98.55%. These results highlight the model's superior classification capabilities compared to previous methods, which achieved lower precision. The deep learning model integrates a custom Burn Layer that introduces random perturbations to the input data, enhancing the model’s resilience to varying signals. With a streamlined 19-layer architecture, including convolutional blocks, batch normalization, max pooling, and global average pooling, the model effectively handles audio classification tasks.
Table
7
presents the hyperparameters and their corresponding values used in the experimental setup. In determining the configuration of audio processing and training, hyperparameters play a crucial role in shaping the overall performance. In the audio analysis, Table
7
provides an exhaustive overview of these key parameters and their respective values employed in the experimental setup, pivotal for extracting meaningful features from audio signals and facilitating effective model training. The essential hyperparameters encompass the sampling frequency, where a rate of 44.1 kHz was utilized to ensure high-quality signal representation; the duration, specifying 2-s segments for analysis, allowing ample time for capturing pertinent audio information; the number of epochs, set at 50 to facilitate comprehensive learning from the data; the batch size, optimized at 32 to balance computational efficiency and model convergence; and the number of MFCC features, with 20 features chosen to capture relevant spectral information. These parameters collectively mold the audio processing pipeline and model training, enabling researchers to effectively analyze audio signals and extract valuable insights by selecting appropriate values.
Table 7 Hyperparameters and their corresponding values were used in the experimental setup
Full size table
To the best of our knowledge, since the dataset presented by Adebayo et al. in 2023 was published in the Data in Brief Journal [
18
], no other studies have cited or used this dataset. Therefore, we have made comparisons with related studies that used different datasets, as shown in Table
1
. Additionally, in Table
8
, we have focused on the audio files, which enable the diagnosis of poultry diseases.
Table 8 Comparative studies on diagnosing poultry diseases using audio files
Full size table
The proposed model stands out by introducing a Burn Layer that injects controlled noise during training, enhancing robustness and generalization, a feature absent in other studies. It also develops an end-to-end pipeline specifically tailored for audio-based poultry health status detection, unlike most studies that rely on traditional machine learning or generic deep learning models. With an exceptional accuracy of 98.55%, the proposed model outperforms all compared methodologies. It efficiently reduces trainable parameters to 191,235, demonstrating high efficiency essential for practical deployment in resource-constrained environments. Comparative studies show other methodologies achieving lower results, such as Xu and Chang's YOLO V7 + LSTM with an mAP of 86%, Huang et al.'s SVM with 90% accuracy, Quintana et al.'s DT with 86.10% accuracy, Cuan et al.'s ResNet-50 model with 91.06% accuracy, and Carpentier et al.'s deep learning models with 88.40% sensitivity. The proposed model’s contributions, including the Burn Layer and specialized pipeline, deliver markedly improved results in poultry disease diagnosis using audio files.
Statistical analysis: posthoc Nemenyi test
In this paper, we performed the statistical analysis using the Posthoc Nemenyi test which allows us to compare the pairs of models to determine which pairs are significantly different. The test produces a test statistic called the Nemenyi statistic, which is calculated as in Eq. (
16
).
$$Nermenyi\; statistic= {\left(\frac{a}{b}\right)}^{2}{-\left(\frac{c}{d}\right)}^{2}$$
(16)
where a and b are the accuracies of two models being compared, and c and d are the times required to achieve those accuracies. The p value for the Nemenyi test is calculated as in Eq. (
17
).
$$p{ - }value{ } = P(Nemenyi\,{\text{statistic}} > {\text{observed}}\,{\text{Nemenyi}}\,{\text{statistic}}){ }$$
(17)
The results of the posthoc Nemenyi test are as follows:
i)
CNN
+
Nadam vs. ResNet (50)
+
BurnLayer
+
Nadam
:
Nemenyi statistic:
\({\left(\frac{95.51}{88.24}\right)}^{2}-{\left(\frac{1}{1}\right)}^{2}=1.4014\)
p-value: P(Nemenyi statistic > 1.4014) = 0.1617
ii)
CNN
+
Nadam vs. The Proposed Model
:
Nemenyi statistic:
\({\left(\frac{97.73}{88.24}\right)}^{2}-{\left(\frac{1}{1}\right)}^{2}=1.2352\)
p-value:
\(P(Nemenyi\; statistic>1.2352)=0.2164\)
iii)
ResNet (50)
+
BurnLayer
+
Nadam vs. The Proposed Model
:
Nemenyi statistic:
\({\left(\frac{97.73}{95.51}\right)}^{2}-{\left(\frac{1}{1}\right)}^{2}=0.0916\)
p-value:
\(P\left(Nemenyi\; statistic>0.0916\right)=0.7626\)
The p values from the Nemenyi test for all comparisons exceed the significance level of 0.05, indicating that the differences in accuracy between the models are not statistically significant. Although the proposed model demonstrates improvements in various performance metrics, these improvements are not statistically significant at the 0.05 level. Despite the lack of statistical significance, the proposed model exhibits the best overall performance: it achieves the highest accuracy (97.73%) and perfect sensitivity (100%), correctly identifying all positive cases. It also shows perfect specificity (100%), accurately identifying all negative cases. The model maintains high precision (95.00%) and the highest F1 score (95.88%), reflecting a strong balance between precision and recall. Additionally, with the highest Matthews Correlation Coefficient (95.17%), the proposed model demonstrates a robust correlation between observed and predicted classifications.
Discussions and limitations
In this study, we propose a deep learning-based model for poultry audio classification, incorporating a Burn Layer for enhanced robustness. The model processes raw audio data and creates digital representations before applying a custom Burn Layer, which perturbs the input during training to improve model robustness. The architecture consists of convolutional blocks, global average pooling, fusion layers, and fully connected layers with another Burn Layer for additional robustness. Adamax optimizer is utilized to tackle the overfitting problem and improve the performance stability.
Compared to the traditional CNN + Nadam model, our proposed model demonstrated superior performance in terms of sensitivity, specificity, precision, negative predictive value, accuracy, F1 score, and Matthews’s correlation coefficient. The inclusion of the Burn Layer proved advantageous in increasing model stability during training and exposed it to varying input, contributing to the model's overall efficacy.
Despite the promising results, limitations do exist in this study. First, the model's performance might degrade if confronted with extremely noisy or unstructured audio data since the Burn Layer introduces only controlled random noise. Further modifications may be needed to account for extreme cases. Second, expanding the dataset to cover a broader variety of poultry breeds and health conditions could strengthen the model's applicability and generalizability. Third, integrating transfer learning techniques could expedite model training and improve performance, particularly when limited training data is available.
Lastly, it is essential to acknowledge ethical concerns surrounding AI adoption in healthcare and veterinary settings. Ensuring privacy, fairness, and avoiding biases must be priorities when leveraging AI for medical diagnoses. Transparent communication and collaboration among experts, policymakers, and stakeholders are crucial to establishing trustworthy and impactful AI solutions. Addressing these limitations and considerations will undoubtedly fuel ongoing research and drive advancements in deep learning-powered poultry health assessment.
Conclusion and future work
This study presents a deep learning model for poultry audio classification that incorporates a custom Burn Layer to enhance robustness during training. The Burn Layer, which introduces random perturbations to input data, helps the model handle varying signals effectively. The model features a streamlined 19-layer architecture with three convolutional blocks, batch normalization, max pooling, and global average pooling, totaling 191,235 trainable parameters. Our model achieves impressive performance, with a sensitivity of 96.77%, specificity of 100.00%, and accuracy of 98.55%, surpassing previous methods in accuracy, precision, and recall.
Future research will focus on refining the Burn Layer by exploring adaptive burn intensities based on performance metrics, integrating recurrent networks to capture long-term dependencies, and validating the model with larger datasets. Additionally, investigating the effect of adversarial examples on model robustness and developing a real-time user interface for poultry health assessment are promising avenues. Our study utilized the dataset from Adebayo et al., which includes 346.wav files categorized as healthy (139), noise (86), and unhealthy (121). Future work will further define inclusion and exclusion criteria, particularly addressing challenges like overlapping voices, to improve dataset relevance and model accuracy.
Data availability
The dataset used in this study is public and all test data are available at:
https://data.mendeley.com/datasets/zp4nf2dxbh/1
.
References
Jukan A, Masip-Bruin X, Amla N. Smart computing and sensing technologies for animal welfare: a systematic review. ACM Comput Surv CSUR. 2017;50(1):1–27.
Google Scholar
Petso T, Jamisola RS Jr, Mpoeleng D. Review on methods used for wildlife species and individual identification. Eur J Wildl Res. 2022;68(1):3.
Article
Google Scholar
Vranken E, Mounir M, Norton T. Sound-based monitoring of livestock. In: Zhang Q, editor. Encyclopedia of digital agricultural technologies. Berlin: Springer; 2023. p. 1358–69.
Chapter
Google Scholar
Gibb R, Browning E, Glover-Kapfer P, Jones KE. Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring. Methods Ecol Evol. 2019;10(2):169–85.
Article
Google Scholar
Farrell DJ. Matching poultry production with available feed resources: issues and constraints. Worlds Poult Sci J. 2005;61(2):298–307.
Article
Google Scholar
Fontana I, Tullo E, Scrase A, Butterworth A. Vocalisation sound pattern identification in young broiler chickens. Animal. 2016;10(9):1567–74.
Article
Google Scholar
Laleye FA, Mousse MA. Attention-based recurrent neural network for automatic behavior laying hen recognition. Multimed Tools Appl. 2024;83:62443–58.
Article
Google Scholar
Tokuda I, Riede T, Neubauer J, Owren MJ, Herzel H. Nonlinear analysis of irregular animal vocalizations. J Acoust Soc Am. 2002;111(6):2908–19.
Article
Google Scholar
Tampuu A, Matiisen T, Semikin M, Fishman D, Muhammad N. A survey of end-to-end driving: Architectures and training methods. IEEE Trans Neural Netw Learn Syst. 2020;33(4):1364–84.
Article
Google Scholar
Shams MY, Hassanien AE, Tang M. Deep belief neural networks for eye localization based speeded up robust features and local binary pattern. In: Shi X, Bohács G, Ma Y, Gong D, Shang X, editors. LISS 2021. Lecture notes in operations research. Singapore: Springer Nature; 2022. p. 415–30.
https://doi.org/10.1007/978-981-16-8656-6_38
.
Chapter
Google Scholar
Hassan E, Shams MY, Hikal NA, Elmougy S. The effect of choosing optimizer algorithms to improve computer vision tasks: a comparative study. Multimed Tools Appl. 2023;82(11):16591–633.
https://doi.org/10.1007/s11042-022-13820-0
.
Article
Google Scholar
Abdallah SE, Elmessery WM, Shams MY, Al-Sattary NSA, Abohany AA, Thabet M. Deep learning model based on ResNet-50 for beef quality classification. Inf Sci Lett. 2023;12(1):289–97.
Article
Google Scholar
Li Y, Chen Y, Wang N, Zhang Z. Scale-aware trident networks for object detection. In:Proceedings of the IEEE/CVF international conference on computer vision; 2019. p. 6054–63.
Salem H, Shams MY, Elzeki OM, Abd Elfattah M, Al-Amri JF, Elnazer S. Fine-tuning fuzzy KNN classifier based on uncertainty membership for the medical diagnosis of diabetes. Appl Sci. 2022;12(3):950.
Article
Google Scholar
Li X, et al. Efficient meta-tuning for content-aware neural video delivery. In: Avidan S, Brostow G, Cissé M, Farinella GM, Hassner T, editors., et al., Computer vision—ECCV 2022. Lecture notes in computer science. Cham: Springer Nature Switzerland; 2022. p. 308–24.
Google Scholar
Shams MY, El-kenawy E-SM, Ibrahim A, Elshewey AM. A hybrid dipper throated optimization algorithm and particle swarm optimization (DTPSO) model for hepatocellular carcinoma (HCC) prediction. Biomed Signal Process Control. 2023;85:104908.
https://doi.org/10.1016/j.bspc.2023.104908
.
Article
Google Scholar
Abdelhamid AA, et al. Innovative feature selection method based on hybrid sine cosine and dipper throated optimization algorithms. IEEE Access. 2023;11:79750–76.
https://doi.org/10.1109/ACCESS.2023.3298955
.
Article
Google Scholar
Adebayo S, et al. Enhancing poultry health management through machine learning-based analysis of vocalization signals dataset. Data Brief. 2023;50:109528.
https://doi.org/10.1016/j.dib.2023.109528
.
Article
Google Scholar
Nam J, Choi K, Lee J, Chou S-Y, Yang Y-H. Deep learning for audio-based music classification and tagging: teaching computers to distinguish rock from bach. IEEE Signal Process Mag. 2018;36(1):41–51.
Article
Google Scholar
Li G, et al. Missing outcome data in recent perinatal and neonatal clinical trials. Pediatrics. 2024;153:e2023063101.
Article
Google Scholar
Morgan NK, Kim E, González-Ortiz G. Holo-analysis of the effects of xylo-oligosaccharides on broiler chicken performance. Br Poult Sci. 2024;65:79–86.
Article
Google Scholar
Nakrosis A, et al. Towards early poultry health prediction through non-invasive and computer vision-based dropping classification. Animals. 2023;13(19):3041.
https://doi.org/10.3390/ani13193041
.
Article
Google Scholar
He P, et al. Research progress in the early warning of chicken diseases by monitoring clinical symptoms. Appl Sci. 2022;12(11):5601.
https://doi.org/10.3390/app12115601
.
Article
Google Scholar
Mao Q, et al. Review detection of Newcastle disease virus. Front Vet Sci. 2022.
https://doi.org/10.3389/fvets.2022.936251
.
Article
Google Scholar
Machuve D, Nwankwo E, Mduma N, Mbelwa J. Poultry diseases diagnostics models using deep learning. Front Artif Intell. 2022.
https://doi.org/10.3389/frai.2022.733345
.
Article
Google Scholar
Liang J, Zhang C, Song J, Guo S. Research and prediction on initial contact pressure distribution of armature-rail contact surface under interference fit. 2024.p. 1–20.
Machuve D, Nwankwo E, Mduma N, Mbelwa J. Poultry diseases diagnostics models using deep learning. Front Artif Intell. 2022;5:733345.
https://doi.org/10.3389/frai.2022.733345
.
Article
Google Scholar
Cai Z, Cui J, Yuan H, Cheng M. Application and research progress of infrared thermography in temperature measurement of livestock and poultry animals: a review. Comput Electron Agric. 2023;205:107586.
https://doi.org/10.1016/j.compag.2022.107586
.
Article
Google Scholar
Caldara F, Nääs I, Garcia R. Infrared thermal image for assessing animal health and welfare. J Anim Behav Biometeorol. 2014;2:66–72.
https://doi.org/10.14269/2318-1265/jabb.v2n3p66-72
.
Article
Google Scholar
Yahav S, Giloh M. Infrared thermography—applications in poultry biological research. Infrared Thermogr. 2012.
https://doi.org/10.5772/27788
.
Article
Google Scholar
Nawaz AH, Amoah K, Leng QY, Zheng JH, Zhang WL, Zhang L. Poultry response to heat stress: its physiological, metabolic, and genetic implications on meat production and quality including strategies to improve broiler production in a warming world. Front Vet Sci. 2021;8:699081.
https://doi.org/10.3389/fvets.2021.699081
.
Article
Google Scholar
Noh J-Y, et al. Thermal image scanning for the early detection of fever induced by highly pathogenic avian influenza virus infection in chickens and ducks and its application in farms. Front Vet Sci. 2021;8:616755.
https://doi.org/10.3389/fvets.2021.616755
.
Article
Google Scholar
Chuang C-H, Chiang C-Y, Chen Y-C, Lin C-Y, Tsai Y-C. Goose surface temperature monitoring system based on deep learning using visible and infrared thermal image integration. IEEE Access. 2021;9:131203–13.
https://doi.org/10.1109/ACCESS.2021.3113509
.
Article
Google Scholar
Gourisaria MK, Arora A, Bilgaiyan S, Sahni M. Chicken disease multiclass classification using deep learning, vol. 614 LNNS. Singapore: Springer Nature Singapore; 2023.
Google Scholar
Carroll B, Anderson D, Daley W, Harbert S, Britton D, Jackwood M. Detecting symptoms of diseases in poultry through audio signal processing. In:IEEE global conference on signal and information processing, Global 2014. 2015. p. 1132–5.
https://doi.org/10.1109/GlobalSIP.2014.7032298
.
Aydin A, Berckmans D. Using sound technology to automatically detect the short-term feeding behaviours of broiler chickens. Comput Electron Agric. 2016;121:25–31.
https://doi.org/10.1016/j.compag.2015.11.010
.
Article
Google Scholar
Quintana MMD, Infante RRD, Torrano JCS, Pacis MC. A hybrid solar powered chicken disease monitoring system using decision tree models with visual and acoustic imagery. In: 2022 14th International conference on computer and automation engineering ICCAE; 2022. p. 65–9.
Li Z, et al. Sex detection of chicks based on audio technology and deep learning methods. Anim Open Access J MDPI. 2022;12(22):3106.
https://doi.org/10.3390/ani12223106
.
Article
Google Scholar
Cuan K, Zhang T, Li Z, Huang J, Ding Y, Fang C. Automatic Newcastle disease detection using sound technology and deep learning method. Comput Electron Agric. 2022;194(January):106740.
https://doi.org/10.1016/j.compag.2022.106740
.
Article
Google Scholar
Jakovljević N, Maljkovic N, Mi\vsković D, Kne\vzević P, Delić V. A broiler stress detection system based on audio signal processing.In: 2019 27th telecommunication forum TELFOR; 2019. p. 1–4.
Wang C, Benetos E, Wang S, Versace E. Joint scattering for automatic chick call recognition. In:2022 30th European signal processing conference (EUSIPCO); 2022. p. 195–9.
https://doi.org/10.23919/EUSIPCO55093.2022.9909738
.
Carpentier L, Vranken E, Berckmans D, Paeshuyse J, Norton T. Development of sound-based poultry health monitoring tool for automated sneeze detection. Comput Electron Agric. 2019;162:573–81.
Article
Google Scholar
Huang J, Wang W, Zhang T. Method for detecting avian influenza disease of chickens based on sound analysis. Biosyst Eng. 2019;180:16–24.
https://doi.org/10.1016/j.biosystemseng.2019.01.015
.
Article
Google Scholar
Jamshidi H, Budak E. On the prediction of surface burn and its thickness in grinding processes. CIRP Ann. 2021;70(1):285–8.
Article
Google Scholar
Suha SA, Sanam TF. A deep convolutional neural network-based approach for detecting burn severity from skin burn images. Mach Learn Appl. 2022;9:100371.
Google Scholar
Zhang P, Nascetti A, Ban Y, Gong M. An implicit radar convolutional burn index for burnt area mapping with Sentinel-1 C-band SAR data. ISPRS J Photogramm Remote Sens. 2019;158:50–62.
Article
Google Scholar
Shrivastava AK, Sharma A, Awale AS, Yusufzai MZK, Vashista M. Assessment of grinding burn of AISI D2 tool steel using Barkhausen noise technique. J Inst Eng India Ser C. 2021;102(4):885–96.
Article
Google Scholar
Jiang S, Wang Y, Wang Y. SelfEvolve: a code evolution framework via large language models.ArXiv Preprint arXiv:2306.02907, 2023.
Cirillo MD, Mirdell R, Sjöberg F, Pham TD. Time-independent prediction of burn depth using deep convolutional neural networks. J Burn Care Res. 2019;40(6):857–63.
https://doi.org/10.1093/jbcr/irz103
.
Article
Google Scholar
Salehin I, Kang D-K. A review on dropout regularization approaches for deep neural networks within the scholarly domain. Electronics. 2023;12(14):3106.
https://doi.org/10.3390/electronics12143106
.
Article
Google Scholar
Liu T, Chen T, Niu R, Plaza A. Landslide detection mapping employing CNN, ResNet, and DenseNet in the Three Gorges reservoir, China. IEEE J Sel Top Appl Earth Obs Remote Sens. 2021;14:11417–28.
Article
Google Scholar
Weng O, et al. Tailor: altering skip connections for resource-efficient inference. ACM Trans Reconfig Technol Syst. 2024;17(1):1–23.
Article
MathSciNet
Google Scholar
Yuan X, Savarese P, Maire M. Accelerated training via incrementally growing neural networks using variance transfer and learning rate adaptation. Adv Neural Info Process Syst. 2024;36:16673–16692.
Google Scholar
Thakur N, Bhattacharjee E, Jain R, Acharya B, Hu Y-C. Deep learning-based parking occupancy detection framework using ResNet and VGG-16. Multimed Tools Appl. 2024;83(1):1941–64.
Article
Google Scholar
Hu Y, Deng L, Wu Y, Yao M, Li G. Advancing spiking neural networks toward deep residual learning. In: IEEE transactions on neural networks and learning systems. 2024. pp 1–15.
Hassan E, Hossain MS, Saber A, Elmougy S, Ghoneim A, Muhammad G. A quantum convolutional network and ResNet (50)-based classification architecture for the MNIST medical dataset. Biomed Signal Process Control. 2024;87:105560.
https://doi.org/10.1016/j.bspc.2023.105560
.
Article
Google Scholar
Antonio CB, Bautista LGC, Labao AB, Naval PC. Vertebra fracture classification from 3D CT lumbar spine segmentation masks using a convolutional neural network. In: Nguyen NT, Hoang DH, Hong T-P, Pham H, Trawiński B, editors. Intelligent information and database systems Lecture notes in computer science. Cham: Springer International Publishing; 2018. p. 449–58.
https://doi.org/10.1007/978-3-319-75420-8_43
.
Chapter
Google Scholar
Alom MZ, Hasan M, Yakopcic C, Taha TM, Asari VK. Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation. arXiv preprint arXiv:1802.06955, 2018.
Sharan RV, Moir TJ. An overview of applications and advancements in automatic sound recognition. Neurocomputing. 2016;200:22–34.
Article
Google Scholar
Incze A, Jancsó H-B, Szilágyi Z, Farkas A, Sulyok C. Bird sound recognition using a convolutional neural network. In:2018 IEEE 16th international symposium on intelligent systems and informatics (SISY). IEEE; 2018. p. 000295–300.
Zhang H, McLoughlin I, Song Y. Robust sound event recognition using convolutional neural networks. In:2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE; 2015. p. 559–63.
Mehta S, Paunwala C, Vaidya B. CNN based traffic sign classification using Adam optimizer. In:2019 international conference on intelligent computing and control systems (ICCS). IEEE; 2019. p. 1293–8.
Vani S, Rao TM. An experimental approach towards the performance assessment of various optimizers on convolutional neural network. In 2019 3rd international conference on trends in electronics and informatics (ICOEI). IEEE; 2019. p. 331–6.
Yaqub M, et al. State-of-the-art CNN optimizer for brain tumor segmentation in magnetic resonance images. Brain Sci. 2020;10(7):427.
Article
Google Scholar
Kumar A, Sarkar A, Pradhan C. Malaria disease detection using CNN technique with SGD, RMSprop and ADAM optimizers. In: Dash S, Acharya B, Mittal M, et al. (eds) Deep learning techniques for biomedical and health informatics. Studies in Big Data, vol. 68. Cham: Springer, 2020. pp. 211–230.
Shams MY, Abd El-Hafeez T, Hassan E. Acoustic data detection in large-scale emergency vehicle sirens and road noise datase. Expert Syst Appl. 2024;249:123608.
https://doi.org/10.1016/j.eswa.2024.123608
.
Article
Google Scholar
Hassan E, Shams MY, Hikal NA, Elmougy S. A novel convolutional neural network model for malaria cell images classification. Comput Mater Contin. 2022;72(3):5889–907.
https://doi.org/10.32604/cmc.2022.025629
.
Article
Google Scholar
Sarhan S, Nasr AA, Shams MY. Multipose face recognition-based combined adaptive deep learning vector quantization. Comput Intell Neurosci. 2020;2020:1–11.
Article
Google Scholar
Xu R-Y, Chang C-L. Deep learning-based poultry health diagnosis: detecting abnormal feces and analyzing vocalizations. In: 2024 10th international conference on applied system innovation (ICASI). 2024. p. 55–7.
https://doi.org/10.1109/ICASI60819.2024.10547723
.
Download references
Funding
Open access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB). Not applicable.
Author information
Authors and Affiliations
Department of Machine Learning and Information Retrieval, Faculty of Artificial Intelligence, Kafrelsheikh University, Kafr El-Sheikh, 33516, Egypt
Esraa Hassan, Mahmoud Y. Shams & Nora El-Rashidy
Department of Data Science, Faculty of Artificial Intelligence, Kafrelsheikh University, Kafrelsheikh, 33516, Egypt
Samar Elbedwehy
Department of Computer Science, Faculty of Science, Minia University, El-Minia, 61519, Egypt
Tarek Abd El-Hafeez
Computer Science Unit, Deraya University, El-Minia, Egypt
Tarek Abd El-Hafeez
Authors
Esraa Hassan
View author publications
You can also search for this author in
PubMed
Google Scholar
Samar Elbedwehy
View author publications
You can also search for this author in
PubMed
Google Scholar
Mahmoud Y. Shams
View author publications
You can also search for this author in
PubMed
Google Scholar
Tarek Abd El-Hafeez
View author publications
You can also search for this author in
PubMed
Google Scholar
Nora El-Rashidy
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
This work was carried out in collaboration among all authors. All Authors designed the study, performed the statistical analysis, and wrote the protocol. Authors MYS, TAEH, and EH managed the analyses of the study, managed the literature searches, and wrote the first draft of the manuscript. All authors read and approved the final manuscript.
Corresponding author
Correspondence to
Tarek Abd El-Hafeez
.
Ethics declarations
Consent for publication
This article does not contain any studies with human participants or animals performed by any of the authors.
Competing interests
The authors declare that they have no known competing interests.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by/4.0/
.
Reprints and permissions
About this article
Cite this article
Hassan, E., Elbedwehy, S., Shams, M.Y.
et al.
Optimizing poultry audio signal classification with deep learning and burn layer fusion.
J Big Data
11
, 135 (2024). https://doi.org/10.1186/s40537-024-00985-8
Download citation
Received
:
19 March 2024
Accepted
:
26 August 2024
Published
:
18 September 2024
DOI
:
https://doi.org/10.1186/s40537-024-00985-8
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Poultry audio classification
Deep learning
Convolutional neural networks
Digital audio signal processing
Disease detection
Sensitivity
Specificity
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00991-w):
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
18 September 2024
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
Doaa El-Shahat
1
,
Ahmed Tolba
1
,
Mohamed Abouhawwash
2
&
…
Mohamed Abdel-Basset
1
Show authors
Journal of Big Data
volume
11
, Article number:
134
(
2024
)
Cite this article
80
Accesses
Metrics
details
Abstract
In late 2023, the United Nations conference on climate change (COP28), which was held in Dubai, encouraged a quick move from fossil fuels to renewable energy. Solar energy is one of the most promising forms of energy that is both sustainable and renewable. Generally, photovoltaic systems transform solar irradiance into electricity. Unfortunately, instability and intermittency in solar radiation can lead to interruptions in electricity production. The accurate forecasting of solar irradiance guarantees sustainable power production even when solar irradiance is not present. Batteries can store solar energy to be used during periods of solar absence. Additionally, deterministic models take into account the specification of technical PV systems and may be not accurate for low solar irradiance. This paper presents a comparative study for the most common Deep Learning (DL) and Machine Learning (ML) algorithms employed for short-term solar irradiance forecasting. The dataset was gathered in Islamabad during a five-year period, from 2015 to 2019, at hourly intervals with accurate meteorological sensors. Furthermore, the Grid Search Cross Validation (GSCV) with five folds is introduced to ML and DL models for optimizing the hyperparameters of these models. Several performance metrics are used to assess the algorithms, such as the
Adjusted R
2
score
,
Normalized Root Mean Square Error
(NRMSE),
Mean Absolute Deviation
(MAD),
Mean Absolute Error
(MAE) and
Mean Square Error
(MSE). The statistical analysis shows that CNN-LSTM outperforms its counterparts of nine well-known DL models with
Adjusted R
2
score
value of 0.984. For ML algorithms, gradient boosting regression is an effective forecasting method with
Adjusted R
2
score
value of 0.962, beating its rivals of six ML models. Furthermore, SHAP and LIME are examples of explainable Artificial Intelligence (XAI) utilized for understanding the reasons behind the obtained results.
Introduction
Climate change (CC) [
1
,
2
] has emerged as a global issue of paramount importance due to its devastating impact on the existence of all forms of life on Earth. It can pose significant threats to biodiversity loss as well as human health and food production [
3
] owing to increased heat and humidity [
4
]. Hence, Intergovernmental Panel on CC (IPCC) has called for urgent decisions to mitigate the consequences of CC [
5
]. Fossil fuels encompass hydrocarbons and their derivatives, containing oil, coal and gas. Fossil fuels are one of the key factors of CC [
6
]. Until now, fossil fuels have been a primary source of non-renewable energy. Unfortunately, the combustion of fossil fuels results in greenhouse gases and carbon dioxide emissions into the atmosphere. When the sunlight penetrates the Earth's atmosphere, these emissions trap the heat and keep it from escaping back to space, which leads to global warming in the long term. Global warning can seriously destroy life on our planet.
Due to the aforementioned detrimental impacts of fossil fuels, about 197 countries at the 28th Conference of Parties (COP28) agreed to gradually shift from traditional fossil fuels to renewable energy sources [
7
]. Also, there has been a concerted effort to harness the power of renewable energy technologies, such as solar [
8
], wind [
9
], hydro [
10
], and geothermal energy [
11
], which offer cleaner and more sustainable options for energy production. Moreover, the field of renewable energies has received increasing political and research attention to overcome CC and the energy crisis. Solar energy is regarded as one of the most ubiquitous renewable energy sources, as the sun is abundant throughout the year. Solar energy has many uses [
12
], such as water heating [
13
], food drying [
14
], heating buildings [
15
], irrigation [
16
], water distillation [
17
] and waste recycling [
18
].
With the help of Photovoltaic (PV) systems [
19
,
20
], the solar irradiance is transformed into electricity. Solar energy is considered a source of clean energy as it does not produce greenhouse gas emissions or other air pollutants during the electricity generation. The irregular presence of solar irradiance and the fluctuating environmental weather conditions (humidity, cloud cover, temperature, wind speed, and wind direction) affect the operational process, management, and stability of PV grids, which in turn affect the amount of electricity production. Therefore, predicting solar irradiance in the future based on historical observations to maintain the availability of solar energy is commonly known as Solar Irradiance Forecasting (SIF).
SIF is considered one of the most challenging issues. In businesses and industries, major decisions, including production, purchasing, and marketing can depend on forecasting. When the solar irradiance is not present, it is accumulated in batteries for future utilization. Hence, the more accurate prediction of solar irradiance helps to provide permanent electricity by using the solar energy stored in batteries during the intervals of solar irradiance absence. The forecast horizon [
21
] is the time period into the future over which the forecast is made, while the forecast resolution represents the scale or size of the frame at the forecast horizon. There are numerous categories of forecasting horizons [
22
,
23
], as shown in Fig.
1
. The figure presents some useful applications that are appropriate for each forecast horizon.
Fig. 1
Categorization of forecast horizons
Full size image
The domain of solar irradiance forecasting has garnered growing attention by many researchers over the past few decades. The literature outlines several methodologies that can be divided into three categories: deterministic [
24
], ML or statistical [
25
,
26
], deep learning [
27
,
28
], and hybrid methods [
29
,
30
]. However, deterministic techniques have certain drawbacks when predicting short-term forecasts [
31
]. Unlike these deterministic methods, ML and DL algorithms are considered more accurate to predict solar irradiance.
Short-term solar irradiance forecasting [
32
] is a challenging task that needs to be urgently solved to cope with the electricity production of many companies and factories. There is also the necessity of having a clean, renewable source of energy that can preserve our environment from pollution and global warming. All the aforementioned reasons motivate us to perform a comparative study to investigate the accuracy of many existing forecasting methods. Our work's primary contribution can be mentioned as follows:
This study investigates the performance of various deep learning algorithm employed for predicting short-term solar irradiance, such as artificial neural network, Convolutional Neural Network (CNN), Long Short Term Memory (LSTM), recurrent neural network, temporal convolutional network, gated recurrent unit, echo state networks, residual neural network and the hybrid model CNN-LSTM.
Also, another comparison among many ML algorithms will be conducted for tackling the solar irradiance forecasting, including linear regression, stochastic gradient descent regression, random forest, least absolute shrinkage and selection operator, gradient boosting regression, decision tree regression and K-nearest neighbor regression.
The GSCV with fivefold cross validation is suggested for hyperparameter tuning for all ML and DL models.
The dataset was collected from Islamabad over five years, from 2015 to 2019, at hourly intervals using precise meteorological instruments.
Several statistical analyses are performed, such as
Adjusted R
2
score
,
NRMSE
,
MAD
,
MSE
and
MAE
to examine the performance of different algorithm.
SHAP and LIME are examples of XAI that is responsible for interpreting and understanding the reasons behind getting specific results.
The main organization of the paper is shown as follows. Sect. "
Literature Review
" introduces a literature review of the recent algorithms developed for predicting the solar irradiance. Sect. "
Time series analysis
" illustrates the time series analysis and the process flow that is adopted by all ML and DL models in this paper. Sect. "
Materials and methods
" presents the structure of nine DL algorithms in addition to seven ML algorithms. In Sect. "
Results analysis and discussion
", the results are covered and discussed. Lastly, the conclusions and recommendations for further research are offered in Sect. "
Conclusions and future works
".
Literature review
Solar irradiance prediction is a challenging task that has garnered the attention of many researchers. Various advanced methods have proven their outperformance in predicting solar irradiance using ML and DL algorithms, aiming at maximizing solar energy production. Artificial Neural Network (ANN) is one of the most common DL model employed for SIF [
33
,
34
,
35
,
36
]. Integrating fuzzy logic with the ANN [
37
] improves the forecasting accuracy by 3.2% compared to the standalone ANN. Typically, fuzzy logic determines the correlation between features. Another study by Kumar and Kalavathi [
38
] proved that the ANN model outperforms the Adaptive Neuro-Fuzzy Inference System (ANFIS) model to predict PV power output.
Moreover, Qing and Niu [
39
] suggested an hour-ahead SIF in desert locations, considering the frequent dust storms. The MLP model outperforms other models, such as support vector regression, K-nearest neighbor, and decision tree regression, in terms of accuracy. The results of backpropagation ANN [
40
] show that predicting PV irradiance is better and more accurate compared to the Autoregressive Moving Average (ARMA). A comparative study [
41
] using six ML algorithms inferred that ANN is the most effective and predictive model out of the six algorithms. Gairaa et al. [
42
] aimed to forecast hourly global solar irradiation for time horizons from h + 1 to h + 6, using two approaches: multiple linear regression and ANN models.
Long short-term memory (LSTM) networks, which belong to the type of Recurrent Neural Networks (RNN), can learn about long-term dependencies while escaping the issue of gradient vanishing. LSTM [
43
] is another DL model proposed for the SIF problem and achieves better outcomes than support vector regression. Additionally, the LSTM model in [
44
] considers the interdependence between hours of the same day. The LSTM model [
45
] performs hyper-parameter tuning to find the optimal values of the parameters. After that, further data is added to see how it affects the improved model. In [
46
], LSTM is enhanced to predict the short-term and long-term solar irradiance for real-world data. Michael et al. [
47
] proposed a novel forecast model incorporating LSTM with Bayesian optimization and LSTM with a drop-out structure. To apply the LSTM model's predictions for solar radiation estimation, K-means and the red/blue ratio classify image pixels into clouds or the sky [
48
].
Researchers have adopted various models for SIF, including support vector machine [
49
], random forest [
50
], K-Nearest Neighbor [
51
], gated recurrent unit [
52
], echo state network [
53
], and temporal convolution network [
54
]. In this study [
55
], deep learning based on MLP, GRU, LSTM, CNN and CNN-LSTM models uses monthly data to estimate solar radiation. CNN outperforms other models with a MSE for global horizontal irradiance of 12.68 W/m
2
. The hybridization of the ML models offers a potent technique that exploits the strengths of various models. ALHM [
56
] is a hybrid forecast method that combines the ANN with a support vector machine, which has proven to be more accurate for five minutes ahead and daily forecasts.
Also, CNN is incorporated with LSTM [
57
], such that CNN is used to extract spatial features and LSTM is used to extract temporal features from historical solar irradiance. The work of Kumari and Toshniwal [
58
] proposed the LSTM-CNN by using LSTM to get the temporal features from time-series data of solar irradiance and then CNN to get the spatial features. Gao et al. [
59
] suggested integrating CNN-LSTM with CEEMDAN, which extracts features from constitutive series in historical data, in order to reliably predict solar irradiance. In [
60
], the hybrid model CNN-LSTM-MLP is mixed with the error correction and the VMD method to achieve the one-hour ahead solar irradiance forecasting. Also, Malakouti et al. [
61
] proposed the CNN-LSTM model for predicting the wind power, which demonstrated more accurate results. Guermoui et al. [
62
] developed an enhanced hybrid model for multi-hour ahead DNI forecasting. In [
63
], the authors applied the adjusted Boland–Ridley–Lauret (BRL) model was applied on two time scales, daily and hourly components.
Time series analysis
Time series analysis is a technique for analyzing data gathered over periods of time. In time series analysis, data comprises a set of observations or samples recorded at regular intervals throughout a predetermined time frame, as opposed to being recorded at random intervals. Time series analysis usually needs a lot of data points to guarantee consistency and reliability. Time series data is commonly used in forecasting, which predicts future data based on previous data. Figure
2
demonstrates the process flow and steps of analyzing the time series model for Global Horizontal Irradiance (GHI) data.
Fig. 2
The process flow of the time series model
Full size image
After gathering the solar irradiance data from photovoltaic grids, data preprocessing and feature extraction are performed to eliminate any noise, zero, and null values from this data. The next step conducts data normalization to eliminate biased findings and generate features that are on a comparable scale. There are two separate sets of data: the training set and the testing set. The model is trained with the chosen training set, and its performance is checked with a testing set. The final step discusses various statistical analyses done to investigate the performance of the trained models.
Materials and methods
Artificial Intelligence (AI) [
64
] encompasses methods that enable machines, especially computer systems, to imitate human intelligence in order to perform various tasks. ML is a part of AI that contains advanced techniques that allow machines to learn from data. Furthermore, DL is a branch of ML that uses neural networks for complex data processing. This section will introduce various ML algorithms employed for solving the SIF problem. Also, we will present the most famous algorithms used for tackling the problem in the field of DL. Figure
3
depicts a general view of AI, ML, and DL. Figure
4
exhibits the nine DL algorithms and seven ML algorithms that will be examined in our study.
Fig. 3
General view of artificial intelligence, machine learning, and deep learning
Full size image
Fig. 4
Classification of solar irradiance forecasting models
Full size image
Deep learning algorithms
The subsection delves into the most prominent deep learning models that are highly suggested for SIF, including Artificial Neural Network (ANN) [
65
], Multilayer Perceptron (MLP) neural network [
66
], Convolutional Neural Network (CNN) [
67
], Recurrent Neural Network (RNN) [
68
], Long Short Term Memory (LSTM) [
69
], Temporal Convolutional Network (TCN) [
70
], Gated Recurrent Unit (GRU) [
71
], Echo State Networks (ESN) [
72
], Residual Neural network [
73
], and finally the hybrid model CNN-LSTM [
74
]. The main structure and details about these DL algorithms will be explained below.
Artificial neural network
The ANN [
75
,
76
] is a computational model consisting of linked neurons arranged into input, hidden, and output layers, designed to simulate the operations of the human brain, as seen in Fig.
5
. It is highly recommended for recognizing patterns and forecasting tasks [
77
]. The input layer is mainly the first layer that receives initial data from the real world. ANN can contain one or more hidden layers. Data is passed from the input layer to the hidden layer, which processes it using learning methods and an activation function. Furthermore, the output layer is the last layer of ANN, consisting of a specific number of nodes representing the output values.
Fig. 5
Architecture of ANN
Full size image
Convolutional neural network
Similar to other forms of ANNs, CNNs [
67
,
78
] are composed of interconnected artificial neurons through weights and arranged in layers. It is typically designed to learn and extract features from visual data or images. Through the use of convolutional layers, pooling layers, CNNs can learn from raw pixel data. By using CNN on time series data for forecasting, it becomes possible to effectively capture temporal relationships and extract important features. Additionally, CNN can interpret the incoming data as a sequence of one-dimensional signals. The CNN utilizes one-dimensional convolutions and pooling procedures to learn local patterns and detect temporal correlations within the data.
CNN comprises many layers like convolution layer, flatten layer, pooling layer, and output layer. For clarity, Fig.
6
displays the architecture of CNN. The convolution layer is usually called the feature extractor layer since it gets the best features of the image and time series data by sliding the filter over the next receptive field of the same data. The CNN employs the Rectified Linear Unit (ReLU) activation function for setting all negative values to be zero using the following equation (see Fig.
7
):
$$f(x) = \max (0,x)$$
(1)
Fig. 6
Architecture of CNN
Full size image
Fig. 7
Graphical representation of the ReLU activation function
Full size image
The pooling layer helps to lessen the spatial volume of data after the convolution layer. There are many pooling methods, such as average pooling, max pooling, and L2-norm pooling. The max pooling approach chooses the highest value within the window, as can be seen in Fig.
8
. It can be found after a convolution layer or between two of them but not after the fully connected layer to lessen the computational cost. The flatten layer is located between the CNN and the ANN, and its function is to convert the output of the CNN into an input that the ANN can process to learn complex patterns and make predictions. Finally, the output layer obtains the final output.
Fig. 8
Max pooling technique
Full size image
Recurrent neural network
Unlike other neural networks architectures, RNN [
79
] enables bidirectional information flow through both forward and backward propagation. RNNs are suggested for handling sequence and time series data [
80
], such as audio, sensor data [
81
], and text [
82
]. The RNN consists of three layers: an input layer (
\(x\)
), a recurrent hidden layer (
\(h\)
), and an output layer (
\(O\)
), as shown in Fig.
9
. It can be modeled mathematically as follows [
83
]:
$$h_{t} = f_{RNN} (U \times x_{t} + W \times h_{t - 1} + b)$$
(2)
$$\hat{y}_{t} = \sigma (Vh_{t} )$$
(3)
where
\(x_{t}\)
and
\(h_{t}\)
are is the input and the hidden states at the current time step
\(t\)
. Furthermore,
\(h_{t - 1}\)
refers to the hidden state at the previous time step. The variable
\(\widehat{{y_{t} }}\)
refers to predicted output at a time step
\(t\)
. The parameters
\(U\)
,
\(W\)
, and
\(V\)
are the weight vectors for input, hidden, and output layers, respectively. The symbol
\(f_{RNN}\)
indicates the used activation function and
\(b\)
indicates the bias term. The
\(\sigma\)
is the sigmoid activation function. The feedback loop in its current hidden layer enables the RNN to keep the memory of past information. This short-term memory makes the network to analyze previous inputs when producing output and revealing the relationships between data points that are far from each other (Table
1
).
Fig. 9
Architecture of RNN cell
Full size image
Table 1 Types of RNN
Full size table
Table
2
encounters various types of RNN according to the number of input/output layers. Also, the table mentions the best suited application for each type of RNN. RNNs work by applying back propagation through time. In this manner, weights for the current and previous inputs are updated by propagation of the error from the last time step to the first one. An advantage of RNN is that the model size does not increase with the size of the input. Long time steps cause weight gradients to vanish, becoming small numbers close to zero. Thus, the network does not learn and isn't suitable for long-term dependencies.
Table 2 Specifications of the used weather data
Full size table
Long short term memory
LSTM can tackle the vanishing gradient problem faced by RNN, since it depends on gates mechanism by introducing input, output, and forget gates [
83
]. Figure
10
shows the LSTM cell structure. These gates control which information is retained throughout the network. The forget gate (
\(f_{t}\)
) seeks to forget and delete any irrelevant information from the LSTM cell in a specific time step, as can be seen in Fig.
11
. The input gate (
\(i_{t}\)
) adds and updates new information while the output gate (
\(O_{t}\)
) returns the updated information. The mathematical model of LSTM cell is defined as follows:
$$f_{t} = \sigma (w_{f} x_{t} + U_{f} h_{t - 1} + b_{f} )$$
(4)
$$\widehat{{c_{t} }} = \tanh (w_{c} x_{t} + U_{c} h_{t - 1} + b_{c} )$$
(5)
$$i_{t} = \sigma (w_{i} x_{t} + U_{i} h_{t - 1} + b_{i} )$$
(6)
$$c_{t} = f_{t} \odot c{}_{t - 1} + i_{t} \odot c_{t}$$
(7)
$$O_{t} = \sigma (w_{0} x_{t} + U_{0} h_{t - 1} + b_{0} )$$
(8)
$$h_{t} = O_{t} \odot \tanh (c_{t} )$$
(9)
where the parameters
\(w_{i}\)
,
\(w_{c}\)
,
\(w_{f}\)
,
\(w_{o}\)
,
\(U_{i}\)
,
\(U_{f}\)
,
\(U_{c}\)
and
\(U_{O}\)
represent weight vectors.
\(x_{t}\)
,
\(h_{t}\)
and
\(c_{t}\)
indicate the input, hidden state and cell state at a time step
\(t\)
.
\(h_{t - 1}\)
and
\(c_{t - 1}\)
indicate the previous hidden state and the previous cell state at a time step
\(t - 1\)
. Moreover,
\(\widehat{{c_{t} }}\)
determines the candidate memory whereas the symbol
\(\odot\)
determines the element wise dot product.
\(b_{f}\)
,
\(b_{c}\)
,
\(b_{O}\)
and
\(b_{i}\)
are considered bias terms.
\(\sigma\)
and
\(\tanh\)
refers to the sigmoid and the
\(\tanh\)
activation functions.
Fig. 10
Architecture of LSTM cell
Full size image
Fig. 11
The function of main LSTM gates
Full size image
Gated recurrent unit
Rather than LSTM, GRU is another network that handles the vanishing gradient problem by introducing the reset and update gate. Despite being similar to LSTM, GRU has few advantages. Since GRU has fewer variables than LSTM, its architecture is simpler and more compact. The structure of the GRU cell is given in Fig.
12
. These gates determine which information is retained throughout the network. The mathematical model of the GRU cell can be formulated as:
$$r_{t} = \sigma (w_{r} x_{t} + U_{r} h_{t - 1} + b_{r} )$$
(10)
$$z_{t} = \sigma (w_{z} x_{t} + U_{z} h_{t - 1} + b_{z} )$$
(11)
$$\widehat{h}_{t} = \tanh (w_{h} x_{t} + U_{h} h_{t - 1} + b_{h} )$$
(12)
$$h_{t} = z_{t} \odot h_{t - 1} + (1 - z_{t} ) \odot \widehat{h}_{t}$$
(13)
where
\(r_{t}\)
and
\(z_{t}\)
indicate both the reset gate and the update gate.
\(w_{r}\)
,
\(w_{z}\)
,
\(w_{h}\)
,
\(U_{r}\)
,
\(U_{z}\)
and
\(U_{h}\)
refer to the weight vectors for input and hidden states.
\(b_{r}\)
,
\(b_{z}\)
and
\(b_{h}\)
represent bias terms.
\(h_{t}\)
and
\(\widehat{h}_{t}\)
are the hidden state and the candidate one at a time step
\(t\)
where as
\(h_{t - 1}\)
is the previous hidden state.
\(\sigma\)
and
\(\tanh\)
are the sigmoid and the
\(\tanh\)
activation functions.
Fig. 12
Architecture of GRU cell
Full size image
Temporal convolutional network
TCNs [
84
,
85
] are a powerful tool for handling sequence data, and it offers several benefits over traditional sequence models, such as RNN and LSTM. Figure
13
exhibits the architecture of TCN. TCNs are parallelizable networks and they can avoid gradient issues (vanishing gradients and exploding gradients). Also, they can handle sequences of varying lengths even long-term and short-term which makes it a versatile choice for many time series and sequence-based tasks. Unlike other convolutional networks, TCN employs the causal and dilated convolutions to handle the increased network depth for longer inputs [
86
]. The dilation factor (d = 1, 2, 4) is generally doubled at each layer. TCNs use 1D convolutional layer, where each layer in the network sees all previous layers' outputs. The TCN model also contains a residual block structure, similar to ResNet [
87
,
88
], for making a skip connection and to facilitate training of deep networks and prevent vanishing gradient descent.
Fig. 13
Architecture of TCN
Full size image
Echo state networks
ESN [
89
,
90
] is a type of RNN. It consists of an input layer (
\(u_{t}\)
), a reservoir layer (
\(w_{res}\)
) and an output layer (
\(y_{t}\)
), as depicted by Fig.
14
. The reservoir layer comprises a large number of recurrently connected nodes. In the figure,
\(w_{in}\)
refers to the connection weights between input and hidden layers. Additionally,
\(w_{out}\)
indicates the connection between the hidden layer and the output layer. The connection weights between the input layer and the reservoir layer remain unchanged once they have been initialized [
91
]. Only the weights of the output layer can be trained that simplifies the training process reducing computational complexity. ESN is beneficial for time series forecasting because it can successfully extract temporal dynamics and nonlinear patterns from data.
Fig. 14
Architecture of ESN
Full size image
Residual neural network
Residual networks [
92
] are characterized by skipping connections or jumping over some layers (see Fig.
15
). The residual connection performs an identity mapping to
\(x\)
, then the element-wise addition will be done
\(x + F(x)\)
. After that, the ReLU activation function will be applied to
\(x + F(x)\)
. If
\(x\)
and
\(F(x)\)
has the same dimension, the element-wise addition will be used. Otherwise, the identity mapping with a linear transformation will be done as
\(w.x + F(x)\)
, where
\(w\)
is a weight matrix. They allow gradients to directly flow through a network, without passing through non-linear activation functions that make neural networks to fall into the gradient vanishing problem.
Fig. 15
Residual connection
Full size image
CNN-LSTM
A CNN-LSTM [
93
] is a hybrid model that incorporates CNN layers at the beginning of CNN-LSTM model to extract relevant features from input data. These features are then passed to the subsequent LSTM layers for interpreting the features across time steps [
94
]. The architecture of the CNN-LSTM model is depicted in Fig.
16
, containing a fully connected layer of large number of neurons, organized in three layers: input, dense and output.
Fig. 16
Architecture of the hybrid CNN-LSTM model
Full size image
Machine learning algorithms
This subsection presents some of the most popular ML algorithms like linear regression [
95
], Stochastic Gradient Descent (SGD) regression [
96
], Least Absolute Shrinkage and Selection Operator (LASSO) [
97
], random forest [
98
], gradient boosting regression [
99
], decision tree regression [
100
] and K-Nearest Neighbor (KNN) regression [
101
]. These regression models are regarded as popular tool for statistical analyses and they will be explained below.
Linear regression
Linear regression [
102
,
103
] is a particular type of supervised ML algorithm employed for tackling regression issues and estimating the correlation between two both variables. It postulates a linear relationship between the independent variable (feature) and the dependent one (target) with the objective of finding the best fit line that expresses the relationship. The line is found by lessening the sum of squared differences between the real values and predicted ones. The generalized formulation for linear regression is:
$$\hat{y} = X\beta + b$$
(14)
$$\beta = (X^{T} X)^{ - 1} X^{T} y$$
(15)
$$b = mean(y) - mean(X\beta )$$
(16)
$$\hat{y}_{new} = X_{new} \beta + b$$
(17)
where
\(\hat{y}\)
indicates the predicted value.
\(X\)
denotes the matrix of input features.
\(\beta\)
defines a vector of weights or coefficients.
\(b\)
is the intercept term (bias).
\(X_{new}\)
refers to the new data.
\(\hat{y}_{new}\)
is the new predicted Value. The used weights vector for linear regression is
\(\beta\)
= [134.77742582, 135.5658808, 32.3798911, -13.60180839, 14.06928621, -18.5702262, -1.53548565, 46.40629438, 11.85316545], while the intercept value (
\(b\)
) is equal to 315.6134357765278.
Stochastic gradient descent regression
One of the supervised ML algorithms that may be used to tackle regression issues is called SGD regression [
104
]. The iteratively updating of the model weights using a small random portion of the training data instead of the entire draining data makes SGD a common choice for large-scale regression tasks.
Least absolute shrinkage and selection operator
LASSO regression [
105
] is regarded as an extension of Ordinary Least Square (OLS). With OLS, the estimates may suffer from high variance and lack of interpretation due to the presence of large number of predictors [
106
]. Hence, LASSO comes to handle the aforementioned drawbacks of OLS. It is used for regression problems with more accurate prediction. The main objective of LASSO regression is to discover the values of the coefficients that reduce the summation of the squared differences between the actual values and predicted values [
107
]. LASSO function adds a penalty term to the loss function of OLS equal to the absolute value of the weights associated with each feature variable. The loss function for LASSO regression can be expressed as below:
$$LASSO_{loss} = OLS_{loss} + Penalty\_term = \sum\limits_{i = 1}^{m} {(y_{i} - \sum\limits_{j = 1}^{p} {\hat{y}_{ij} w_{j} } )^{2} + \alpha \sum\limits_{j = 1}^{p} {|w_{j} |} }$$
(18)
where
\(m\)
and
\(p\)
refers to the number of observations and dimensionality of features.
\(\alpha\)
is a hyperparameter that indicates the strength of the regularization.
\(w_{j}\)
are the assigned weights for LASSO. L1 regularization moves any weight values to zero to allow other coefficients to take non-zero values.
\(y_{i}\)
denotes the
\(i^{th}\)
actual value and
\(\hat{y}_{ij}\)
represents the
\(i^{th}\)
predicted value of feature
\(j\)
. The used LASSO weights vector in our work is [134.78748341, 135.60866578, 32.32577104, −13.64606087, 12.66907409, −17.07588968, −1.32131259, 46.09668833, 11.72830064] and LASSO Intercept is equal to 315.6143943704034.
Random forest
Random forest [
108
] is an ensemble technique that has the ability for solving both regression and classification problems with the use of multiple decision trees. It supports the bagging technique where each decision tree is trained on a random subset of the training data. It has gained a significant popularity in recent years because of its impressive performance, simplicity in implementation and little processing requirements [
109
]. It is used to deal with bias-variance trade-offs for reducing the variance of a prediction model.
Random forests [
110
,
111
] involve creating multiple decision trees during training and then outputting the average prediction (in the case of regression) of the individual trees. Each tree is trained on a random subset of the data and features, which helps to reduce overfitting and improve generalization. The pooling of multiple trees helps stabilize predictions and increase accuracy. Additionally, this approach provides better interpretability and faster training times in many practical applications. The random forest approach contains three parameters to adjust:
Number of estimators
represents the number of trees. Increasing the number of estimators can lead to higher accuracy, but it is computationally expensive.
Maximum features
indicate the number of features for making a split decision.
Maximum depth of a tree
indicates how deep the tree can grow. The deeper the tree, the more complex it is.
Gradient boosting regression
Gradient boosting regression [
112
] is a supervised ML problem based on the idea of an ensemble method derived from a decision tree. It seeks to minimize the loss function of the gradient boosting regression model through the addition of many weak learners using gradient descent. Theses decision trees are built in a greedy approach with split points that minimize the function loss.
Decision tree regression
A decision tree regression is a ML algorithm that aims to predict a continuous target that partitions the features/variables into regions and passing predictions to each region based on feature values. It works by constructing a tree-based structure by recursively splitting the feature space based on input values. The leaves reflect the values of target variables values, while the branch lines represent the combinations of input variables that result in these values [
113
]. It predicts continuous target variables by traversing the tree and assigning values to unseen data points.
KNN regression
KNN is another ML algorithm that is highly recommended for regression and classification tasks. The model constructs its prediction based on finding the K nearest data points to a particular input by averaging the observations in the same neighborhood or choosing the majority class [
26
]. The distance between the neighbors and the given data can be calculated using Euclidean distance. The Euclidean distance (
ED
) between two points
\(x = (x_{1} ,x_{2} ,...,x_{d} )\)
and
\(y = (y_{1} ,y_{2} ,...,y_{d} )\)
in a
\(d\)
-dimensional space is calculated as:
$$ED_{x,y} = \sqrt {\sum\limits_{j = 1}^{d} {(x_{j} - y_{j} )^{2} } }$$
(19)
The KNN is built by K neighbors and each of the neighbors is assigned a weight based on its distance to the query point. The closer a neighbor, the higher its weight is. The weight of the
\(i^{th}\)
neighbor (
\(w_{i}\)
) is given by:
$$w_{i} = \frac{1}{{ED_{{x,y_{i} }} }}$$
(20)
\(ED_{{x,y_{i} }}\)
is the distance between the query point
\(x\)
and its
\(i^{th}\)
neighbor
\(y_{i}\)
. The predicted value (
\(\hat{y}\)
) for the query point is the weighted average of the target values of its nearest neighbors that can be computed by:
$$\hat{y} = \frac{{\sum\limits_{k = 1}^{K} {w_{k} .y_{k} } }}{{\sum\limits_{k = 1}^{K} {w_{k} } }}$$
(21)
\(y_{k}\)
is the target value of the
\(k^{th}\)
neighbor, where
\(k = 1,2,...,K\)
.
Interpretable results
Explainable Artificial Intelligence (XAI) allows anyone to understand the logic or reasons produced by the ML model. It aims to develop AI systems that demonstrate more transparency in their decision-making processes. The level of interpretability of a model directly correlates with the ease of understanding the underlying justifications for certain judgments or projections. The ML model is more interpretable than another if its explanations are more understandable to a person than the choices made by the other model. Understanding the underlying reasoning behind a model's prediction is crucial for the widespread adoption of prediction in many applications.
Shapley additive explanations
SHapley Additive exPlanations (SHAP) [
114
] utilizes the principles of game theory to elucidate the predictions of any ML model by quantifying the impact of each feature on the prediction output. The SHAP method identifies the primary features that influence the model's forecast. SHAP approximates the original model to a specific input and reduces the impact of missing features as follows:
$$g(z{\prime} ) = \varphi_{o} + \sum\limits_{j = 1}^{m} {\varphi_{j} z_{j}{\prime} }$$
(22)
where
\(g\)
represents the explanation model.
\(z{\prime} \in \{ 0,1\}^{m}\)
is a binary variable where 1 indicates that the simplified features are same as the original and 0 indicates that the features are not the same as the original. Moreover,
\(\varphi_{j}\)
refers to the attribute effect for the
\(j^{th}\)
feature such that
\(j = 1,...,m\)
and
\(m\)
determines the number of simplified features.
Local interpretable model-agnostic explanation
The Local Interpretable Model-Agnostic Explanation (LIME) [
115
] serves as a "explainer" to elucidate the prediction outcome for individual data samples. The LIME output consists of interpretations that depict the contribution of each feature to the prediction for a specific sample, serving as a means of providing local interpretability.
Results analysis and discussion
This section will explore the solar irradiance dataset and the results of different models from ML and DL on this data. SubSect. "
Environment setup
" will present the configuration of the experimental environment in which our tests are conducted. The characteristics of the solar irradiance dataset are outlined in the following subSect. "
Dataset description
". SubSect. "
Dataset distribution
" explains the distribution of data. The preprocessing steps and feature selection are discussed in subSect. "
Data preprocessing and feature selection
". An illustration of the data that has been divided into training, testing, and validation may be seen in subSect. "
Data split
". SubSect. "
Hyperparameters tuning
" introduces the GSCV to optimize the hyperparameters of various ML and DL models. SubSect. "
Evaluation metrics of algorithms for solar irradiance forecasting
" presents the models evaluation for checking their performance. Finally, the results of DL and ML models are discussed and statistically analyzed in subSect. "
Results for deep learning models
" and subSect. "
Results for machine learning algorithms
", respectively.
Environment setup
For fair comparison among all algorithms, they are implemented and run on the same data taken from Kaggle website. All algorithms are trained with GPU NVidia Tesla P100 and RAM of 16 GB. They are developed using Python environment of Version 3.10.12.
Dataset description
This study utilizes global horizontal solar irradiance data acquired in Islamabad, located at 33.64°N, 72.98°E and 500 m above sea level with meteorological parameters. The data was gathered over five years from 2015 to 2019 taken at hourly intervals by employing accurate meteorological devices. The weather data contains 14 columns. The dataset comprises 41256 samples. The characteristics and specifications of the data are listed in Table
2
.
Dataset distribution
To understand the distribution of data, we employ several statistical measures, including mean (
\(Mean\)
) and standard deviation (
\(Std\)
) that are calculated as in Eq. (
23
) and Eq. (
24
), respectively. Furthermore, box plots [
116
] are utilized to summarize data distributions, detect any skewness, identify outliers, and make comparisons between distributions. They offer five measures, including the minimum (
Min
), first quartile, median, third quartile, and maximum (
Max
) of the data values, that help to identify data spread and identify potential anomalies.
$$Mean = \frac{{\sum\limits_{i = 1}^{n} {x_{i} } }}{n}$$
(23)
$$Std = \sqrt {\frac{{\sum\limits_{i = 1}^{n} {x_{i} - Mean} }}{n}}$$
(24)
\(x_{i}\)
is the
\(i^{th}\)
observation in data, where
\(1 \le i \le n\)
.
\(n\)
indicates the number of observations. Table
3
introduces the statistical analysis of data distribution. From the table, the dataset has different distributions, where some variables have a wide range of values with significant variations, such as GHI and DNI, while other variables have smaller ranges and lower variations, such as T_amb, RH, WS, WS_gust, WD, WD_std, and BP. The variables dni_dev and cleaning have a substantial number of zero values.
Table 3 The statistical analysis on the solar irradiance dataset distribution
Full size table
Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while retaining the variation in the data. This is particularly useful when dealing with high-dimensional data to make it easier to visualize and analyze, without losing significant information. From Fig.
17
, it is evident that the cumulative explained variance plateaus after 9 components. This means that while PCA can reduce the dimensions, the first 9 components or features already capture most of the variability in the data. Since our data originally contains 9 features, which is the total number of features in the original dataset, would not provide any dimensionality reduction benefits.
Fig. 17
PCA reduction technique for the weather data
Full size image
Furthermore, Fig.
18
displays the importance of each feature. A higher importance value means that the specific feature will have a larger effect on the model that is being used to predict the target value GHI. The input DHI values have a significant effect on the target GHI with an importance value of 1.441757. Moreover, Fig.
19
shows the relationship between each feature and the other features. Figure
20
depicts the
entropy
[
117
] for the data samples over time, which takes a value from 0 to 1. A higher
entropy
value indicates a more uncertain or random distribution, whereas a lower
entropy
value corresponds to a more predictable or deterministic distribution. By inspecting the figure, we can see that most entropy values lies in the interval from 0.4 to 0.8. When the entropy is moving toward 1, it means that the dataset has a random distribution, which puts more challenges in predicting the solar irradiance.
Fig. 18
The importance of each feature of the weather data
Full size image
Fig. 19
The relationship between each feature and the other features
Full size image
Fig. 20
Entropy for the data samples over time
Full size image
Data preprocessing and feature selection
Data preprocessing and feature selection ensure data quality, data normalization, and noise reduction. Data distribution shows that dni_dev and cleaning features have many zero values. Hence, dni_dev and cleaning is dropped from data, as well as unnamed: 0 is dropped as it contains the index of samples. Also, the contribution and correlation of these features to the output is weak, thus deleting them will be advantageous. From the Person correlation plot in Fig.
21
, the darker degree of red and blue colors indicates a strong correlation, while the brighter one indicates weak correlation. Finally, the remaining features that will be taken into consideration are GHI, DNI, DHI, T_amb, RH, WS, WS_gust, WD, WD_std and BP.
Fig. 21
The heatmap correlation of the features
Full size image
The analysis reveals that a large number of values, approximately 19,403 from GHI are equal to zero. Indeed, the zero values represent night values, and it is clear that they are zero owing to the absence of solar radiation. Thus, it is advisable to clean the data at night from 19:00 to 5:00 in the morning and only utilize data from the daylight period when solar radiation is present. Zero values will be dropped to 4236 after this process.
Since there are several data distributions—wide range and small range—data normalization is crucial for eliminating bias results, creating features of a comparable scale, enhancing ML algorithm stability and convergence, improving model accuracy and generalizability by minimizing outliers, and improving efficiency by minimizing complexity and storage needs. The standard scalar normalization (
\(z\)
) of data can be calculated by:
$$z = \sqrt {\frac{{x_{i} - Mean}}{Std}}$$
(25)
where
\(x_{i}\)
refers to the
\(i^{th}\)
value from data to be normalized.
\(Mean\)
and
\(Std\)
are computed as in Eq. (
24
) and Eq. (
25
), respectively.
From the statistical analysis of the data nature, input data contains DNI, DHI, T_amb, RH, WS, WS_gust, WD, WD_std and BP that has a high correlation with the target data (GHI) with a window size of 4 for neural networks. The window size in a neural network is crucial for effectively capturing local information, compressing information, extracting attributes at different scales, and achieving a balance between local and global information.
Data split
After the normalization step, the data split phase starts, as depicted in Fig.
22
. For ML algorithms, the data is divided into training and testing sets for the standard training and evaluation process. However, for DL algorithms, it is common to split the data into training, testing, and validation sets. We use the training set to train the DL model, the testing set to assess its performance, and the validation set for hyperparameter tuning and model selection. This helps in choosing the best hyperparameters for the model and comparing the performance of different models due to the complexity of DL algorithms compared to ML ones. The validation set is unnecessary for ML algorithms because they have simple model architectures in contrast to DL models that have more complicated structures.
Fig. 22
Data split for DL and ML algorithms
Full size image
For the DL algorithms the data is divided into three sets: training, validation and testing, with 70%, 15%, and 15%, respectively as adopted by many previous studies [
43
,
118
,
119
,
120
]. For ML approaches, the data is divided into two sets: training and testing, with 20,000 records of the data for the training process and 5,785 records for the testing process [
36
].
Hyperparameters tuning
GSCV [
121
,
122
] is a powerful technique for optimizing the hyperparameters of DL models. Cross-validation is a crucial method used to build better-fitting models by training and testing on all parts of the training dataset. The grid search with fivefold cross-validation [
123
,
124
] is utilized on the training and validation sets and the testing set is kept away for final evaluation. The number of folds can be increased or decreased. A higher number of folds value may lead to a less biased model, and more complex model. A lower number of folds value may like the simple train-test split method. fivefold cross validation [
125
,
126
] is adopted in many previous works. In GSCV, the training set is partitioned into five equal-sized folds or parts. During the five iterations, one fold is used for testing while the remaining folds are used for training. This process is repeated until all folds have been used for testing.
Figure
23
depicts the used grid search with fivefold cross-validation for DL algorithms. For fair comparison among all DL algorithms, it has been taken into account that the models are trained and validated on the same parts of the data, as can be seen by the figure. Also, the grid search cross-validation is performed on the training set for better hyperparameters tuning, as can be illustrated in Fig.
24
. Moreover, each algorithm is trained and tested on the same parts of the data, as in the figure.
Fig. 23
Grid search cross-validation for DL algorithms
Full size image
Fig. 24
Grid search cross-validation for ML algorithms
Full size image
Evaluation metrics of algorithms for solar irradiance forecasting
Various performance measures are employed for evaluating all the models, including Mean Squared Error (
MSE
),
R
2
score
,
Adjusted R
2
score
, Median Absolute Deviation (
MAD
), Root Mean Squared Error (
RMSE
), Normalized Root Mean Squared Error (
NRMSE
) and Mean Absolute Error (
MAE
).
MSE
is a primary performance metric that measures the average of the squares of errors and computed by:
$$MSE = \frac{{\sum\limits_{i = 1}^{n} {(y_{i} - \widehat{y}_{i} )^{2} } }}{n}$$
(26)
\(n\)
indicates the number of the samples and
\(y_{i}\)
indicates the
\(i^{th}\)
actual value, where
\(i = 1,...,n\)
.
\(\widehat{{y_{i} }}\)
represents the
\(i^{th}\)
predicted value.
R
2
score is a statistical indicator that indicates how closely is the regression predictions from the actual data points. The 1.0 value means that the regression predictions completely fit the actual data. When the
R
2
score
value moves from one to zero, the predictions move away from the actual data.
R
2
score
can be calculated by:
$$R^{2} (\% ) = 1 - \frac{{\sum\limits_{i = 1}^{n} {(\widehat{y}_{i} - \overline{y}_{i} )^{2} } }}{{\sum\limits_{i = 1}^{n} {(y_{i} - \overline{y}_{i} )^{2} } }} \times 100$$
(27)
\(\overline{y}\)
represents the mean of all the values.
R
2
score
measures the overall fit of the model to the data and has a tendency to increase when additional independent variables are added to the model, even if they have little or no explanatory power. This can lead to overfitting, while
adjusted R
2
score
considers the goodness-of-fit while taking into account the number of predictors and avoids overfitting. It can be computed as:
$$Adjusted{\mkern 1mu} R^{2} = 1 - \left\lceil {\frac{{(1 - R^{2} )(n - 1)}}{{(n - k - 1)}}} \right\rceil$$
(28)
where
\(k\)
represents the number of independent variables in the model.
RMSE
is a common performance metric measures the average difference between predicted values and real values. A lower
RMSE
indicates a superior model and more accurate predictions. The lower
RMSE
determines that there is a small difference between the real and the predicted values.
RMSE
can be formulated mathematically as follows [
127
]:
$$RMSE(w/m^{2} ) = \sqrt {\frac{{\sum\limits_{i = 1}^{n} {(y_{i} - \widehat{{y_{i} }})^{2} } }}{n}}$$
(29)
The Normalized
RMSE
(
NRMSE
) is an extension for
RMSE
which is more suitable for different scales of data and can be calculated as follows:
$$NRMSE = \frac{RMSE}{{y_{\max } - y_{\min } }}$$
(30)
\(y_{\max }\)
and
\(y_{\min }\)
determine the maximum and minimum actual values. Another metric to evaluate the models is
Mean Absolute Deviation (MAD)
.
MAD
is mainly defined as the median difference between the observations (actual values) and model output (predictions). It can be expressed as:
$$MAD = Median(|y_{i} - \widehat{y}|)$$
(31)
\(Median\)
is the midpoint of a data collection; half of the data points have values lower than or equal to it, and half have values higher or equal to it.
MAE
is defined as the average variance between the real and predicted values and computed using Eq. (
32
) [
127
].
$$MAE(w/m^{2} ) = \frac{1}{n} \times \sum\limits_{i = 1}^{n} {|y_{i} - \widehat{{y_{i} }}|}$$
(32)
Results for deep learning models
In this subsection, we will investigate the performance of many DL algorithms for tackling SIF problem. To ensure a fair comparison among all DL algorithms, they were trained under identical environmental circumstances. We used Keras API of version 2.12.0 which is a Python framework for deep learning to implement all our selected DL models. Keras is built on top of TensorFlow and offers a comprehensive set of tools for constructing and optimizing neural networks. The loss function for each model is also determined by squaring the mean error, as shown in Eq. (
26
). The Adam optimizer is utilized with all DL except for the DNN model, we use rmse optimizer. Table
4
records the best learning rate values obtained by the grid search cross validation for each DL model. The batch size is defined as 32, indicating the quantity of samples processed prior to updating the model. Moreover, the window size is equal to 4. The architecture of various DL models is stored in Table
5
.
Table 4 Learning rate for each DL model
Full size table
Table 5 The architecture of different DL models
Full size table
The number of epochs is set to 100, where it represents the number of complete iterations through the training dataset. Early stopping is triggered if a fault arises during training or if there is no improvement in the model's validation performance for a specific period of epochs (
\(patience = 5\)
). The parameter
\(patience\)
represents the number of epochs with no improvement. The kernel size for CNN and CNN-LSTM is equal to 3. Additionally, the ReLU activation function is utilized by the hidden layer, whilst the linear activation function is utilized by the output layer. Furthermore, a lower dropout rate is set to 0.2 in order to prevent overfitting.
Table
6
records the results of various DL models, including ANN, CNN, RNN, LSTM, GRU, TCN, ESN, Residual NN, MLP and CNN-LSTM. We employ many performance metrics to assess the model's quality, such as the number of parameters, the
R
2
score
,
\(Adjusted R^{2} score\)
,
MSE
,
RMSE
,
NRMSE, MAE
and
MAD
. The bold font in the table indicates the best results obtained by the existing models. From the results, we can see that CNN-LSTM comes in the first rank by achieving the maximum
Adjusted R
2
score
value of 0.984, which means the regression predictions are more closely aligned with the actual data than other comparing models. Also, CNN-LSTM obtains the minimum values for
NRMSE
= 0.036 and
MSE
= 1265.721, and the second minimum values for
MAE
= 16.461 and
MAD
= 8.498. The CNN achieved the second ranking compared to its counterparts with
Adjusted R
2
score
of 0.982. On the other side, TCN comes in third rank with
Adjusted R
2
score
= 0.981 and has the largest number of parameters with value of 2,176,257. Moreover, RNN comes in the fourth rank with
Adjusted R
2
score
= 0.978, followed by LSTM, Residual NN, GRU, ANN, MLP and ESN. The number of parameters affects the model's complexity. MLP has the least number of parameters equal to 701.
Table 6 The results of the DL models for solar irradiance forecasting
Full size table
Figure
25
displays the rank of each model using five performance metrics, including the
Adjusted R
2
score
,
MSE, NRMSE, MAE
, and
MAD
. It can be seen that the CNN-LSTM model achieves the best results compared to their rivals. Furthermore, ESN has the worst performance. Figure
26
shows the sum of ranks over the previous five metrics to give a general view on the model ranking for solar radiance forecasting. The rank of DL models from the best to the worst comes as follows: CNN-LSTM, TCN, CNN, RNN, GRU, LSTM, Residual NN, MLP, ANN and ESN.
Fig. 25
The rank of each DL model using adjusted R
2
score, MSE, NRMSE, MAE and MAD metrics
Full size image
Fig. 26
The sum of ranks of each DL model
Full size image
Figure
27
describes the hybrid CNN-LSTM model for predicting solar irradiance. The figure depicts that the model contains two convolution layers, one dropout layer, one pooling layer, two LSTM layers and one dense layer. Presentation of the CNN-LSTM model's training and validation loss curves can be found in Fig.
28
. According to the figure, the CNN-LSTM has varying numbers of epochs for the different folds due to the implementation of the early stopping parameter. This model terminated the training process if the loss in both training and validation did not change during model fitting. It is clear that the loss values of the training set and the validation set for all folds are in good agreement with one another. This suggests that the predictions made by the proposed model are in line with the actual data, and that there is neither overfitting nor underfitting present. Figure
29
depicts the predictions of CNN-LSTM model compared to the actual ones for the first 100 time steps.
Fig. 27
CNN-LSTM model summary
Full size image
Fig. 28
The loss curve for training and validation set for all five folds of CNN-LSTM model
Full size image
Fig. 29
CN-LSTM predictions
Full size image
Results for machine learning algorithms
This section examines the performance of several ML models, such as linear regression, SGD regression, LASSO, gradient boosting regression, random forest, decision tree regression, and KNN regression. Table
7
contains the best parameters values of different parameters associated with each ML model that are found using the grid search cross validation technique. By inspecting the table, we display the various values of each parameter for all ML models. Also, the best hyperparameters values are recorded. We used Sci-kit API of version 1.2.2 which is a Python framework for ML to implement all our selected ML models.
Table 7 The best hyperparameters values found by grid search cross validation of each ML model
Full size table
Table
8
presents a comparison among numerous ML algorithms, including linear regression, SGD regression, LASSO, random forest, gradient boosting regression, decision tree regression and KNN regression. The table shows that the gradient boosting regression gets the best results for most of the performance metrics. Gradient boosting regression has the highest
Adjusted R
2
score
with a value of 0.962, which puts it at the top of the algorithms. It also obtains the minimum values for MSE = 3415.02, NRMSE = 0.058 and MAE = 29.33. KKN regression attains the second highest value of the
Adjusted R
2
score
, which is 0.945. Moreover, linear regression comes in the third rank with an
Adjusted R
2
score
value of 0.893.
Table 8 The results of ML algorithms
Full size table
Figure
30
displays the ranking of the existing seven ML approaches according to five distinct evaluations: Adjusted R
2
score, MSE, NRMSE, MAE, and MAD. We can observe that gradient boosting regressions attains the first ranking in five metrics, while SGD shows the worst performance. Also, Fig.
31
displays the ranking of several algorithms based on the total sum of rankings for all performance metrics. A lower sum of ranks indicates that the ML model performs better. The gradient boosting regression has a minimum value of the sum of ranks with a value of 5. Then, KNN comes in at the next rank after gradient boosting regression. SGD reserves the last rank with a value of 26, which demonstrates poor performance. Moreover, Fig.
32
depicts the predictions obtained by gradient boosting regression for the first 100 time steps, which has proven that the predicted values of gradient boosting regression are closely aligned with the actual values.
Fig. 30
The rank of each ML model using
Adjusted R
2
score
,
MSE
,
NRMSE
,
MAE
and
MAD
metrics
Full size image
Fig. 31
The sum of ranks of each ML model
Full size image
Fig. 32
Gradient boosting regression predictions
Full size image
To explain the previous results obtained by ML models and the outperformance of gradient boosting regression, XAI with SHAP and LIME are introduced. Mean absolute SHAP values are often shown as bar plots that order features according to their importance, as seen in Fig.
33
. The ordering of features and the respective magnitudes of the mean absolute SHAP values are the most important factors to consider. Here, we can see that DHI is the most influential feature contributing to the GHI output, whereas the least informative feature contributing to the GHI output is WS_gust.
Fig. 33
Mean absolute values of the SHAP for all features
Full size image
The bar plot in Fig.
33
did not tell us how the underlying values of each feature relate to the model's predictions. Figure
34
illustrates how the features contribute to the model's predictions. Each feature of the data is represented by a row in the plot. The horizontal
x
-axis indicates the SHAP values. The blue color refers to lower values of the feature, whereas the red color refers to higher values of this feature. The feature that has a wide spread of SHAP values, has a greater contribution to the model’s predictions. The feature values that are assembled around zero have a minimal impact on the model’s predictions. From the plot, we can see that the lower values of DHI have negative SHAP values, whereas the higher values of DHI have positive SHAP values. Also, DHI has a wide spread of SHAP values, which reflects the significant effect on the model's predictions. LIME plot in Fig.
35
shows that DHI and DNI have the most impact for output.
Fig. 34
Summary of beeswarm plot ranked by mean absolute SHAP value
Full size image
Fig. 35
LIME plot
Full size image
Conclusions and future works
Various DL and ML algorithms were employed for SIF. This paper presents a comparison for the most common nine DL algorithms, such as ANN, CNN, LSTM, GRU, RNN, TCN, ESN, CNN-LSTM MLP and residual NN. Another experiment is conducted to compare among seven of the existing algorithms, containing linear regression, SGD regression, LASSO, random forest, decision tree regression and KNN regression. The GSCV is employed to find the best hyperparameter values for all DL and ML models. The dataset was gathered from Islamabad over five years at hourly intervals using accurate meteorological instruments. Moreover, we measure the effectiveness of each model using various metrics like MSE,
Adjusted R
2
score
, RMSE, MAE and MAD. Additionally, SHAP and LIME are used in our study to provide an explanation and comprehension for the acquired result of the best model. For deep learning algorithms, the statistical analysis shows that CNN-LSTM outperforms its counterparts, whereas gradient boosting regression achieves a superior results compared to other ML models.
Hybridization can be a powerful tool, as hybrid models can achieve better results than individual ones. Hence, in the future, we hope to develop a hybrid model for solar irradiance forecasting. Hyperparameter tuning strategies seek to find the optimal values of parameters that lead to improved performance and the learning process of ML algorithms. Other hyperparameter tuning can be a future direction through the integration of metaheuristic techniques to optimize the parameter values [
128
,
129
]. Possible future directions to enhance the work would revolve mainly around different time horizons: short-term, medium-term, and long-term, to study the effectiveness of the proposed model. This study is interested in solar irradiance forecasting, and we aim at investigating other challenging problems, such as wind forecasting [
130
], PV power forecasting [
131
] and price forecasting [
132
].
Data availability
https://www.kaggle.com/datasets/ahmdtolba/solardata-islamabad/data
. No datasets were generated or analysed during the current study.
References
Steg L. Psychology of climate change. Annu Rev Psychol. 2023;74:391–421.
Article
Google Scholar
Eckardt NA, et al. Climate change challenges, plant science solutions. Plant Cell. 2023;35(1):24–66.
Article
Google Scholar
Mirón IJ, Linares C, Díaz J. The influence of climate change on food production and food safety. Environ Res. 2023;216: 114674.
Article
Google Scholar
Matthews T. Humid heat and climate change. Prog Phys Geog Earth Environ. 2018;42(3):391–405.
Article
Google Scholar
Lee, H., et al., Climate change 2023: synthesis report. Contribution of working groups I, II and III to the sixth assessment report of the intergovernmental panel on climate change. 2023.
Sain K. Climate change and fossil fuels: impacts, challenges and plausible mitigation. J Geol Soc India. 2023;99(4):454–8.
Article
Google Scholar
Ozdemir I, et al., COP28
.
2023.
Nijsse FJ, et al. The momentum of the solar energy transition. Nat Commun. 2023;14(1):6542.
Article
Google Scholar
Nassar YF, et al. Carbon footprint and energy life cycle assessment of wind energy industry in Libya. Energy Convers Manage. 2024;300: 117846.
Article
Google Scholar
Kamran M. Hydro energy renewable energy conversion systems. Amsterdam: Elsevier; 2021.
Google Scholar
Soltani M, et al. Environmental, economic, and social impacts of geothermal energy systems. Renew Sustain Energy Rev. 2021;140: 110750.
Article
Google Scholar
Obaideen K, et al. Solar energy: Applications, trends analysis, bibliometric analysis and research contribution to sustainable development goals (SDGs). Sustainability. 2023;15(2):1418.
Article
Google Scholar
Chinnasamy S, Arunachalam A. Experimental investigation on direct expansion solar-air source heat pump for water heating application. Renew Energy. 2023;202:222–33.
Article
Google Scholar
Madhankumar S, et al. A review on the latest developments in solar dryer technologies for food drying process. Sustain Energy Technol Assess. 2023;58: 103298.
Google Scholar
Kurbonov K. Use of renewable energy sources for heating buildings. Open Access Repos. 2023;4(03):349–54.
Google Scholar
Kumar SS, et al. Solar powered water pumping systems for irrigation: a comprehensive review on developments and prospects towards a green energy approach. Mater Today Proc. 2020;33:303–7.
Article
Google Scholar
Tuly S, et al. Effects of design and operational parameters on the performance of a solar distillation system: a comprehensive review. Groundw Sustain Dev. 2021;14: 100599.
Article
Google Scholar
Niyommaneerat W, Suwanteep K, Chavalparit O. Sustainability indicators to achieve a circular economy: a case study of renewable energy and plastic waste recycling corporate social responsibility (CSR) projects in Thailand. J Clean Prod. 2023;391: 136203.
Article
Google Scholar
Morey M, et al. A comprehensive review of grid-connected solar photovoltaic system: architecture, control, and ancillary services. Renew Energy Focus. 2023;45:307–30.
Article
Google Scholar
Guermoui M, et al. A comprehensive review of hybrid models for solar radiation forecasting. J Clean Prod. 2020;258: 120357.
Article
Google Scholar
Akhter MN, et al. Review on forecasting of photovoltaic power generation based on machine learning and metaheuristic techniques. IET Renew Power Gener. 2019;13(7):1009–23.
Article
Google Scholar
Bassous GF, Calili RF, Barbosa CH. Development of a low-cost data acquisition system for very short-term photovoltaic power forecasting. Energies. 2021;14(19):6075.
Article
Google Scholar
Nie Y, et al. Open-source sky image datasets for solar forecasting with deep learning: a comprehensive survey. Renew Sustain Energy Rev. 2024;189: 113977.
Article
Google Scholar
Morf H. A validation frame for deterministic solar irradiance forecasts. Renew Energy. 2021;180:1210–21.
Article
Google Scholar
Voyant C, et al. Machine learning methods for solar radiation forecasting: a review. Renew Energy. 2017;105:569–82.
Article
Google Scholar
Demir V, Citakoglu H. Forecasting of solar radiation using different machine learning approaches. Neural Comput Appl. 2023;35(1):887–906.
Article
Google Scholar
Ajith M, Martínez-Ramón M. Deep learning algorithms for very short term solar irradiance forecasting: a survey. Renew Sustain Energy Rev. 2023;182: 113362.
Article
Google Scholar
Kumari P, Toshniwal D. Deep learning models for solar irradiance forecasting: a comprehensive review. J Clean Prod. 2021;318: 128566.
Article
Google Scholar
Liu J, et al. Hourly stepwise forecasting for solar irradiance using integrated hybrid models CNN-LSTM-MLP combined with error correction and VMD. Energy Convers Manage. 2023;280: 116804.
Article
Google Scholar
Lu Y, et al. Predicting surface solar radiation using a hybrid radiative transfer-machine learning model. Renew Sustain Energy Rev. 2023;173: 113105.
Article
Google Scholar
Lara-Benítez P, et al. Short-term solar irradiance forecasting in streaming with deep learning. Neurocomputing. 2023;546: 126312.
Article
Google Scholar
Ferkous K, et al. A novel learning approach for short-term photovoltaic power forecasting-a review and case studies. Eng Appl Artif Intell. 2024;133: 108502.
Article
Google Scholar
Feng J, Wang W, Li J. An LM-BP neural network approach to estimate monthly-mean daily global solar radiation using MODIS atmospheric products. Energies. 2018;11(12):3510.
Article
Google Scholar
Praynlin, E. and J.I. Jensona. Solar radiation forecasting using artificial neural network. In 2017 Innovations in Power and Advanced Computing Technologies (i-PACT). 2017. IEEE.
Pazikadin AR, et al. Solar irradiance measurement instrumentation and power solar generation forecasting based on Artificial Neural Networks (ANN): a review of five years research trend. Sci Total Environ. 2020;715: 136848.
Article
Google Scholar
Haider SA, et al. Deep learning and statistical methods for short-and long-term solar irradiance forecasting for Islamabad. Renew Energy. 2022;198:51–60.
Article
Google Scholar
Sivaneasan B, Yu C, Goh K. Solar forecasting using ANN with fuzzy logic pre-processing. Energy Proc. 2017;143:727–32.
Article
Google Scholar
Kumar KR, Kalavathi MS. Artificial intelligence based forecast models for predicting solar power generation. Mater Today Proc. 2018;5(1):796–802.
Article
Google Scholar
Lima M, et al. MLP back propagation artificial neural network for solar resource forecasting in equatorial areas. Renew Energy Power Qual J (RE&PQJ). 2018;1:175–80.
Article
Google Scholar
Laopaiboon, T., et al. Hour-ahead solar forecasting program using back propagation artificial neural network. in 2018 international conference and utility exhibition on green energy for sustainable development (ICUE). 2018. IEEE.
Ledmaoui Y, et al. Forecasting solar energy production: a comparative study of machine learning algorithms. Energy Rep. 2023;10:1004–12.
Article
Google Scholar
Gairaa K, et al. Contribution of ordinal variables to short-term global solar irradiation forecasting for sites with low variabilities. Renew Energy. 2022;183:890–902.
Article
Google Scholar
Alzahrani A, et al. Solar irradiance forecasting using deep neural networks. Proc Comput Sci. 2017;114:304–13.
Article
Google Scholar
Qing X, Niu Y. Hourly day-ahead solar irradiance prediction using weather forecasts by LSTM. Energy. 2018;148:461–8.
Article
Google Scholar
Ashfaq Q, et al. Hour-ahead global horizontal irradiance forecasting using long short term memory network. In 2020 IEEE 23rd International Multitopic Conference (INMIC). 2020. IEEE.
Lee D, Kim K. Recurrent neural network-based hourly prediction of photovoltaic power output using meteorological information. Energies. 2019;12(2):215.
Article
Google Scholar
Michael NE, et al. Short-term solar irradiance forecasting based on a novel Bayesian optimized deep Long Short-Term Memory neural network. Appl Energy. 2022;324: 119727.
Article
Google Scholar
Eşlik AH, Akarslan E, Hocaoğlu FO. Short-term solar radiation forecasting with a novel image processing-based deep learning approach. Renew Energy. 2022;200:1490–505.
Article
Google Scholar
Bae KY, Jang HS, Sung DK. Hourly solar irradiance prediction based on support vector machine and its error analysis. IEEE Trans Power Syst. 2016;32(2):935–45.
Google Scholar
Huertas Tato J, Centeno Brito M. Using smart persistence and random forests to predict photovoltaic energy production. Energies. 2018;12(1):100.
Article
Google Scholar
Chen C-R, Three Kartini U. K-nearest neighbor neural network models for very short-term global solar irradiance forecasting based on meteorological data. Energies. 2017;10(2):186.
Article
Google Scholar
Kong X, et al. Multi-step short-term solar radiation prediction based on empirical mode decomposition and gated recurrent unit optimized via an attention mechanism. Energy. 2023;282: 128825.
Article
Google Scholar
Seepanomwan, K. Better Multi-step Time Series Prediction Using Sparse and Deep Echo State Network. In 2023 8th International Conference on Control and Robotics Engineering (ICCRE). 2023. IEEE.
Song, Z. and L.E. Brown. Multi-dimensional evaluation of temporal neural networks on solar irradiance forecasting. In 2019 IEEE Innovative Smart Grid Technologies-Asia (ISGT Asia). 2019. IEEE.
Azizi N, et al. Deep learning based long-term global solar irradiance and temperature forecasting using time series with multi-step multivariate output. Renew Energy. 2023;206:135–47.
Article
Google Scholar
Wang Y, et al. Adaptive learning hybrid model for solar intensity forecasting. IEEE Trans Industr Inf. 2018;14(4):1635–45.
Article
Google Scholar
Zang H, et al. Short-term global horizontal irradiance forecasting based on a hybrid CNN-LSTM model with spatiotemporal correlations. Renew Energy. 2020;160:26–41.
Article
Google Scholar
Kumari P, Toshniwal D. Long short term memory–convolutional neural network based deep hybrid approach for solar irradiance forecasting. Appl Energy. 2021;295: 117061.
Article
Google Scholar
Gao B, et al. Hourly forecasting of solar irradiance based on CEEMDAN and multi-strategy CNN-LSTM neural networks. Renew Energy. 2020;162:1665–83.
Article
Google Scholar
Huang X, et al. Hybrid deep neural model for hourly solar irradiance forecasting. Renew Energy. 2021;171:1041–60.
Article
Google Scholar
Malakouti SM, et al. Predicting wind power generation using machine learning and CNN-LSTM approaches. Wind Eng. 2022;46(6):1853–69.
Article
Google Scholar
Guermoui M, et al. Enhancing direct normal solar irradiation forecasting for heliostat field applications through a novel hybrid model. Energy Convers Manage. 2024;304: 118189.
Article
Google Scholar
Guermoui M, Boland J, Rabehi A. On the use of BRL model for daily and hourly solar radiation components assessment in a semiarid climate. Euro Phys J Plus. 2020;135(2):1–16.
Article
Google Scholar
Zhuhadar LP, Lytras MD. The application of AutoML techniques in diabetes diagnosis: current approaches, performance, and future directions. Sustainability. 2023;15(18):13484.
Article
Google Scholar
Abiodun OI, et al. State-of-the-art in artificial neural network applications: a survey. Heliyon. 2018.
https://doi.org/10.1016/j.heliyon.2018.e00938
.
Article
Google Scholar
Mfetoum IM, et al. A multilayer perceptron neural network approach for optimizing solar irradiance forecasting in Central Africa with meteorological insights. Sci Rep. 2024;14(1):3572.
Article
Google Scholar
Albawi, S., T.A. Mohammed, and S. Al-Zawi. Understanding of a convolutional neural network. In 2017 international conference on engineering and technology (ICET). 2017. IEEE.
Kasongo SM. A deep learning technique for intrusion detection system using a recurrent neural networks based framework. Comput Commun. 2023;199:113–25.
Article
Google Scholar
Staudemeyer RC, ER Morris, 2019 Understanding LSTM--a tutorial into long short-term memory recurrent neural networks. arXiv preprint
arXiv:1909.09586
.
Tandale SB, Stoffel M. Recurrent and convolutional neural networks in structural dynamics: a modified attention steered encoder–decoder architecture versus LSTM versus GRU versus TCN topologies to predict the response of shock wave-loaded plates. Comput Mech. 2023.
https://doi.org/10.1007/s00466-023-02317-8
.
Article
MathSciNet
Google Scholar
Zarzycki K, Ławryńczuk M. Advanced predictive control for GRU and LSTM networks. Inf Sci. 2022;616:229–54.
Article
Google Scholar
Yao X, et al. Echo state network with multiple delayed outputs for multiple delayed time series prediction. J Franklin Inst. 2022;359(18):11089–107.
Article
MathSciNet
Google Scholar
Li H. Short-term wind power prediction via spatial temporal analysis and deep residual networks. Front Energy Res. 2022;10: 920407.
Article
Google Scholar
Ghimire S, et al. Deep learning CNN-LSTM-MLP hybrid fusion model for feature optimizations and daily solar radiation prediction. Measurement. 2022;202: 111759.
Article
Google Scholar
Abraham A. Artificial neural networks handbook of measuring system design. Hoboken: Wiley; 2005.
Google Scholar
Yegnanarayana B. Artificial neural networks. PHI Learning Pvt. Ltd.: Delhi; 2009.
Google Scholar
Vaka M, et al. A review on Malaysia’s solar energy pathway towards carbon-neutral Malaysia beyond Covid’19 pandemic. J Clean Prod. 2020;273: 122834.
Article
Google Scholar
Gu J, et al. Recent advances in convolutional neural networks. Pattern Recogn. 2018;77:354–77.
Article
Google Scholar
Medsker LR, Jain L. Recurrent neural networks. Des Appl. 2001;5(64–67):2.
Google Scholar
Wang J, et al. NGCU: a new RNN model for time-series data prediction. Big Data Res. 2022;27: 100296.
Article
Google Scholar
Uddin MZ, et al. A body sensor data fusion and deep recurrent neural network-based behavior recognition approach for robust healthcare. Inform Fusion. 2020;55:105–15.
Article
Google Scholar
Bani-Almarjeh M, Kurdy M-B. Arabic abstractive text summarization using RNN-based and transformer-based architectures. Inf Process Manage. 2023;60(2): 103227.
Article
Google Scholar
Zhang W, et al. Prediction high frequency parameters based on neural network. IOP Conf Ser Mater Sci Eng. 2019.
https://doi.org/10.1088/1757-899X/631/5/052035
.
Article
Google Scholar
Lee K, Ray J, Safta C. The predictive skill of convolutional neural networks models for disease forecasting. PLoS ONE. 2021;16(7): e0254319.
Article
Google Scholar
He Y, Zhao J. Temporal convolutional networks for anomaly detection in time series. J Phys Conf Ser. 2019.
https://doi.org/10.1088/1742-6596/1213/4/042050
.
Article
Google Scholar
Ji Q, et al. Short-term prediction of the significant wave height and average wave period based on VMD-TCN-LSTM algorithm. EGUsphere. 2023;2023:1–27.
Google Scholar
Sarwinda D, et al. Deep learning in image classification using residual network (ResNet) variants for detection of colorectal cancer. Proc Comput Sci. 2021;179:423–31.
Article
Google Scholar
Ronald M, Poulose A, Han DS. iSPLInception: an inception-ResNet deep learning architecture for human activity recognition. IEEE Access. 2021;9:68985–9001.
Article
Google Scholar
Jaeger H. Echo state network. Scholarpedia. 2007;2(9):2330.
Article
Google Scholar
Gallicchio C, Micheli A, Pedrelli L. Design of deep echo state networks. Neural Netw. 2018;108:33–47.
Article
Google Scholar
Zheng K, et al. Long-short term echo state network for time series prediction. IEEE Access. 2020;8:91961–74.
Article
Google Scholar
Alaeddine H, Jihene M. Deep residual network in network. Comput Intell Neurosci. 2021;2021:1–9.
Article
Google Scholar
Livieris IE, Pintelas E, Pintelas P. A CNN–LSTM model for gold price time-series forecasting. Neural Comput Appl. 2020;32:17351–60.
Article
Google Scholar
Rajagukguk RA, Ramadhan RA, Lee H-J. A review on deep learning models for forecasting time series data of solar irradiance and photovoltaic power. Energies. 2020;13(24):6623.
Article
Google Scholar
James G, et al. An introduction to statistical learning: with applications in python. Cham: Springer; 2023.
Book
Google Scholar
Alrababeh NM, BaniMustafa AM. Regression for predicting effort in object-oriented software projects. SSRN Elect J. 2022.
https://doi.org/10.2139/ssrn.4141236
.
Article
Google Scholar
Li Y, Lu F, Yin Y. Applying logistic LASSO regression for the diagnosis of atypical Crohn’s disease. Sci Rep. 2022;12(1):11340.
Article
Google Scholar
Borup D, et al. Targeting predictors in random forest regression. Int J Forecast. 2023;39(2):841–68.
Article
Google Scholar
Velthoen J, et al. Gradient boosting for extreme quantile regression. Extremes. 2023;26(4):639–67.
Article
MathSciNet
Google Scholar
Rathore SS, Kumar S. A decision tree regression based approach for the number of software faults prediction. ACM SIGSOFT Soft Eng Notes. 2016;41(1):1–6.
Article
Google Scholar
Goyal R, Chandra P, Singh Y. Suitability of KNN regression in the development of interaction based software fault prediction models. Ieri Procedia. 2014;6:15–21.
Article
Google Scholar
James G, et al. Linear regression. In: James G, Witten D, Hastie T, Tibshirani R, Taylor J, editors., et al., An introduction to statistical learning With applications in python. Cham: Springer; 2023. p. 69–134.
Chapter
Google Scholar
Groß J. Linear regression. Berlin: Springer, Berlin Heidelberg; 2003.
Book
Google Scholar
Jastrzębski, S., et al., Three factors influencing minima in sgd
.
arXiv preprint, 2017.
Ranstam J, Cook J. LASSO regression. J Br Surgery. 2018;105(10):1348–1348.
Article
Google Scholar
Chatterjee T, Chowdhury R. Improved sparse approximation models for stochastic computations in Handbook of neural computation. Amsterdam: Elsevier; 2017.
Google Scholar
Tang N, et al. Solar power generation forecasting with a LASSO-based approach. IEEE Internet Things J. 2018;5(2):1090–9.
Article
Google Scholar
Rigatti SJ. Random forest. J Insur Med. 2017;47(1):31–9.
Article
Google Scholar
Babar B, et al. Random forest regression for improved mapping of solar irradiance at high latitudes. Sol Energy. 2020;198:81–92.
Article
Google Scholar
Munshi, A. and R. Moharil. Solar radiation forecasting using random forest. In AIP Conference Proceedings. 2022. AIP Publishing.
Villegas-Mier CG, et al. Optimized random forest for solar radiation prediction using sunshine hours. Micromachines. 2022;13(9):1406.
Article
Google Scholar
Zemel R, Pitassi T. A gradient-based boosting algorithm for regression problems. Adv Neu Inform Proc Syst. 2000.
https://doi.org/10.3389/fnbot.2013.00021
.
Article
Google Scholar
Voyant C, et al. Prediction intervals for global solar irradiation forecasting using regression trees methods. Rene Energy. 2018;126:332–40.
Article
Google Scholar
Lundberg SM, Lee S-I. A unified approach to interpreting model predictions. Adv Neu Inform Proc Syst. 2017.
https://doi.org/10.1093/bioadv/vbad016
.
Article
Google Scholar
Zafar MR, Khan N. Deterministic local interpretable model-agnostic explanations for stable explainability. Machi Learn Knowled Ext. 2021;3(3):525–41.
Article
Google Scholar
Benjamini Y. Opening the box of a boxplot. Am Stat. 1988;42(4):257–62.
Article
Google Scholar
Ding, R., et al., Evaluation of landslide susceptibility in mountainous areas of Changji city at the northern foot of Tianshan Mountain based on coupled model of weight of evidence and Shanon’s entropy. 2022.
Mellit A, Pavan AM, Lughi V. Deep learning neural networks for short-term photovoltaic power forecasting. Renew Energy. 2021;172:276–88.
Article
Google Scholar
Phan Q-T, et al. A novel forecasting model for solar power generation by a deep learning framework with data preprocessing and postprocessing. IEEE Trans Ind Appl. 2022;59(1):220–31.
Article
Google Scholar
Boubaker S, et al. Deep neural networks for predicting solar radiation at Hail Region. Saudi Arabia Ieee Access. 2021;9:36719–29.
Article
Google Scholar
Chadha A, Kaushik B. A hybrid deep learning model using grid search and cross-validation for effective classification and prediction of suicidal ideation from social network data. N Gener Comput. 2022;40(4):889–914.
Article
Google Scholar
Malakouti SM, Menhaj MB, Suratgar AA. The usage of 10-fold cross-validation and grid search to enhance ML methods performance in solar farm power generation prediction. Cleaner Eng Technol. 2023;15: 100664.
Article
Google Scholar
Bates S, Hastie T, Tibshirani R. Cross-validation: what does it estimate and how well does it do it? J Am Stat Assoc. 2024;119(546):1434–45.
Article
MathSciNet
Google Scholar
Berrar D. Cross-validation. Amsterdam: Elsevier; 2019.
Book
Google Scholar
Satria, A., O.S. Sitompul, and H. Mawengkang. 5-Fold cross validation on supporting k-nearest neighbour accuration of making consimilar symptoms disease classification. In 2021 International Conference on Computer Science and Engineering (IC2SE). 2021. IEEE.
Yadav, S. and S. Shukla. Analysis of k-fold cross-validation over hold-out validation on colossal datasets for quality classification. In 2016 IEEE 6th International conference on advanced computing (IACC). 2016. IEEE.
Hodson TO. Root-mean-square error (RMSE) or mean absolute error (MAE): when to use them or not. Geosci Model Dev. 2022;15(14):5481–7.
Article
Google Scholar
Dong Y, et al. Predicting dissolved oxygen level using Young’s double-slit experiment optimizer-based weighting model. J Environ Manage. 2024;351: 119807.
Article
Google Scholar
Vaisakh T, Jayabarathi R. Analysis on intelligent machine learning enabled with meta-heuristic algorithms for solar irradiance prediction. Evol Intel. 2022;15(1):235–54.
Article
Google Scholar
Wang H-Z, et al. Deep learning based ensemble approach for probabilistic wind power forecasting. Appl Energy. 2017;188:56–70.
Article
Google Scholar
Khelifi R, et al. Short-Term pv power forecasting using a hybrid TVF-EMD-ELM strategy. Int Trans Elect Energy Syst. 2023;2023(1):6413716.
Google Scholar
Zhao Y, Li J, Yu L. A deep learning ensemble approach for crude oil price forecasting. Energy Econ. 2017;66:9–16.
Article
Google Scholar
Download references
Funding
Open access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).
Author information
Authors and Affiliations
Department of Computer Science, Faculty of Computers and Informatics, Zagazig University, Zagazig, 44519, Egypt
Doaa El-Shahat, Ahmed Tolba & Mohamed Abdel-Basset
Department of Mathematics, Faculty of Science, Mansoura University, Mansoura, 35516, Egypt
Mohamed Abouhawwash
Authors
Doaa El-Shahat
View author publications
You can also search for this author in
PubMed
Google Scholar
Ahmed Tolba
View author publications
You can also search for this author in
PubMed
Google Scholar
Mohamed Abouhawwash
View author publications
You can also search for this author in
PubMed
Google Scholar
Mohamed Abdel-Basset
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
Doaa El-Shahat and Ahmed Tolba: Investigation, Methodology, Resources, Visualization, Software, Writing—original draft, Writing—review & editing.  Mohamed Abouhawwash: Conceptualization, Methodology, Visualization, Software, Writing—review & editing.  Mohamed Abdel-Basset: Conceptualization, Methodology, Resources, Visualization, Validation, Supervision, Writing—review & editing.
Corresponding author
Correspondence to
Mohamed Abouhawwash
.
Ethics declarations
Ethics approval and consent to participate
The author confirms the sole responsibility for this manuscript. The author read and approved the final manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by/4.0/
.
Reprints and permissions
About this article
Cite this article
El-Shahat, D., Tolba, A., Abouhawwash, M.
et al.
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting.
J Big Data
11
, 134 (2024). https://doi.org/10.1186/s40537-024-00991-w
Download citation
Received
:
27 February 2024
Accepted
:
26 August 2024
Published
:
18 September 2024
DOI
:
https://doi.org/10.1186/s40537-024-00991-w
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Solar radiation
Deep learning
Machine learning
Hybrid models
XAI
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00994-7):
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
18 September 2024
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
Ali Mohammed Alsaffar
1
,
2
,
Mostafa Nouri-Baygi
1
&
Hamed M. Zolbanin
3
Journal of Big Data
volume
11
, Article number:
133
(
2024
)
Cite this article
51
Accesses
Metrics
details
Abstract
The frequent usage of computer networks and the Internet has made computer networks vulnerable to numerous attacks, highlighting the critical need to enhance the precision of security mechanisms. One of the most essential measures to safeguard networking resources and infrastructures is an intrusion detection system (IDS). IDSs are widely used to detect, identify, and track malicious threats. Although various machine learning algorithms have been used successfully in IDSs, they are still suffering from low prediction performances. One reason behind the low accuracy of IDSs is that existing network traffic datasets have high computational complexities that are mainly caused by redundant, incomplete, and irrelevant features. Furthermore, standalone classifiers exhibit restricted classification performance and typically fail to produce satisfactory outcomes when dealing with imbalanced, multi-category traffic data. To address these issues, we propose an efficient intrusion detection model, which is based on hybrid feature selection and stack ensemble learning. Our hybrid feature selection method, called MI-Boruta, combines mutual information (MI) as a filter method and the Boruta algorithm as a wrapper method to determine optimal features from our datasets. Then, we apply stacked ensemble learning by using random forest (RF), Catboost, and XGBoost algorithms as base learners with multilayer perceptron (MLP) as meta-learner. We test our intrusion detection model on two widely recognized benchmark datasets, namely UNSW-NB15 and CICIDS2017. We show that our proposed IDS outperforms existing IDSs in almost all performance criteria, including accuracy, recall, precision, F1-Score, false positive rate, true positive rate, and error rate.
Introduction
The Internet has turned into an important part of our daily lives and an indispensable tool. The reliance of people on the Internet for accessing a range of electronic services, such as online education, banking, business, job search, and shopping, as part of their daily routines has been steadily increasing. Following the widespread growth in Internet usage, the cyberthreat landscape has evolved rapidly [
1
] and security breaches have now become a routine [
2
], making information systems, computer networks, and activities associated with them prone to new and sophisticated attacks. Consequently, recent years have witnessed a surge in data and privacy breaches [
2
,
3
] and the security of cyberspace has become the focus of much more scrutiny. For instance, many studies have focused on detecting cyberattacks on computer networks, such as the Internet of Things [
4
,
5
], vehicle networks [
6
], and wireless networks [
7
], or on developing secure systems in certain business sectors, such as healthcare [
8
,
9
] and energy [
1
]. Regardless of the application domain, what cybersecurity initiatives have in common is that they all need to develop a robust security system to fend off cyberattacks. Although different security tools have been used to secure networks, such as user authentication mechanisms, data encryption techniques, firewalls, and antivirus software, they cannot be effective against all threats and cyberattacks [
10
]. Detecting cyberattacks, which are constantly growing in numbers and sophistication, requires intelligent security mechanisms, such as intrusion detection systems (IDSs) to protect networks and information systems from an array of threats [
11
,
12
]. IDS is a hardware or software system that can carry out a variety of tasks, such as analyzing and monitoring computer networks, examining abnormal activity patterns, monitoring and inspecting user and system activities, and identifying common attacks. Hence, the criticality of IDS in network security cannot be overstated as it plays a crucial role in detecting and preventing malicious activities [
13
] that have the potential to violate security policies and compromise the confidentiality, integrity, and availability (CIA) of information [
14
,
15
].
Generally, IDS are classified into two types: host intrusion detection systems (HIDS) and network intrusion detection systems (NIDS) [
16
]. A HIDS is established on a single host to monitor all activities on the host, e.g., scanning for suspicious activities and violations of security policies. On the other hand, a NIDS is established to secure all devices and networks against potential security breaches. IDS can also be categorized into two types based on the detection mechanism they use: (1) signature-based IDS or misuse-based IDS, and (2) anomaly-based IDS [
17
]. Signature-based IDS identifies known attack patterns by utilizing a database of their signatures [
18
]. The main disadvantage of these systems is that they cannot detect zero-day attacks or more recent attacks if their databases are outdated. Anomaly-based IDS, on the other hand, are designed to monitor and analyze all actions across a network by learning normal and anomalous network behaviors; therefore, they can also detect novel attacks [
19
,
20
].
Over the past two decades, the performance of IDSs has been enhanced through the application of various machine learning techniques; however, these techniques still face various challenges. First, since intrusion detection algorithms deal with large amounts of network traffic data, and these data contain redundant, noisy, and/or irrelevant features, reducing the feature space by selecting the most informative features still bears an important weight on the performance of IDS. Second, the use of single algorithm models may create statistical, representational, or computational issues that are driven by the numerous, multi-dimensional features of massive datasets. In the context of IDS, a single algorithm classifier may reduce the efficiency of an IDS and make it unable to detect multiclass attacks.
Our goal in this paper is to address these challenges. Initially, we opt for the most suitable subset of features to enhance the effectiveness of machine learning algorithms, which helps to reduce the amount of data, storage space, and processing cost required. Next, we develop a stacked ensemble learning model to detect cyberattacks. Finally, we test our model using two benchmark datasets and compare our results with existing studies, most of which have developed IDS using single algorithm models. The experimental results show that our model performs well based on different evaluation metrics for binary and multi-class classification. Our study offers the following contributions:
1.
Using a combination of filter and wrapper methods, it develops a hybrid feature selection method that combines the benefits of the two approaches and enables the development of a simpler, yet more accurate intrusion detection model.
2.
It uses random forest (RF), Catboost, and XGBoost algorithms as base learners and multilayer perceptron (MLP) as the meta-learner to develop a stacked ensemble model that improves the multi-class classification performance of existing models in the literature.
3.
Constructing and testing the model on two well-known benchmark datasets of different sizes illustrate that the proposed model is generalizable.
The subsequent sections of the paper are structured as follows. Sect. "
Related work
" presents the related work for IDS feature selection techniques and ensemble learning. In Sect. "
Methodology
", the proposed methodology is presented. Sect. "
Experimental Results and Discussion
" discusses the experimental analysis and evaluates the results. Finally, Sect. "
Conclusion
" concludes the paper.
Related work
Many studies have been carried out to implement an efficient IDS. From a broad perspective, these studies can be classified based on two criteria: the type of feature selection method and the type of classification algorithm they use. Feature selection methods can be further divided into the filter, wrapper, and embedded methods. Similarly, classification algorithms can be split into single and ensemble learning algorithms. In what follows, we present a summary of the literature on IDSs, focusing on feature selection and model building approaches used in those studies.
The main idea behind filter methods is to select the most relevant features from a dataset based on their intrinsic characteristics, without involving a machine learning algorithm to predict the target variable. In filter feature selection methods, the features are evaluated based on some statistical measure, such as correlation  [
21
,
22
], information gain[
23
–
27
], chi-square test [
28
,
29
,
30
], or ANOVA F-test [
26
,
30
], and then ranked according to their scores. The top-ranked features are then selected for further analysis or used as input for a machine learning algorithm. Some advantages of using filter feature selection methods are that they are computationally efficient, easy to implement, and can work well with high-dimensional datasets; however, they may not always capture complex relationships between features and the target variable, and may also suffer from redundancy issues [
31
,
32
].
In contrast to filter methods, wrapper feature selection methods use a machine learning algorithm to evaluate the relevance of features and select the best subset for a given model. In wrapper methods, the feature selection process is integrated with the training process of a machine learning algorithm. The algorithm is used to evaluate the performance of different subsets of features and select the one that results in the best model performance. This process is typically done using a cross-validation approach, where the dataset is divided into training and validation sets, and the algorithm is evaluated on multiple iterations. Wrapper feature selection methods can be more accurate than filter methods, as they take into account the interactions between features and the target variable. However, they can be computationally expensive and prone to overfitting if the number of features is large compared to the size of the dataset. Examples of wrapper methods that are used in prior studies include recursive feature elimination (RFE) [
33
,
34
], Pigeon-inspired optimization [
35
,
36
], Cuckoo Search Optimization [
37
], Particle Swarm Optimization [
38
,
39
], Bat Optimization [
40
], Grey Wolf Optimization [
39
], and the Grasshopper optimization algorithms [
41
]. These methods are commonly used in machine learning applications where the number of features is high, and the goal is to find the optimal subset of features that leads to the best model performance [
42
,
43
].
Embedded methods are techniques for feature selection that involve incorporating the feature selection process into the model training process itself. These methods aim to find the most informative subset of features to improve model performance and reduce overfitting. In embedded methods, feature selection is performed during the model training process to select the best subset of features that can be used to train the model. This can be achieved by adding a penalty term to the model's objective function that encourages the selection of only the most informative features. Some examples of embedded feature selection methods that are used in previous studies include Gradient Boosting [
26
,
44
,
45
,
46
], and decision tree-based algorithms such as Random Forest [
30
,
47
,
48
], which select the most important features at each split. Embedded feature selection methods are often preferred over traditional filter and wrapper methods because they can lead to more accurate and efficient models by simultaneously selecting the most relevant features and optimizing model performance. However, they can be computationally expensive and require more resources compared to other feature selection methods [
31
,
49
].
Regarding the classification algorithms used in previous studies, single algorithm classifiers that predict binary outcomes [
24
,
26
,
35
,
36
,
37
,
38
,
39
,
40
] comprise a significant proportion. In contrast, a smaller proportion of single algorithm efforts [
23
,
33
,
47
,
50
] have tackled the more challenging problem of predicting multi-class network attacks. Some studies [
17
,
25
,
41
,
44
,
46
] have predicted both binary and multi-class outcomes. A similar pattern is observed among the ensemble learning models: studies that predict binary outcomes include [
21
,
27
,
28
,
29
,
51
,
52
]; those that focus on multi-class prediction include [
34
], and studies that predict network attacks both as binary and multi-class outcomes include [
22
,
30
,
48
,
53
]. Table
1
provides a summary of previous studies that have developed IDSs using machine learning techniques.
Table 1 A Summary of Prior Research on Intrusion Detection Systems
Full size table
Our review of prior research identifies several gaps in the literature. First, a large number of prior studies have used outdated datasets, such as KDD99 or NSL-KDD, that do not cover modern network attacks. Second, when building and testing an IDS using large datasets, such as CICIDS2017, researchers have typically tended to focus on a subset of commonly occurring network attacks while disregarding the infrequent ones. Therefore, their IDSs are unable to detect all possible cyber threats. Third, only a few attempts have been made to evaluate intrusion detection systems using multiple datasets that vary in terms of the types of attacks they represent as well as their size. Fourth, few studies have focused on developing and testing IDSs for multi-class prediction using datasets that include all current types of network attacks. Lastly, almost all previous studies, especially those predicting multi-class attacks, have a fairly large false positive rate.
In order to address these deficiencies, we develop an IDS that improves the accuracy of predicting network attacks, while reducing the false positive rate. Our approach involves employing a hybrid feature selection method, which we call MI-Boruta, to optimize feature selection through the use of both mutual information (MI) as a filter method and the Boruta algorithm as a wrapper method. After selecting the most relevant features from our datasets, we construct a stacked ensemble model that combines several machine learning algorithms, including random forest (RF), Catboost, and XGBoost, as base learners, with a multilayer perceptron (MLP) serving as the meta-learner. We evaluate the effectiveness of our proposed method by conducting experiments on two widely recognized benchmark datasets.
Methodology
As we explained before, filter, wrapper, and embedded feature selection models have their advantages and disadvantages. In this study, we use a combination of filter and wrapper methods to reap the benefits of both feature selection techniques.
Footnote
1
When combining these two methods, filter methods can be used initially to identify the most relevant features based on some criterion, such as correlation with the target variable, and these selected features can then be passed on to the wrapper method for further evaluation. Wrapper methods can then be used to fine-tune the subset of features, by considering all possible combinations and selecting the subset that maximizes the model performance. This combination approach can help to reduce the number of features and improve the accuracy and interpretability of the model while avoiding some of the drawbacks of either method used alone.
After implementing our hybrid feature selection method, we utilize a stacked ensemble learning classifier to enhance the detection performance of IDSs. The architecture and stages of our methodology are illustrated in Fig.
1
, and we elaborate further on the individual steps of our approach in the subsequent sections.
Fig. 1
Proposed Methodology
Full size image
Data description
This research uses two benchmark datasets: the UNSW-NB15 and the CICIDS2017 datasets. We describe these two datasets in the following subsections.
UNSW-NB15 dataset (D
1
)
Created by the Australian cybersecurity center [
54
], the UNSW-NB15 dataset (D
1
) comprises both training and testing partitions. The training set is composed of 175,341 records, whereas the test set includes 83,332 records. The records of D
1
can be broadly classified into normal network traffic and cyberattack, which itself has different types. The dataset has 44 features, two of which are class labels (i.e.,
attack cat
and
label
). Features of the dataset are of binary, categorical, or numeric (float and integer) formats. D
1
consists of ten classes, wherein one class represents normal traffic, and the remaining nine classes represent different types of attacks: DoS, Worms, Backdoors, Fuzzers, Shellcode, Generic, Reconnaissance, Exploits, and Port scans [
55
]. The distribution of these ten classes is shown in Table
2
. In the D
1
dataset, the majority class is “Normal” with 93,000 instances and the minority class is “Worms” with 174 instances. The imbalance ratio for each class is shown in Fig.
2
. As we see in this figure,
D
1
is highly imbalanced, with the majority class being over 534 times more frequent than the minority class.
Table 2 The class distribution of the UNSW-NB15 dataset
Full size table
Fig. 2
Distribution of classes for D
1
Full size image
$$\text{Imbalance Ratio }=\frac{Number\, of\, instances\, in\, majority\, class}{Number\, of\, instance\, in\, minority\, class}$$
(1)
CICIDS2017 dataset (D
2
)
This dataset was created by the Canadian Institute for Cybersecurity (CIC) in 2017 [
56
] and includes 2,830,743 records that are spread across eight files. Each record comprises 78 features plus a label. D2 contains 15 classes, including benign network traffic and 14 attack types: DDoS, DoS Slowloris, DoS Slowhttptest, DoS Hulk, DoS GoldenEye, Heartbleed, PortScan, Bot, FTP-Patator, SSH-Patator, Web Attack-Brute Force, Web Attack-XSS, Web Attack-SQL Injection, and Infiltration. Table
3
shows the class distribution of D
2
. Before preprocessing this dataset, we combined DoS slowloris, DoS Slowhttptest, DoS Hulk, and DoS GoldenEye into a single class called DOS attack. Similarly, we aggregated Web Attack-XSS, Web Attack-Sql-Injection, and Web Attack-Brute Force to form a single class called the Web Attack class. In the D
2
dataset, the majority class is “Benign” which has 2,273,097 instances, and the minority class is “Heartbleed” with 11 instances. The imbalance ratio for each class is shown in Fig.
3
. As we see in this figure, D
2
is highly imbalanced. The “Benign” class is over 206,649 times more frequent than the “Heartbleed” class.
Table 3 The class distribution of the CICIDS2017 dataset
Full size table
Fig. 3
Distribution of classes for D
2
Full size image
Drawing from the methodologies employed by leading authorities in the fields of statistical analysis and deep learning [
57
,
58
], we aggregated the training and testing datasets for both D
1
and D
2
. Subsequently, we randomly partitioned the combined data into two non-overlapping subsets, with 80% of the data allocated for model training and 20% reserved for testing and validation. This approach is particularly suited for larger datasets as it circumvents the computationally intensive run-time required by k-fold cross-validation. Moreover, it affords the model with sufficient data to learn from, while still retaining an adequate amount of data for validation and testing purposes. Following the construction of our model using the training datasets, we will proceed to assess its binary and multiclass classification performance for both D
1
and D
2
.
Data preprocessing
Data preprocessing is a necessary step in data mining that transforms the data into a suitable format for analysis and knowledge discovery. This phase contains three steps: data filtration, data numeralization, and data normalization.
Data filtration
Real-world data typically contain anomalous, incomplete, and inconsistent values that come from heterogeneous platforms and can degrade the classification performance of machine learning models. For instance, the feature ‘Flow Packets/s’ in D
2
includes abnormal values such as ‘NaN’ and ‘Infinity’ that may negatively influence the classification accuracy of predictive models. To deal with such instances, we applied basic data cleaning, including removing infinite data values and missing data samples from the datasets.
Data numeralization
This step converts the symbolic data (non-numeric features) into numerical values (numeric features) by using 1-N numeric coding, where N is the number of symbols. This paper has converted all the non-numeric features such as
proto
,
service
, and
state
in D
1
into numerical values.
Data normalization
Feature scaling is an important step in data preprocessing. In this study, we applied the
min–max normalization
to scale the features within the range of [0,1]. The formula for the
min–max normalization
is given below:
$${X}_{normalized}=\frac{{X-X}_{min}}{{X}_{max}-{X}_{min}}$$
(2)
where
\({X}_{normalized}\)
represents the value after normalization,
\(X\)
is the value before normalization,
\({X}_{min}\)
is the minimum value of the feature, and
\({X}_{max}\)
is the maximum value of the feature.
Feature selection
Feature selection is an important aspect of constructing an efficient IDS. Feature selection eliminates irrelevant and redundant features from a dataset to improve the efficiency of machine learning algorithms. As discussed earlier, there are three main feature selection methods: filter methods [
32
], wrapper methods [
59
], and embedded methods [
60
]. Filter methods select the most useful features without using any learning algorithm. They are based on the intrinsic characteristics of features, such as correlation, consistency, information distance, statistical measures, etc. These methods are fast and have low computational complexity. Wrapper methods employ a learning algorithm to determine a subset of features, which are subsequently evaluated by the classification algorithm. As a result, these methods can be computationally complex. Embedded methods are similar to wrapper methods, but the learning algorithm and search strategy are achieved simultaneously. In other words, feature selection in these methods is made during the learning process itself, which could make them computationally intensive for large datasets. In this section, we propose a hybrid features selection method by combining mutual information as the filter method and the Boruta algorithm as the wrapper method to select the best features from each of the datasets.
Mutual information
Mutual information (MI) is one of the most used filter feature selection methods [
61
]. It measures the amount of mutual dependence between two random variables (features). More precisely, MI measures the quantity of information that is gained about one random variable by observing the other random variable. Mutual information has several advantages over other filter feature selection methods. First, mutual information is a non-parametric method, which means it does not make assumptions about the distribution of the data. This is particularly useful when dealing with complex or non-linear relationships between variables. Second, mutual information can capture both linear and non-linear [
62
] dependencies between variables. This is in contrast to some other methods, such as the Pearson correlation, which only capture linear relationships. Third, mutual information is robust to noise and outliers [
63
], which can be a problem for some other methods.
For these reasons, we utilize MI in the first step of our feature selection. MI can be calculated using the following equation:
$$MI=\left(X;Y\right)=e\, \left(X\right)-\,e \,(X|Y)$$
(3)
where
\(MI=\left(X;Y\right)\)
is the value of mutual information for variables
\(X\)
and
\(Y\)
,
\(e\left(X\right)\)
represents the marginal entropy of
\(X\)
, and
\(e(X|Y)\)
represents the conditional entropy of
\(X\)
given
\(Y\)
.
Boruta algorithm
Boruta is a wrapper feature selection algorithm [
64
] that identifies the most important variables in a dataset. It works by creating shadow features that mimic the original features and comparing their importance to that of the original features using a random forest model [
65
]. The algorithm then assigns a tentative status to each feature based on its importance relative to the shadow features. Features that are more important than shadow features are given a confirmed status, while those that are less important are given a rejected status. The algorithm iteratively re-runs the random forest model until all features have been assigned a confirmed or rejected status, providing a robust feature selection process. The Boruta algorithm offers several advantages. First, it is a model-agnostic algorithm, meaning it can be used with any machine learning model and is not biased toward a specific algorithm. Second, it can handle different types of data, such as numerical, categorical, and mixed data. Third, the algorithm can identify complex relationships between features and can handle interactions between them. Finally, it is a robust algorithm that can handle noisy data and is less likely to overfit compared to other feature selection methods [
64
,
66
].
The Boruta algorithm has the following steps:
1.
Extend the dataset by creating shadow features that mimic the original features.
2.
Shuffle the values of the added duplicate features to eliminate any correlation with the response variable.
3.
Train a random forest classifier on the extended dataset (including both the original and shadow features) and calculate the
\(Z score\)
, which is the mean of accuracy loss divided by the standard deviation of accuracy loss.
4.
Find the maximum
\(Z score\)
among the shadow attributes
\((MZSA)\)
, and then tentatively tag the features as ‘important’ if their
Z Score
is significantly greater than
MZSA
or as ‘unimportant’ otherwise.
5.
Repeat steps 3 and 4 until all features are tagged as important or unimportant.
6.
Remove the unimportant features and retain the important ones.
7.
Remove all shadow attributes.
8.
Optionally, rank the important features and select a subset of top-ranked features for model training.
A MI-boruta algorithm for feature selection
In this section, we propose an MI-Boruta algorithm to select the best subset of features from D
1
and D
2
. At first, the MI method is used to select
\({X}_{MI-best}\)
from the set of all possible pairs of features in the feature space, with
\({X}_{MI-best}\)
being chosen based on the value of MI between the pairs.
Footnote
2
Based on the MI algorithm, 31 and 51 features were respectively selected out of 42 and 78 features of D
1
and D
2
. Table
4
shows features selected by the MI algorithm. In the next step, the Boruta algorithm uses the feature importances obtained from the random forest models it trains to select the best
\({Y}_{Boruta -best}\)
subset of features. Based on the Boruta algorithm, 33 and 59 features were respectively selected out of 42 and 78 features of D
1
and D
2
. Table
5
shows features selected by Boruta. Lastly, the final subset of features,
\({Z}_{MI-Boruta -best}\)
, is selected by finding the intersection between
\({X}_{MI-best}\)
and
\({Y}_{Boruta -best}\)
. Algorithm
1
summarizes this process. Based on this algorithm, 27 and 37 features are respectively selected out of 42 and 78 features of D
1
and D
2
. Table
6
depicts important selected features.
Algorithm 1
MI-Boruta Approach for Features Selection
Full size image
Table 4 Feature selected through the MI approach
Full size table
Table 5 Features selected through the boruta approach
Full size table
Table 6 Features selected through the MI-boruta approach
Full size table
Ensemble learning classifier
Ensemble learning classifiers combine two or more machine learning algorithms to attain better performance in contrast to using individual algorithms. The approach of ensemble learning can be classified into three categories: boosting, bagging, and stacking. The boosting method, which was first discussed by Schapire [
67
] in 1990, involves intensive training of models on misclassified data during the training phase. Eventually, the model with the highest accuracy is selected as the classifier for the test data. The bagging approach, developed in 1996 by Breiman [
68
], involves using homogeneous models to predict the class of test data. Initially, the predictions of the selected homogenous models are recorded. Then, the class that is predicted by the majority of models is assigned to the test data. Wolpert [
69
] introduced the stacking approach, also known as
stacked generalization
, in 1992. This method is considered an effective strategy as it provides a general framework to combine numerous ensemble algorithms. It involves two levels of learning, where the initial (base) learners are trained using a training dataset in level 0, and meta-learning takes place in level 1. The base learners generate a new dataset for the meta-learner, and this new dataset is used to train the meta-learner. The trained meta-learner is then used to classify the test set. Unlike boosting and bagging ensembles that typically use the same type of model, stacked ensemble can integrate different types of models. Therefore, selecting the optimal base learner is a crucial part of the stacking approach, and several base learners should be selected for the training dataset instead of a single one.
Overall, stacked ensemble learning provides several advantages over bagging and boosting methods:
Improved predictive accuracy: Stacked ensembles can achieve higher predictive accuracy than individual models or other ensemble techniques, such as bagging and boosting. This is due to the combination of diverse base models in the stacked ensemble, which can capture different aspects of the data and reduce bias and variance [
69
].
Better robustness: Stacked ensemble learning can reduce the risk of overfitting and increase the robustness of the model by incorporating a meta-learner that combines the outputs of the base models. The meta-learner can identify and correct errors made by the base models, leading to better generalization performance [
70
].
Flexibility: Unlike bagging and boosting, which typically use the same type of model, stacked ensembles can integrate different types of models, including both linear and non-linear models. This allows for more flexibility in model selection and can improve performance on complex datasets [
71
].
To leverage the advantages of stacked ensembles, we will employ them in this paper by using random forest, CatBoost, and XGBoost algorithms as base learners and multilayer perceptron (MLP) as the meta-learner. As we discuss later in the paper, using stacked ensembles enables us to reduce the variance of prediction errors, as well as to improve robustness by minimizing prediction dispersion [
72
].
Random forest algorithm
Breiman’s random forest (RF) [
73
] is an ensemble-based classification method that constructs multiple classification trees by drawing bootstrap samples from the original data. Each tree then casts a vote on how to categorize a new item. The final decision is made based on the majority vote. RF offers several advantages, such as the ability to predict important variables, low classification errors, the capability to handle imbalanced datasets, efficient handling of big data sets with thousands of variables, addressing overfitting, and maintaining accurate classification across various datasets. These advantages make RF a powerful and versatile classification method.
XGBoost algorithm
eXtreme Gradient Boosting (XGBoost) [
74
] is an ensemble learning method that is based on the gradient boosting decision tree (GBDT) algorithm. The Xgboost supports linear classifiers and uses traditional CART (Classification And Regression Trees) as the base classifier. It enhances prediction accuracy by utilizing Taylor expansion for the cost function and incorporating second derivatives. The XGBoost algorithm utilizes an additive training technique to optimize the objective function, where the optimization process of each subsequent step relies on the results of the previous step. The principles of the XGBoost algorithm are as follows:
1.
XGBoost employs an additive training technique to optimize the objective function, which means that the optimization process of latter steps relies on the optimization result of the previous steps. The
\({t}^{th}\)
objective function of the model is expressed by the following equation:
$${obj}^{(t)}= {\sum }_{i=1}^{n}l\left({y}_{i} , {\widehat{y}}_{i}^{t-1}\right)+ {f}_{t}({x}_{i}))+\Omega ({f}_{t})+C$$
(4)
where
\(l\)
represents the loss term of the
\({t}^{th}\)
round,
\(\Omega\)
is the regularization term, and
\(C\)
represents a constant term. The value of
\(\Omega ({f}_{t})\)
is shown in Eq.
5
$$\Omega \left({f}_{t}\right)=\upgamma . {T}_{t}+\uplambda \frac{1}{2} \sum_{j=1}^{T}{w}_{j}^{2}$$
(5)
where both
\(\gamma\)
and
\(\lambda\)
are customization parameters of XGBoost. In general, increasing the values of these two parameters simplifies the tree structure, effectively addressing the problem of overfitting.
2.
Perform a second-order Taylor expansion on Eq.
4
This process is shown in Eq.
6
where
\(g\)
is the first derivative, and
\(h\)
is the second derivative.
\({g}_{i}\)
and
\({h}_{i}\)
can be described as the following:
$${obj}^{(t)}= {\sum }_{i=1}^{n}\left[l\left({y}_{i} , {\widehat{y}}_{i}^{t-1}\right)+{g}_{i}{f}_{t}\left({x}_{i}\right)+\frac{1}{2}{h}_{i}{f}_{t}^{2}+\Omega ({f}_{t})+C\right]$$
(6)
$${g}_{i}={\partial }_{{\widehat{y}}_{i}^{t-1}} l\left({y}_{i} , {\widehat{y}}_{i}^{t-1}\right)$$
(7)
$${h}_{i}={\partial }_{{\widehat{y}}_{i}^{t-1}}^{2} l\left({y}_{i} , {\widehat{y}}_{i}^{t-1}\right)$$
(8)
3.
Substitute (5), (7), and (8) into (6) and take the derivative. Then, solutions can be obtained from (9) and (10) as follows:
$${w}_{j}^{*}=- \frac{\sum {g}_{i}}{\sum {h}_{i}+\lambda }$$
(9)
$${obj}^{*}=- \frac{1}{2}\sum_{j=1}^{T}{\frac{\left(\sum {g}_{i}\right)}{\sum {h}_{i}+\lambda }}^{2}+\upgamma .\text{ T}$$
(10)
In these equations,
\({w}_{j}^{*}\)
refers to the weights solution, and
\({obj}^{*}\)
represents the score of the loss function. The smaller this score, the better the structure of the tree.
Categorical boosting (Catboost) algorithm
Catboost is a machine learning classifier based on the gradient boosting decision tree framework and is available as an open-source library [
75
]. This classifier offers several advantages, including (1) the ability to handle categorical data using statistical methods, (2) optimization of extensive input parameters to reduce overfitting, (3) reduction of the loss function in each iteration to improve model generalization, (4) faster performance than other boosting algorithms thanks to the implementation of symmetric trees, (5) only requiring a small number of parameters, and (6) suitability for large datasets with low latency requirements.
Multilayer perceptron algorithm
MLP is a feed-forward artificial neural network that is constructed with three or more layers. The first layer is the input layer, the last layer is the output layer, and the middle layer has one or more hidden layers. Each layer contains a set of neurons or nodes. MLP learns a function
\(f\left(x\right): {R}^{m}\to {R}^{o}\)
by training on a dataset using the backpropagation learning method. Here,
\(m\)
represents the number of input dimensions, and
\(o\)
represents the number of output dimensions. In an MLP, each layer can be characterized by the following equivalent equations:
$$y=\phi \left(\sum_{i=1}^{n}{W}_{i}X+b\right)$$
(11)
$$y=\phi \left({W}^{T}X+b\right)$$
(12)
In the above equations,
\(\phi\)
is the activation function,
\(W\)
is the weight vector,
\(X\)
is the input vector, and
\(b\)
is the bias. For this study, we used MLP as the meta-learner, consisting of one hidden layer with 200 neurons
Stacked ensemble classifier
This section describes a proposed stacking ensemble classifier consisting of base classifiers (level 0) and meta-classifiers (level 1). The input training dataset (D) for the base classifiers is composed of selected features from MI-Boruta. The predictions from level 0 serve as input for level 1. Overfitting issues during training are addressed by implementing a
k
-fold cross-validation method. The training data (D) is partitioned into
k
disjoint subsets of equal size (
\({D}_{1}, {D}_{2},{D}_{3},{\dots ,D}_{n}\)
), with one of the
k
-subsets used for testing and the remaining subsets (
\(k-1/k\)
) used for training the classifiers. In this study, the base classifiers (level 0) include RF, XGB, and Catboost, and are trained using five-fold cross-validation. The meta-classifier (level 1) utilizes the MLP algorithm to combine the results of the three base classifiers and generate the final predicted output. Table
7
shows the parameters of the stacked ensemble model. Algorithm
2
outlines the general algorithm for stacking ensemble classifiers.
Algorithm 2
Stacked Ensemble Classifier
Full size image
Table 7 Experimental model parameters
Full size table
Performance evaluation
In this section, we evaluate our proposed model using different performance metrics, such as accuracy, recall, precision, F
1
score, and the AUCROC curve. These metrics are derived from a confusion matrix, which is a two-dimensional table that compares the predicted and actual classes and distinguishes the outcomes of the classification. The confusion matrix is based on the following four values:
True Negative (TN): both the original data as well as the predicted data are false.
True Positive (TP): both the original data as well as the predicted data are true.
False Negative (FN): original data are true, but the predicted data are false.
False Positive (FP): original data are false, but the predicted data are true.
Accuracy is the ratio of instances that have been correctly classified to the total number of instances. This can be defined as:
$$accuracy= \frac{TP+TN}{TP+TN+FP+FN}$$
(13)
Recall, also referred to as the detection rate (DR), true positive rate (TPR), or sensitivity, is the percentage of total true positive (TP) cases divided by the total number of true positive (TP) and false positive (FN) cases. It is calculated as illustrated in the following formula:
$$Recall=DR=TPR= \frac{TP}{TP+FN}$$
(14)
Precision is the percentage of total true positive (TP) cases divided by the total number of true positive (TP) and false positive (FP) cases. The following formula represents precision:
$$Precision= \frac{TP}{TP+FP}$$
(15)
F
1
score is the harmonic mean of recall and precision defined by the following equation:
$${F}_{1}=2\times \left(\frac{Precision \times Recall}{Precision + Recall}\right)$$
(16)
The False Positive Rate (FPR) shows the percentage of normal traffic detected as an attack. The FPR is calculated as:
$$FPR= \frac{FP}{FP+TN}$$
(17)
The False Negative Rate (FNR) is the percentage of attacks detected as normal traffic. FNR is calculated as:
$$FNR= \frac{FN}{TP+FN}$$
(18)
Error Rate (ER) is calculated using the following equation:
$$\text{ER}= \frac{FP+FN}{TP+TN+FP+FN}$$
(19)
Finally, the Area Under the Curve (AUC) is a parameter used for evaluating the performance of a machine learning model at all categorization thresholds. It is characterized by the false positive and true positive rates on the X and Y axes, respectively. When the curve is close to the top left corner, the model performs better. Conversely, the model performs poorly if the curve is closer to the baseline. AUC is obtained using the following formula:
$$AUC={\int }_{0}^{1}TPRd(FPR)$$
(20)
Experimental results and discussion
In this section, we evaluate the performance of our proposed stacked ensemble classifier with various individual classification algorithms, as well as with state-of-the-art methods. It deserves to mention that model development and evaluation were performed using Python on a machine with the following specifications:
Operating system: Windows 11 (64-bit)
CPU: Ryzen7 5800 h
RAM: 32 GB
HDD: 1 TB SSD
VGA: RTX3070 8 GB
Comparison with individual classifiers
We assessed the performance of our stack ensemble classifier by comparing it to several individual classification algorithms (such as RF, XGBoost, Catboost, and MLP) using the same MI-Boruta feature selection technique. Our study employed a multi-classification approach and evaluated the accuracy and F
1
score using two confusion matrices. Figure
4
provides a summary of the model's performance relative to the individual classification algorithms. According to Fig.
4
a, our stack ensemble classifier outperformed the individual algorithms on both the UNSW-BN15 and CICIDS2017 datasets, with an accuracy of 84.88 and 99.92% for D
1
and D
2
, respectively. Similarly, as shown in Fig.
4
b, our model achieved a higher F
1
-score than the individual classification algorithms on both datasets, with scores of 84.15 and 99.92% for D
1
and D
2
.
Fig. 4
Performance comparison of classifiers for the two datasets
Full size image
We recorded the computation time of each base classifier and the stacked ensemble for UNSWNB-15 and CICIDS2017 datasets, which have different sizes. The computation times of UNSWNB-15 were 0.14 s, 0.24 s, 0.28 s, 0.03 s, and 0.68 s for RF, Catboost, XGBoost, MLP, and stacked ensemble respectively. The computation times of CICIDS2017 were 1.21 s, 3.29 s, 2.44 s, 0.43 s, and 7.21 s for RF, Catboost, XGBoost, MLP, and stacked ensemble respectively. As expected, the stacked ensemble takes more time than individual classification models in both datasets.
Comparison with the state-of-the-art methods
In this section, we present a comparative analysis of our proposed model's performance with earlier research on binary and multi-class classification using D
1
and D
2
. The evaluation metrics are presented in Tables
8
,
9
,
10
. Table
8
shows the binary classification performance of our proposed model and other recent IDS studies for D
1
. Similarly, Table
9
shows the performance of our model and recent IDS studies for binary classification using D
2
. Table
10
compares the multi-class classification performance of our model with that of recent IDS studies for both datasets.
Table 8 Comparison of evaluation metrics between our proposed model and recent IDS studies for binary classification on the UNSW-NB15 dataset (best values are denoted in bold)
Full size table
Table 9 Comparison of evaluation metrics between our proposed model and recent IDS studies for binary classification on the CICIDS2017 dataset (best values are denoted in bold)
Full size table
Table 10 Comparison of evaluation metrics between our proposed model and recent IDS studies for multi-classification for both datasets (best values are denoted in bold)
Full size table
According to the information presented in these tables, our proposed model has a better overall performance than prior studies. For binary classification of D
1
, our model’s accuracy is 95.34%, recall is 94.64%, precision is 92.62%, F
1
score is 93.62%, FPR is 4.2%, and the AUC is 95.19% (Fig.
5
a). For multi-class classification of D
1
using a subset of 27 features (which denotes a simpler model), the accuracy is 84.88%, recall is 84.88%, precision is 84.93%, and F
1
score is 84.15%. Table
11
presents the performance results of the proposed model for each class of D
1
. For D
2
, the accuracy for binary classification is 99.925%, recall is 99.926%, precision is 99.979%, F
1
score is 99.953%, FPR is 0.08%, and the AUC is 99.922% (Fig.
5
b). For multi-class classification of D
2
using a subset of 37 features, the accuracy is 99.92%, recall is 99.92%, precision is 99.92%, and F
1
score is 99.92%. Table
12
presents the performance results of the proposed model for each class of D
2
.The confusion matrices for binary and multi-class classification of both datasets are shown in Figs.
6
and
7
.
Fig. 5
The Area under the ROC Curve for D
1
and D
2
Full size image
Table 11 Performance results of D
1
for multiclass classification
Full size table
Table 12 Performance results of D
2
for multiclass classification
Full size table
Fig. 6
Confusion matrices for binary classification of D
1
and D
2
Full size image
Fig. 7
Confusion matrix for multi-class classification of D
1
and D
2
Full size image
Conclusion
With the widespread use of the Internet, network security has become a crucial aspect in distributed systems. While previous studies have used different machine learning algorithms to improve the performance of intrusion detection systems (IDS), they still face several challenges in achieving optimal performance. To address this, we presented an effective IDS model that utilizes a hybrid feature selection method and an ensemble learning technique on two well-known intrusion detection benchmark datasets. Our approach started by proposing an MI-Boruta algorithm to select the best features from the datasets. Then, we used a stacked ensemble classifier, comprising random forest, XGBoost, and Catboost as base learners and multilayer perceptron as the meta-learner, for classification. Our experimental results indicated excellent performance in binary and multi-class classification for both datasets. Specifically, the accuracy of binary classification was 95.34%, with 94.64% recall, 92.62% precision, 93.62% F
1
score, and a 4.2% false positive rate. For multi-class classification, the accuracy was 84.88%, with 84.88% recall, 84.93% precision, and 84.15% F
1
score, using a subset of 27 features from the UNSW-NB15 dataset. In particular, our model achieved the highest accuracy for binary and multi-class classification on CICIDS2017, with a binary classification accuracy of 99.92%, 99.92% recall, 99.97% precision, 99.95% F
1
score, and 0.08% FPR. For multi-class classification, the accuracy was 99.92%, with 99.92% recall, 99.92% precision, and 99.92% F
1
score, using a subset of 37 features from D
2
. Furthermore, our proposed stacked ensemble classifier outperformed individual classification methods in terms of accuracy and F
1
score. Finally, comparisons with state-of-the-art methods demonstrated that our model provides a competitive advantage in the intrusion detection domain.
Our study has several implications for future research in the field of intrusion detection. First, the proposed hybrid feature selection method and stacked ensemble classifier have demonstrated good results in detecting intrusions in widely used datasets such as UNSW-NB15 and CICIDS017. Therefore, further research could explore the effectiveness of these techniques in other datasets to further evaluate their generalizability. Second, while the proposed model outperformed state-of-the-art methods in terms of accuracy and F
1
score, there is still room for improvement in terms of reducing false positives and false negatives. Future research could investigate how to further improve the model's performance by fine-tuning its parameters or developing new algorithms to address these issues. Third, the proposed model used a combination of RF, XGBoost, and Catboost as base learners with MLP as a meta-learner. Future research could explore the effectiveness of other machine learning algorithms as base or meta-learners to enhance the model's performance. Finally, while the proposed model achieved good results in both binary and multi-classification, it would be interesting to investigate how to improve the performance in specific types of attacks or in more complex scenarios such as detecting attacks in real-time or in highly dynamic environments.
One limitation of our study is its lack of explainability. The model’s decisions and predictions cannot be easily interpreted and understood, making it challenging to identify the root causes of errors or biases. This may limit the paper's practical applications in sensitive domains where interpretability is essential, such as healthcare or finance.
Data availability
Not applicable.
Notes
We do not consider embedded methods for feature selection to keep the computational cost of our methodology as low as possible, given that we will be building and testing our IDS on a significantly large benchmark dataset.
Based on a few rounds of experiments, we used MI ≥ 0.1 for this step.
Abbreviations
RF:
Random forest
BN:
Bayes net
RT:
Random tree
DT:
Decision tree
LR:
Logistic regression
KNN:
K-nearest neighbor
SVM:
Support vector machine
ELM:
Extreme learning machine
XGBoost:
EXtreme gradient boosting
Adaboost:
Adaptive boosting
GBM:
Gradient boosting machine
LightGBM:
Light gradient boosting machine
RT:
Random tree
DS:
Decision stump
ET:
Extra tree
IG:
Information gain
PIO:
Pigeon-inspired optimizer
PSO:
Particle swarm optimization
BOA:
Bat optimization algorithm
GWO:
Grey wolf optimization
RFE:
Recursive feature elimination
CS:
Cuckoo search
SFS:
Sequential forward selection
GTO:
Gorilla troops optimizer
BSA:
Bird swarms algorithm
UMAP:
Uniform manifold approximation and projection
NRS:
Neighborhood rough set
SSA:
Salp swarm algorithm
TS:
Tabu search
RNN:
Recurrent neural network
LSTM:
Long short-term memory
MLP:
Multilayer perceptron
ANN:
Artificial neural network
CNN:
Convolutional neural network
DNN:
Deep neural network
AE:
Autoencoder
DAE:
Denoising autoencoder
VAE:
Variational autoencoder
DNN:
Deep neural network
GRU:
Gated recurrent units
Bi-LSTM:
Bidirectional long-short term memory
BPNN:
Back propagation neural network
RBFN:
Radial basis function network
GBM:
Gradient boosting machine
References
Leszczyna R, Wallis T, Wróbel MR. Developing novel solutions to realise the European energy—information sharing & analysis centre. Decis Support Syst. 2018;122:2019.
https://doi.org/10.1016/j.dss.2019.05.007
.
Article
Google Scholar
Zhang H, Chari K, Agrawal M. Decision support for the optimal allocation of security controls. Decis Support Syst. 2018;115:92–104.
https://doi.org/10.1016/j.dss.2018.10.001
.
Article
Google Scholar
Zadeh A, Jeyaraj A. A multistate modeling approach for organizational cybersecurity exploration and exploitation. Decis Support Syst. 2022;162(August):113849.
https://doi.org/10.1016/j.dss.2022.113849
.
Article
Google Scholar
Li S, Iqbal M, Saxena N. Future industry internet of things with zero-trust security. Inf Syst Front. 2022.
https://doi.org/10.1007/s10796-021-10199-5
.
Article
Google Scholar
Ge M, Syed NF, Fu X, Baig Z, Robles-Kelly A. Towards a deep learning-driven intrusion detection approach for internet of things. Comput Netw. 2021.
https://doi.org/10.1016/j.comnet.2020.107784
.
Article
Google Scholar
Derhab A, Belaoued M, Mohiuddin I, Kurniawan F, Khan MK. Histogram-based intrusion detection and filtering framework for secure and safe in-vehicle networks. IEEE Trans Intell Transp Syst. 2022;23(3):2366–79.
https://doi.org/10.1109/TITS.2021.3088998
.
Article
Google Scholar
Safaldin M, Otair M, Abualigah L. Improved binary gray wolf optimizer and SVM for intrusion detection system in wireless sensor networks. J Ambient Intell Humaniz Comput. 2021;12(2):1559–76.
https://doi.org/10.1007/s12652-020-02228-z
.
Article
Google Scholar
Kumar P, Dwivedi YK, Anand A. Responsible artificial intelligence ( AI ) for value formation and market performance in healthcare: the mediating role of patient ’ s cognitive engagement. Inf Syst Front. 2021.
https://doi.org/10.1007/s10796-021-10136-6
.
Article
Google Scholar
McLeod A, Dolezel D. Cyber-analytics: modeling factors associated with healthcare data breaches. Decis Support Syst. 2018;108:57–68.
https://doi.org/10.1016/j.dss.2018.02.007
.
Article
Google Scholar
Khammassi C, Krichen S. A GA-LR wrapper approach for feature selection in network intrusion detection. Comput Secur. 2017;70:255–77.
https://doi.org/10.1016/j.cose.2017.06.005
.
Article
Google Scholar
Elhag S, Fernández A, Bawakid A, Alshomrani S, Herrera F. On the combination of genetic fuzzy systems and pairwise learning for improving detection rates on intrusion detection systems. Expert Syst Appl. 2015;42(1):193–202.
https://doi.org/10.1016/j.eswa.2014.08.002
.
Article
Google Scholar
Liang W, Xiao L, Zhang K, Tang M, He D, Li KC. Data fusion approach for collaborative anomaly intrusion detection in blockchain-based systems. IEEE Internet Things J. 2022;9(16):14741–51.
https://doi.org/10.1109/JIOT.2021.3053842
.
Article
Google Scholar
Jiang K, Wang W, Wang A, Wu H. Network intrusion detection combined hybrid sampling with deep hierarchical network. IEEE Access. 2020;8(3):32464–76.
https://doi.org/10.1109/ACCESS.2020.2973730
.
Article
Google Scholar
Mukhopadhyay A, Chatterjee S, Bagchi KK, Kirs PJ, Shukla GK. Cyber risk assessment and mitigation (CRAM) framework using logit and probit models for cyber insurance. Inf Syst Front. 2019;21(5):997–1018.
https://doi.org/10.1007/s10796-017-9808-5
.
Article
Google Scholar
Tchernykh A, Schwiegelsohn U, Ghazali Talbi E, Babenko M. Towards understanding uncertainty in cloud computing with risks of confidentiality, integrity, and availability. J Comput Sci. 2019.
https://doi.org/10.1016/j.jocs.2016.11.011
.
Article
Google Scholar
Stampar M, Fertalj K. “Artificial intelligence in network intrusion detection. 2015 38th Int Conv Inf Commun Technol Electron Microelectron. 2015.
https://doi.org/10.1109/MIPRO.2015.7160479
.
Article
Google Scholar
Muhuri PS, Chatterjee P, Yuan X, Roy K, Esterline A. Using a long short-term memory recurrent neural network (LSTM-RNN) to classify network attacks. Inf. 2020;11(5):1–21.
https://doi.org/10.3390/INFO11050243
.
Article
Google Scholar
Wan J, et al. An efficient impersonation attack detectionmethod in fog computing. Comput Mater Contin. 2021;68(1):268–81.
https://doi.org/10.32604/cmc.2021.016260
.
Article
Google Scholar
Pranto MB, Ratul MHA, Rahman MM, Diya IJ, Bin Zahir Z. Performance of machine learning techniques in anomaly detection with basic feature selection strategy-a network intrusion detection system. J Adv Inf Technol. 2022;13(1):36–44.
https://doi.org/10.1272/jait.13.1.36-44
.
Article
Google Scholar
Ozkan-Okay M, Samet R, Aslan O, Gupta D. A comprehensive systematic literature review on intrusion detection systems. IEEE Access. 2021;9:157727–60.
https://doi.org/10.1109/ACCESS.2021.3129336
.
Article
Google Scholar
Wu C, Li W. Enhancing intrusion detection with feature selection and neural network. Int J Intell Syst. 2021;36(7):3087–105.
https://doi.org/10.1002/int.22397
.
Article
Google Scholar
Iwendi C, Khan S, Anajemba JH, Mittal M, Alenezi M, Alazab M. The use of ensemble models for multiple class and binary class classification for improving intrusion detection systems. Sensors. 2020;20(9):1–37.
https://doi.org/10.3390/s20092559
.
Article
Google Scholar
Kurniabudi DS, Darmawijoyo MY, Idris BB, Bamhdi AM, Budiarto R. CICIDS-2017 dataset feature analysis with information gain for anomaly detection. IEEE Access. 2020;8:132911–21.
https://doi.org/10.1109/ACCESS.2020.3009843
.
Article
Google Scholar
Mebawondu JO, Alowolodu OD, Mebawondu JO, Adetunmbi AO. Network intrusion detection system using supervised learning paradigm. Sci Afr. 2020.
https://doi.org/10.1016/j.sciaf.2020.e00497
.
Article
Google Scholar
Mebawondu OJ, Popoola OS, Ayogu II, Ugwu CC, Adetunmbi AO. Network intrusion detection models based on naives bayes and c4.5 algorithms. Proc 2022 IEEE Niger 4th Int Conf Disruptive Technol Sustain Dev NIGERCON 2022. 2022.
https://doi.org/10.1109/NIGERCON54645.2022.9803086
.
Article
Google Scholar
Ahsan M, Gomes R, Chowdhury MM, Nygard KE. Enhancing machine learning prediction in cybersecurity using dynamic feature selector. J Cybersecurity Priv. 2021;1(1):199–218.
https://doi.org/10.3390/jcp1010011
.
Article
Google Scholar
Rashid MM, Kamruzzaman J, Ahmed M, Islam N, Wibowo S, Gordon S. Performance enhancement of intrusion detection system using bagging ensemble technique with feature selection. 2020 IEEE Asia-Pac Conf Comput Sci Data Eng. 2020.
https://doi.org/10.1109/CSDE50874.2020.9411608
.
Article
Google Scholar
Zheng X, Wang Y, Jia L, Xiong D, Qiang J. Network intrusion detection model based on Chi-square test and stacking approach. 2020 7th Int Conf Inf Sci Control Eng. 2020.
https://doi.org/10.1109/ICISCE50968.2020.00185
.
Article
Google Scholar
Oriola O. A stacked generalization ensemble approach for improved intrusion detection. Int J Comput Sci Inf Secur. 2020;18(5):62–7.
Google Scholar
Abbas A, Khan MA, Latif S, Ajaz M, Shah AA, Ahmad J. A new ensemble-based intrusion detection system for internet of things. Arab J Sci Eng. 2021;47(2):1805–19.
https://doi.org/10.1007/s13369-021-06086-5
.
Article
Google Scholar
Guyon I, Elisseeff A. An introduction to variable and feature selection. J Mach Learn Res. 2003;3:1157–82.
https://doi.org/10.1016/j.aca.2011.07.027
.
Article
Google Scholar
Chandrashekar G, Sahin F. A survey on feature selection methods. Comput Electr Eng. 2014;40(1):16–28.
https://doi.org/10.1016/j.compeleceng.2013.11.024
.
Article
Google Scholar
Mirlashari M, Rizvi SAM. Feature selection technique-based network intrusion system using machine learning. 2020 IEEE World Conf Appl Intell Comput. 2022.
https://doi.org/10.1109/AIC55036.2022.9848861
.
Article
Google Scholar
Sharma NV, Yadav NS. An optimal intrusion detection system using recursive feature elimination and ensemble of classifiers. Microprocess Microsyst. 2021;85:104293.
https://doi.org/10.1016/j.micpro.2021.104293
.
Article
Google Scholar
Alazzam H, Sharieh A, Sabri KE. A feature selection algorithm for intrusion detection system based on Pigeon inspired optimizer. Expert Syst Appl. 2020.
https://doi.org/10.1016/j.eswa.2020.113249
.
Article
Google Scholar
Zhang L, Xu C. A intrusion detection model based on convolutional neural network and feature selection. 2022 5th Int Conf Artif Intell Big Data. 2022.
https://doi.org/10.1109/ICAIBD55127.2022.9820384
.
Article
Google Scholar
Imran M, Khan S, Hlavacs H, Khan FA, Anwar S. Intrusion detection in networks using cuckoo search optimization. Soft Comput. 2022;26(20):10651–63.
https://doi.org/10.1007/s00500-022-06798-2
.
Article
Google Scholar
Ogundokun RO, Awotunde JB, Sadiku P, Adeniyi EA, Abiodun M, Dauda OI. An enhanced intrusion detection system using particle swarm optimization feature extraction technique. Procedia Comput Sci. 2021;193:504–12.
https://doi.org/10.1016/j.procs.2021.10.052
.
Article
Google Scholar
Alzubi QM, Anbar M, Sanjalawe Y, Al-Betar MA, Abdullah R. Intrusion detection system based on hybridizing a modified binary grey wolf optimization and particle swarm optimization. Expert Syst Appl. 2022;204:117597.
https://doi.org/10.1016/j.eswa.2022.117597
.
Article
Google Scholar
Narayanasami S, et al. Biological feature selection and classification techniques for intrusion detection on BAT. Wirel Pers Commun. 2021.
https://doi.org/10.1007/s11277-021-08721-8
.
Article
Google Scholar
Dwivedi S, Vardhan M, Tripathi S. Building an efficient intrusion detection system using grasshopper optimization algorithm for anomaly detection. Cluster Comput. 2021;24(3):1881–900.
https://doi.org/10.1007/s10586-020-03229-5
.
Article
Google Scholar
Kohavi R, John GH. Wrappers for feature subset selection. Artif Intell. 1997.
https://doi.org/10.1007/978-3-642-39038-8_27
.
Article
Google Scholar
Guyon I, Barnhill JWS. Gene selection for cancer classification using support vector machines. Lect Notes Comput Sci. 2008;5139:62–72.
https://doi.org/10.1007/978-3-540-88192-6_8
.
Article
Google Scholar
Kasongo SM, Sun Y. Performance analysis of intrusion detection systems using a feature selection method on the UNSW-NB15 dataset. J Big Data. 2020.
https://doi.org/10.1186/s40537-020-00379-6
.
Article
Google Scholar
Tang C, Luktarhan N, Zhao Y. An efficient intrusion detection method based on LightGBM and autoencoder. Symmetry. 2020;12(9):1–16.
https://doi.org/10.3390/sym12091458
.
Article
Google Scholar
Wang Z, Liu J, Sun L. EFS-DNN: an ensemble feature selection-based deep learning approach to network intrusion detection system. Secur Commun Netw. 2022.
https://doi.org/10.1155/2022/2693948
.
Article
Google Scholar
Yin Y, et al. IGRF-RFE: a hybrid feature selection method for MLP-based network intrusion detection on UNSW-NB15 dataset. J Big Data. 2023.
https://doi.org/10.1186/s40537-023-00694-8
.
Article
Google Scholar
Rajadurai H, Gandhi UD. A stacked ensemble learning model for intrusion detection in wireless network. Neural Comput Appl. 2020;34(18):15387–95.
https://doi.org/10.1007/s00521-020-04986-5
.
Article
Google Scholar
Liu H, Yu L. Toward integrating feature selection algorithms for classification and clustering. IEEE Trans Knowl Data Eng. 2005;17(4):491–502.
https://doi.org/10.1109/TKDE.2005.66
.
Article
Google Scholar
Nazir A, Khan RA. A novel combinatorial optimization based feature selection method for network intrusion detection. Comput Secur. 2021;102:102164.
https://doi.org/10.1016/j.cose.2020.102164
.
Article
Google Scholar
Rashid M, Kamruzzaman J, Imam T, Wibowo S, Gordon S. A tree-based stacking ensemble technique with feature selection for network intrusion detection. Appl Intell. 2022;52(9):9768–81.
https://doi.org/10.1007/s10489-021-02968-1
.
Article
Google Scholar
Mushtaq E, Zameer A, Khan A. A two-stage stacked ensemble intrusion detection system using five base classifiers and MLP with optimal feature selection. Microprocess Microsyst. 2022;94:104660.
https://doi.org/10.1016/j.micpro.2022.104660
.
Article
Google Scholar
Mokbal F, Dan W, Osman M, Ping Y, Alsamhi S. An efficient intrusion detection framework based on embedding feature selection and ensemble learning technique. Int Arab J Inf Technol. 2022;19(2):237–48.
https://doi.org/10.34028/iajit/19/2/11
.
Article
Google Scholar
Moustafa N, Slay J. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set). Mil Commun Inf Syst Conf. 2015.
https://doi.org/10.1109/MilCIS.2015.7348942
.
Article
Google Scholar
Moustafa N, Slay J. The evaluation of network anomaly detection systems: statistical analysis of the UNSW-NB15 data set and the comparison with the KDD99 data set. Inf Secur J. 2016;25(1–3):18–31.
https://doi.org/10.1080/19393555.2015.1125974
.
Article
Google Scholar
Sharafaldin I, Lashkari AH, Ghorbani AA. Toward generating a new intrusion detection dataset and intrusion traffic characterization. Conf Inf Syst Secur Priv. 2018.
https://doi.org/10.5220/0006639801080116
.
Article
Google Scholar
T. Hastie, R. Tibshirani, and J. Friedman, “The elements of statistical learning: data mining, inference, and prediction,”
Springer Sci. Bus. Media
, 2009.
Yoshua B, Ian G, Aaron C. Deep learning. Cambridge: MIT Press; 2015.
Google Scholar
Ganapathy S, Kulothungan K, Muthurajkumar S, Vijayalakshmi M, Yogesh L, Kannan A. Intelligent feature selection and classification techniques for intrusion detection in networks: a survey. Eurasip J Wirel Commun Netw. 2013;271(1):1–16.
https://doi.org/10.1186/1687-1499-2013-271
.
Article
Google Scholar
Liu H, Zhou M, Liu Q. An embedded feature selection method for imbalanced data classification. IEEE/CAA J Autom Sin. 2019;6(3):703–15.
https://doi.org/10.1109/JAS.2019.1911447
.
Article
Google Scholar
Battiti R. Using mutual information for selecting features in supervised neural net learning. IEEE Trans Neural Networks. 1994;5(4):537–50.
https://doi.org/10.1109/72.298224
.
Article
Google Scholar
Vergara JR, Estévez PA. A review of feature selection methods based on mutual information. Neural Comput Appl. 2014;24(1):175–86.
https://doi.org/10.1007/s00521-013-1368-0
.
Article
Google Scholar
W. Li, “Mutual Information Functions Versus Correlation Functions in Binary Sequences,” vol. 60, pp. 249–252, 1989,
https://doi.org/10.1007/978-1-4757-0623-9_35
.
Kursa MB, Rudnicki WR. Feature selection with the boruta package. J Stat Softw. 2010;36(11):1–13.
https://doi.org/10.18637/jss.v036.i11
.
Article
Google Scholar
Anand N, Sehgal R, Anand S, Kaushik A. Feature selection on educational data using Boruta algorithm. Int J Comput Intell Stud. 2021;10(1):27.
https://doi.org/10.1504/ijcistudies.2021.113826
.
Article
Google Scholar
Kursa MB, Jankowski A, Rudnicki WR. Boruta—a system for feature selection. Fundam Inform. 2010;101(4):271–85.
https://doi.org/10.3233/FI-2010-288
.
Article
MathSciNet
Google Scholar
Schapire RE. The strength of weak learnability. Mach Learn. 1990;5(2):197–227.
https://doi.org/10.1023/A:1022648800760
.
Article
Google Scholar
Bbeiman L. Bagging predictors LEO. Kluwer Acad Publ Boston Manuf Netherlands Bagging. 1996;24:123–40.
https://doi.org/10.3390/risks8030083
.
Article
Google Scholar
Wolpert DH. Stacked generalization. Neural Netw. 1992;5:241–55.
Article
Google Scholar
Sagi O, Rokach L. Ensemble learning: a survey. Wiley Interdiscip Rev Data Min Knowl Discov. 2018;8(4):1–18.
https://doi.org/10.1002/widm.1249
.
Article
Google Scholar
Galar M, Fernandez A, Barrenechea E, Bustince H, Herrera F. A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches. IEEE Trans Syst Man Cybern. 2012;42(4):463–84.
Article
Google Scholar
Džeroski S, Ženko B. Is combining classifiers with stacking better than selecting the best one? Mach Learn. 2004;54(3):255–73.
https://doi.org/10.1023/B:MACH.0000015881.36452.6e
.
Article
Google Scholar
Breiman L. Random forests. Mach Learn. 2001;45(1):5–32.
Article
Google Scholar
Chen T, Guestrin C. XGBoost: a scalable tree boosting system. Proc 22nd ACM SIGKDD Int Conf Knowl Discov Data Min. 2016.
https://doi.org/10.1145/2939672.2939785
.
Article
Google Scholar
L. Prokhorenkova, G. Gusev, A. Vorobev, A. V. Dorogush, and A. Gulin, “Catboost: Unbiased boosting with categorical features,”
Adv. Neural Inf. Process. Syst.
, pp. 6638–6648, 2018.
Kareem SS, Mostafa RR, Hashim FA, El-Bakry HM. An effective feature selection model using hybrid metaheuristic algorithms for iot intrusion detection. Sensors. 2022;22(4):1–23.
https://doi.org/10.3390/s22041396
.
Article
Google Scholar
Du X, Cheng C, Wang Y, Han Z. Research on network attack traffic detection hybridalgorithm based on UMAP-RF. Algorithms. 2022;15(7):1–17.
https://doi.org/10.3390/a15070238
.
Article
Google Scholar
Louk MHL, Tama BA. Dual-IDS: a bagging-based gradient boosting decision tree model for network anomaly intrusion detection system. Expert Syst Appl. 2023;213(PB):119030.
https://doi.org/10.1016/j.eswa.2022.119030
.
Article
Google Scholar
Nkenyereye L, Tama BA, Lim S. A stacking-based deep neural network approach for effective network anomaly detection. Comput Mater Contin. 2020;66(2):2217–27.
https://doi.org/10.32604/cmc.2020.012432
.
Article
Google Scholar
Juan Fu J, Lan Zhang X. Gradient importance enhancement based feature fusion intrusion detection technique. Comput Netw. 2022;214:109180.
https://doi.org/10.1016/j.comnet.2022.109180
.
Article
Google Scholar
Tayde MV, Adhao RB, Pachghare V. Ensemble based feature selection technique for flow based intrusion detection system. 2022 IEEE 7th Int Conf Converg Technol. 2022.
https://doi.org/10.1109/I2CT54291.2022.9824425
.
Article
Google Scholar
Lazzarini R, Tianfield H, Charissis V. A stacking ensemble of deep learning models for IoT intrusion detection. Know-Based Syst. 2023;279:110941.
https://doi.org/10.1016/j.knosys.2023.110941
.
Article
Google Scholar
Yang Z, Liu Z, Zong X, Wang G. An optimized adaptive ensemble model with feature selection for network intrusion detection. Concurr Comput Pract Exp. 2022.
https://doi.org/10.1002/cpe.7529
.
Article
Google Scholar
Wang A, Wang W, Zhou H, Zhang J. Network intrusion detection algorithm combined with group convolution network and snapshot ensemble. Symmetry. 2021.
https://doi.org/10.3390/sym13101814
.
Article
Google Scholar
He H, Huang G, Zhang B, Qin L. Research on boruta-ET-based anomalous traffic detection model. Secur Commun Networks. 2022;2022:8.
https://doi.org/10.1155/2022/9169266
.
Article
Google Scholar
Harini R, Maheswari N, Ganapathy S, Sivagami M. An effective technique for detecting minority attacks in NIDS using deep learning and sampling approach. Alexandria Eng J. 2023;78(June):469–82.
https://doi.org/10.1016/j.aej.2023.07.063
.
Article
Google Scholar
Download references
Author information
Authors and Affiliations
Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran
Ali Mohammed Alsaffar & Mostafa Nouri-Baygi
Department of Computer Techniques Engineering, Imam Al-Kadhum College (IKC), Baghdad, Iraq
Ali Mohammed Alsaffar
School of Business Administration, University of Dayton, Dayton, OH, USA
Hamed M. Zolbanin
Authors
Ali Mohammed Alsaffar
View author publications
You can also search for this author in
PubMed
Google Scholar
Mostafa Nouri-Baygi
View author publications
You can also search for this author in
PubMed
Google Scholar
Hamed M. Zolbanin
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
A.M.A. and H.M.Z wrote the manuscript. A.M.A. was the lead in data preprocessing, model building, and evaluation. M.N.B. supervised the study and was a major contributor in model building and evaluation. All authors read and approved the final manuscript.
Corresponding author
Correspondence to
Mostafa Nouri-Baygi
.
Ethics declarations
Competing interests
The authors declare no competing interests.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by-nc-nd/4.0/
.
Reprints and permissions
About this article
Cite this article
Alsaffar, A.M., Nouri-Baygi, M. & Zolbanin, H.M. Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning.
J Big Data
11
, 133 (2024). https://doi.org/10.1186/s40537-024-00994-7
Download citation
Received
:
24 August 2023
Accepted
:
27 August 2024
Published
:
18 September 2024
DOI
:
https://doi.org/10.1186/s40537-024-00994-7
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Intrusion detection system
Machine learning
Feature selection
Stacked ensemble
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00992-9):
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
17 September 2024
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
Zheng Li
1
,
Xiaojie Zhang
1
,
Chongyuan Sun
1
,
Zefeng Li
1
,
He Fei
1
&
…
Dongbing Zhao
1
Show authors
Journal of Big Data
volume
11
, Article number:
132
(
2024
)
Cite this article
93
Accesses
Metrics
details
Abstract
Background
The tumor microenvironment (TME) provides a region for intricate interactions within or between immune and non-immune cells. We aimed to reveal the tissue architecture and comprehensive landscape of cells within the TME of colorectal cancer (CRC).
Methods
Fresh frozen invasive adenocarcinoma of the large intestine tissue from 10× Genomics Datasets was obtained from BioIVT Asterand. The integration of microarray-based spatial transcriptomics (ST) and RNA sequencing (RNA-seq) was applied to characterize gene expression and cell landscape within the TME of CRC tissue architecture. Multiple R packages and deconvolution algorithms including MCPcounter, XCELL, EPIC, and ESTIMATE methods were performed for further immune distribution analysis.
Results
The subpopulations of immune and non-immune cells within the TME of the CRC tissue architecture were appropriately annotated. According to ST and RNA-seq analyses, a heterogeneous spatial atlas of gene distribution and cell landscape was comprehensively characterized. We distinguished between the cancer and stromal regions of CRC tissues. As expected, epithelial cells were located in the cancerous region, whereas fibroblasts were mainly located in the stroma. In addition, the fibroblasts were further subdivided into two subgroups (F1 and F2) according to the differentially expressed genes (DEGs), which were mainly enriched in pathways including hallmark-oxidative-phosphorylation, hallmark-e2f-targets and hallmark-unfolded-protein-response. Furthermore, the top 5 DEGs, SPP1, CXCL10, APOE, APOC1, and LYZ, were found to be closely related to immunoregulation of the TME, methylation, and survival of CRC patients.
Conclusions
This study characterized the heterogeneous spatial landscape of various cell subtypes within the TME of the tissue architecture. The TME-related roles of fibroblast subsets addressed the potential crosstalk among diverse cells.
Background
Colorectal cancer (CRC) is a common malignancy and is the second leading cause of cancer-related mortality worldwide. According to an estimation, more than 1.9 million new CRC cases and 935,000 deaths occurred in 2020 [
1
]. Among CRC cases, early onset disease (at < 50 years of age) accounts for 10%, and its incidence is increasing, particularly in high-income countries. Screening for CRC is now recommended beginning at 45 years of age [
2
]. CRC can be characterized by symptoms including changes in bowel habits, abdominal pain, and sometimes blood in the stool [
3
]. The prognosis of CRC has improved due to advancements in treatment, including surgery, chemotherapy, and radiation therapy. However, tumors often develop resistance to treatment because of intratumoral heterogeneity and clonal evolution. CRC has served as a genetic and biological paradigm for the evolution of solid tumors. Genotyping of tumor tissues according to supportive information provided by somatic genetic alterations has become a routine examination in clinical practice [
4
,
5
].
Tumor microenvironment (TME) denotes the non-cancerous cells and components surrounding the tumor cells, including malignant cells, fibroblasts, tumor vasculature, lymphocytes, dendritic cells, and molecules produced and released by them [
6
,
7
]. TME has contributed to revealing and comprehending the roles of non-genetic and non-cellular intrinsic factors in cancer development [
8
]. Constant cross-talk between tumor cells and the TME plays decisive roles in tumor proliferation, progression, metastasis, and response to therapies [
9
]. Accordingly, the TME is now considered a therapeutic target in cancer, attracting increasing research and clinical investigation.
Owing to the technological development of next-generation sequencing- and imaging-based approaches, spatial transcriptomics (ST) can now be used to comprehensively elucidate the expression levels of all or selected genes throughout the tissue space. Biological insights into a range of disease contexts can be achieved by integrating ST and single-cell RNA sequencing (scRNA-seq) [
10
,
11
]. Emerging studies have investigated the spatial heterogeneity in pancreatic ductal adenocarcinoma [
12
], bone marrow niche organization [
13
], liver [
14
], squamous cell carcinoma [
15
], mammalian spermatogenesis [
16
], breast cancer [
17
], lung cancer [
18
], prostate cancer [
19
], dorsal root ganglia [
20
], cervical squamous cell carcinoma [
21
], esophageal squamous cell carcinoma [
22
], and colorectal cancer [
23
]. Although previous studies have focused on the spatial heterogeneity of the TME in colorectal cancer, cancer-associated fibroblasts (CAF) have received much attention because comprehensive insights into diverse subpopulations are insufficient and more supportive data are needed.
Given these considerations, we conducted an analysis to characterize the heterogeneous spatial landscape of various subpopulations within the TME of CRC tissues by integrating ST and RNA-seq. Subsets of fibroblasts were identified to reveal potential crosstalk among diverse cells. This comprehensive landscape and cellular architecture may provide novel insights into the advancement of CRC management in the future.
Methods
Patients and sample collection
In this study, spatial gene expression data were acquired by visiting the 10 × Genomics Datasets (
https://www.10xgenomics.com
). Spatial imaging data and feature/barcode matrix HDF5 (filtered) documents were investigated from “Space Ranger 1.2.0: Human Colorectal Cancer: Whole Transcriptome Analysis (Visium Spatial Targeted Demonstration (v1 Chemistry))”. Freshly frozen invasive adenocarcinoma of the large intestine tissue was obtained from BioIVT Asterand. The tissue was classified as T4aN0M0 (stage IIB) according to the 8th edition of the staging system issued by the American Joint Committee on Cancer (AJCC) and Union for International Cancer Control (UICC). The available 10 × genomics data used for validation study was 5 µm section from Human Intestinal Cancer named Human Intestine Cancer (FPPE). The FFPE tissue was purchased from BioIVT Asterand Human Tissue Specimens. All data analyzed in this study were available from the aforementioned open-access website; therefore, ethics approval and informed consent were waived by the National Cancer Center in China.
Tissue image preparation
The tissue was embedded and cryosectioned as described in the Visium Spatial Protocols Tissue Preparation Guide (Demonstrated Protocol CG000240). Tissue sections of 10 µm were placed on Visium Gene Expression slides, fixed, and stained following methanol fixation, hematoxylin and eosin (H&E) staining, and imaging for visible spatial protocols (CG000160). H&E images were acquired using a Nikon Eclipse Ti2-E microscope with the following settings: (a) color camera, (b) 10× objective; (c) Numerical Aperture:0.45, and (d) exposure:20 ms. The detailed description of the data process in this section was applied in the user guide (
https://cdn.10xgenomics.com/image/upload/v1660261286/support-documents/CG000238_VisiumSpatialTissueOptimizationUserGuide_RevE.pdf
).
Gene expression library preparation and sequencing
The Visium Gene Expression library (T1T2-E8) was prepared as described in the Visium Spatial Reagent Kit User Guide (CG000239 Rev D). Sequencing data were processed using Space Ranger. The specific parameters were as follows: (a) sequencing instrument: Illumina NovaSeq 6000, flow cell HHYWHDSXY (lanes 1–4); (b) sequencing depth: 112,228 mean reads per cell; (c) sequencing configuration: paired-end (28 × 90), Dual-Indexed Sequencing. Read 1: 28 cycles (16 bp barcode, 12 bp UMI); i7 index: 10 cycles; i5 index: 10 cycles; Read 2: 90 cycles (transcript); (d) Slide: V10A13-206; (e) Area: C1. The key metrics were as follows: (a) spots detected,3,138; (b) median genes per spot,3,538; (c) Median UMI counts per spot:8,906. The detailed description of the data process in this section was applied in the user guide (
https://cdn.10xgenomics.com/image/upload/v1660261286/support-documents/CG000239_Visium_Spatial_Gene_Expression_User_Guide_Rev_F.pdf
).
Processing of RNA sequencing data
The RNA-seq data were processed and visualized using the R packages Seurat, ggplot2, cowplot, dplyr, and hdf5r [
24
]. The SCTransform function was used to normalize the data (assay = “Spatial”). Dimensionality reduction clustering was then performed using RunPCA, RunUMAP, and RunTSNE functions (dims = 1:30). The cell types were annotated according to the previously reported cell type gene markers as follows: EPCAM-Epithelial, PECAM1-Endothelial, COL3A1-Fibroblasts, AIF1-Macrophage, CD79A-B cell, JCHAIN-Plasma cell, CD4-T cell, AKT3 and AXL-NK cells, and PTPRC-Immune cells. Fibroblasts were further extracted and classified into new clusters of c0, c1, c2, c3, c4, and c5, using the FindClusters function (resolution = 0.4). It should be claimed that the new c0-c4 were different from c0-c17 in the beginning of the study. Based on the results of the differential expression analysis, we classified fibroblasts into F1 (c1) and F2 (c0, c2, c3, and c4). The heatmap of differentially expressed genes (DEGs) were visualized using the DoHeatmap function. The FindAllMarkers function was performed to identify the top five DEGs between F1 and F2 clusters (min.pct = 0.25, logfc.threshold = 0.25, test.use = "wilcox"). The R packages irGSEA and UCell were applied to conduct Gene Set Enrichment Analysis (GSEA) of RNA-seq data from patients with colorectal cancer[
25
]. The gene set enrichment score was calculated using the irGSEA score function (assay = “Spatial”, seeds = 123, ncores = 1, min.cells = 3, min.feature = 0, msigdb = T, species = “Homo sapiens”, category = “H”, kcdf = “Gaussian”).
Spatial transctiptomic analysis
The R packages Seurat, dplyr, and hdf5r were used for spatial transcriptomic (ST) analysis [
24
]. Tissue images were loaded using the Read10X_h5 function of Seurat. We visualized the count and features of gene expression using the VlnPlot function of R packages ggplot2 and cowplot. The structure of the tissue section was divided into cancer and stromal regions after being read by pathology experts. The spatial locations of epithelial cells, plasma cells, fibroblasts, B cells, T cells, endothelial cells, and NK cells were analyzed and visualized using the DimPlot function. We conducted ST analysis based on the method of multimodal intersection, which made a combination of gene expression modal and spatial architecture modal of colorectal cancer tissue sections. After identifying the top five DEGs (tDEGs), the spatial location of expression of these genes was determined using the SpatialFeaturePlot function (alpha = c (0.5, 1)). In addition, the immunohistochemical (IHC) and fluorescent staining images of these genes, which were available in the Human Protein Atlas (HPA) database (
https://www.proteinatlas.org/
), were also displayed to demonstrate the intracellular expression sites of these genes.
Immunological, methylation, function enrichment and survival related analyses
In this section, mRNA transcriptomic data of colorectal cancer were investigated from cohorts within The Cancer Genome Atlas (TCGA) database. The standardized pan-cancer dataset TCGA TARGET GTEx (PANCAN, N = 19,131, G = 60,499) was downloaded from the UCSC (
https://xenabrowser.net/
). Gene expression data for colon adenocarcinoma/rectum adenocarcinoma esophageal carcinoma (TCGA-COAD/READ) were selected for subsequent analysis. Furthermore, we extracted the expression data of the tDEGs in each sample and transformed each expression value into log2 (x + 0.001). The mcpcounter, XCELL, EPIC, and ESTIMATE methods in the R package IOBR was used to estimate the population assumption of tumor-infiltrating immune and spatial cell populations [
26
]. Additionally, gene markers of immunoregulatory pathways, including chemokines, receptors, MHC, immunoinhibitors, and immunostimulators, were extracted to calculate Pearson’s correlation with tDEGs. We performed a similar correlation analysis using the expression values of the tDEGs and RNA-moderated genes (m1A, m5C, and m6A). Mutation data (MuTect2) were processed and visualized using the R packages, maftools and ComplexHeatmap [
27
,
28
]. Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were conducted using the R packages clusterProfilerorg and org.Hs.eg.db [
29
]. The least absolute shrinkage and selection operator (LASSO) regression algorithm and multivariate Cox regression analysis were used to construct a prognostic model of survival using the R package glmnet [
30
]. The log-rank test was used to compare differences in survival between groups. Time-ROC analysis was used to compare the predictive accuracy of the risk score. The Spearman correlation between the model and immune cells was analyzed using the R package ggstatsplot.
Statistical analysis
All analyses in this study were conducted using R software for Windows. In the immune infiltrating analysis, we extracted the expression data of the tDEGs in each sample and transformed each expression value into log2 (x + 0.001) after filtering all normal samples. Then, Spearman’s correlation was calculated in each subgroup [
31
]. In the similar method, Pearson correlation was performed for methylation and immune regulation analysis [
32
]. The chi square test was utilized to evaluate the difference in gene mutation frequency in each group of samples [
33
]. Considering the survival analysis, we took the first step to convert counts data to TPM and normalize the data log2 (TPM + 1), keeping samples with clinical information at the same time. For Kaplan–Meier curves, p-values and hazard ratio (HR) with 95% confidence interval (CI) were generated by log-rank tests and univariate cox proportional hazards regression, which were used to compare differences in survival between the groups [
34
]. The timeROC analysis was conducted to further compare the predictive accuracy. LASSO with tenfold cross-validation was used for feature selection [
35
]. Statistical analyses with two sides were performed, and all the difference with
P
< 0.05 was considered statistically significant.
Results
Analysis of identity and density of gene expression distribution
The design of this study is briefly summarized and illustrated in Fig.
1
A. After the RNA-seq profiling data, 18 clusters (cluster 0 to cluster 17) were identified automatically by Seurat. The gene expression and gene number of these clusters showed a wide range of changes according to the violin plots (Fig.
1
B, D). We further explored the spatial location of the density of gene expression distribution. As shown, there were more genes enriched in cancer regions with relatively high expression compared with stromal regions (Fig.
1
C, E). Additionally, the mitochondrial gene was not detected in the tissue, which indicated the high quality of the RNA-seq data (Fig. S1).
Fig. 1
Flow chart and pre-analysis of colorectal cancer (CRC).
A
The simple flow chart of study design.
B
,
C
Expression in various identities and spatial architecture.
D
,
E
Gene count in various identities and spatial architecture
Full size image
Annotation of celltypes within RNA-seq
In this section, UMAP is applied to demonstrate 18 clusters (Fig.
2
A). The spatial location of these clusters was well marked and provided a comprehensive display of the colorectal cancer tissue (Fig.
2
B). According to the gene markers for certain cell clusters reported previously, we annotated the cell groups based on the expression of marker genes (MGs) as follows: EPCAM-Epithelial, PECAM1-Endothelial, COL3A1-Fibroblasts, AIF1-Macrophage, CD79A-B cells, JCHAIN-Plasma cells, CD4-T cells, AKT3 and AXL-NK cells, and PTPRC-Immune cells. Subsequently, we revealed the TSNE map of the corresponding cell types and genes in proper order (Fig.
2
C–L). A bubble plot was then used to demonstrate the average expression and expressed percentage of MGs in clusters of 0 to 17 (Fig.
2
M). The level of MGs in these clusters was also displayed using a violin plot (Fig. S2). The top three most prominently expressed genes in each cluster were investigated and displayed using a heatmap plot. From this analysis, we could clearly distinguish the various gene modules expressed in different clusters (Fig.
2
N).
Fig. 2
Annotation of celltypes within RNA-seq.
A
UMAP visualization of RNA-seq.
B
Spatial location of identities within CRC.
C–L
TSNE plots of diverse gene markers.
M
Average expression levels of gene markers in identities.
N
Top 3 genes with significantly different expression in each identity
Full size image
The integration of rank-based gene set enrichment analysis
Pathology experts were consulted to determine the cancerous and stromal regions of the tissue. As shown in Fig.
3
A, the relatively dark areas surrounded by red dotted lines were identified as regions of colon cancer, whereas the lighter areas without red dotted lines were stromal regions. The distribution of annotated cell clusters was represented comprehensively, including epithelial cells, plasma cells, fibroblasts, B cells, T cells, endothelial cells, and NK cells within the tissue architecture (Fig.
3
B). In combination with tissue imaging, a large number of epithelial cells were located in the cancer region, while fibroblasts were mainly located in the stroma, which was in line with our expectations. We then determined the DEGs with increased or decreased expression or no significant difference within the aforementioned clusters using AUCell, UCell, singscore, and ssgsea (Fig.
3
C). Furthermore, we investigated the correlation between these clusters and the inflammatory response using UCell. The density distribution of UCell within the aforementioned clusters is shown in Fig.
3
D. In addition, PCA, tSEN, and UMAP methods were applied to show the results of the dimension reduction analysis (Fig.
3
E–G). The UCell scores of hallmark-inflammatory-response in these clusters were calculated and are shown in Fig.
3
H, I. Subsequently, we explored the correlation between clusters and a variety of HALLMARK pathways using the robust rank aggregation (RRA) method, which can comprehensively evaluate the results of difference analysis and screen out gene sets that are significantly enriched in most gene set enrichment analysis methods. According to the results, the top 3 relevant pathways were hallmark-oxidative-phosphorylation, hallmark-e2f-targets and hallmark-unfolded-protein-response (Fig.
3
J).
Fig. 3
The integration of rank-based gene set enrichment analysis.
A
Distribution of cancer and stromal regions.
B
Spatial architecture of celltypes within CRC.
C
Analysis of genes with significantly different expression.
D
UCell’s distribution map of celltypes in inflammatory pathway.
E–G
PCA, tSEN and UMAP maps of inflammatory pathway.
H, I
Ucell score maps of inflammatory pathway.
J
Correlation analysis of pathway in clusters
Full size image
Analysis of fibroblasts subpopulation in CRC tissue
Fibroblasts are important components of RNA sequencing results. The UMAP plot is shown and the fibroblasts are circled by a dotted line (Fig.
4
A). From the visualization of the PCA and UMAP results, fibroblasts were classified into subpopulations F1 and F2 (Fig.
4
B, C). To verify if immune cells were mixed in the F1 and F2, we demonstrated the expression of the gene makers of each cell type in extracted fibroblasts (c0-c4), indicating no mixture of immune cells was involved (
Fig S3
). We then explored the spatial location of the two fibroblast subtypes F1 and F2 within CRC tissue. The F1 subtype was mainly located in the stromal region, whereas a portion of the F2 subtype was located in the cancer region (Fig.
4
D). To investigate the differences and heterogeneity in biological genetics, we analyzed differentially expressed genes (DEGs). According to the results of DEGs, there were five top genes with the most differential expression between F1 and F2 after weighted analysis, with the top five differentially expressed genes (tDEGs) identified, including SPP1, CXCL10, APOE, APOC1, and LYZ (Fig.
4
E). The expression of tDEGs in the 18 identities mentioned above was visualized (Fig. S4A–E). tDEGs were enriched in identity 13 (Fig S4F). We further showed the spatial landscapes of the expression of these five genes and compared them with those of the original CRC tissue. We found that the tDEGs were mainly expressed in the stromal region of the tissue (Fig.
4
F–K). Additionally, the UMAP plots of tDEGs are shown (Fig.
4
L–P). To further validate these findings, we applied another sample of large intestine CRC named Human Intestine Cancer (FPPE) from 10 × genomics database. The information of CRC samples and datasets used in the study was summarized in Table S1. The gene expression and gene number of the clusters of validation sample showed a wide range of changes according to the violin plots (Fig. S5A, C). We further explored the spatial location of the density of gene expression distribution. As shown, there were more genes enriched in cancer regions with relatively high expression compared with stromal regions (Fig. S5B, D). In further study of validation, we demonstrated the spatial distribution of tDEGs expression. It revealed that tDEGs were mainly enriched in the stromal area of the CRC tissue, which was consistent with previous findings (Fig. S5E–J).
Fig. 4
Analysis of fibroblasts subpopulation in CRC tissue.
A
Fibroblasts in UMAP visualization.
B, C
PCA and UMAP of fibroblasts clustered into F1 and F2.
D
The spatial location of F1 and F2.
E
Top five differentially expressed genes (tDEGs) identified by differential analysis
F–J
Spatial expression of SPP1, CXCL10, APOE, APOC1 and LYZ.
K
Original CRC tissue.
L–P
UMAP plots of SPP1, CXCL10, APOE, APOC1 and LYZ
Full size image
Immunoassay and methylation analyses of tDEGs
Immune infiltration analysis was performed to investigate the relationship between the tDEGs and various types of immune cells. Based on the COAD and COADREAD cohorts, we found that tDEGs were closely related to the immune infiltration of the tumor microenvironment (TME) within CRC (all
P
< 0.05). In addition to the fibroblasts, tDEGs were mainly related to cytotoxic lymphocytes and monocytic lineage according to the results (all
P
< 0.05) (Fig.
5
A). Additionally, 150 gene markers of immunoregulatory pathways, including chemokines (41 gene markers), receptors (18 gene markers), MHC (21 gene markers), immunoinhibitors (24 gene markers), and immunostimulators (46 gene markers) were extracted to calculate the Pearson correlation with the tDEGs. The results showed that tDEGs play important roles in the immunoregulatory pathways of the TME in CRC. There was a strong correlation between tDEGs and immunoregulation of the TME (all
P
< 0.05) (Fig.
5
B). Considering the important regulatory role of methylation on cell function and pathways, we extracted the expression of tDEGs and gene markers of three types of RNA modification methods, including m1A (10 gene markers), m5C (13 gene markers), and m6A (21 gene markers). We further classified the gene markers of methylation into writers, readers, and erasers, according to their functions. The correlation between tDEGs and methylation regulation was comprehensively demonstrated in this analysis (Fig.
5
C). Then, Xcell, EPIC, and ESTIMATE methods were performed to investigate immune infiltration. All these methods revealed that tDEGs were closely related to the TME of colorectal cancer tissue architecture. The detail data was provided as Tables S1, S2, S3.
Fig. 5
Immunoassay and methylation analyses of tDEGs.
A
Correlation analysis between tDEGs and TME components.
B
Correlation analysis between tDEGs and regulatory factors of TME.
C
Correlation analysis between tDEGs and modification including m1A, m5C and m6A
Full size image
Function enrichment analysis of the DEGs between F1 and F2
Gene Ontology (GO) analysis was conducted to elucidate the roles of the DEGs in cellular components (CC), molecular functions (MF), and biological processes (BP). According to the analysis, DEGs were mainly involved in extracellular structure organization of BP, collagen-containing extracellular matrix of CC, and extracellular matrix structural constituents of MF (Fig.
6
A). Gene Set Variation Analysis (GSVA) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis were performed. DEGs were mainly enriched in antigen processing and presentation, Epstein-Barr virus, phagosome, human T-cell leukemia virus 1 infection, and metabolic pathways (Figs.
6
B, C). The results of the GO and KEGG analyses are summarized in Table S2. Furthermore, we integrated the results of functional enrichment analysis and visualized the connection between them in one plot (Fig.
6
D).
Fig. 6
Function enrichment analysis of the DEGs between F1 and F2.
A
Gene Ontology (GO) analysis of the DEGs.
B, C
Gene Set Variation Analysis (GSVA) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis.
D
Connection plot of function enrichment analysis
Full size image
Potential survival prediction module and immunologic correlation analysis
To investigate the impact of tDEGs on the survival of patients with CRC, we established a risk score based on the expression of tDEGs using the Least Absolute Shrinkage and Selection Operator (LASSO). The overall survival (OS) trend that changed with the increase in the risk score is shown (Fig.
7
A). The Kaplan–Meier survival analysis of CRC patients with high- and low-risk scores was performed, indicating that the OS of patients with high-risk scores was poorer (HR = 1.896, 95%CI:1.266–2.839,
P
< 0.01) (Fig.
7
B). Then, a receiver operating characteristic (ROC) curve was established to evaluate the accuracy and specificity of the prediction for 1-year, 3-year, and 5-year OS (Fig.
7
C). In addition, the trends of progression-free survival (PFS) and disease-specific survival (DSS) change with risk score (PFS panel and DSS panel) were also demonstrated (Fig. S6A, D). Similar Kaplan–Meier and ROC curves for PFS and DSS were obtained (PFS: HR = 2.385, 95%CI:1.637–3.477,
P
< 0.01; DSS: HR = 2.717, 95%CI:1.575–4.687,
P
< 0.01) (Fig. S6B, C; E–F). Furthermore, we investigated the relationship between genes in the OS panel (gene panel: APOE, CXCL10, and SPP1) and eight gene markers related to immune checkpoints. It showed that CRC patients with low OS-panel expression were more correlated with immune checkpoints (Fig.
7
D–F). The waterfall diagram of tumor mutational burden (TMB) was used to explore the heterogeneity of genes within the OS panel (Fig. S7A–C). Spearman analysis was applied to explore the correlation between the OS panel and immune infiltration within the TME, including B cells, CD4
+
T cells, CD8
+
T cells, neutrophils, macrophages, and myeloid dendritic cells. Consequently, there was a significant positive correlation between the OS panel and CD4
+
T cells (
P
= 4.9e−05, Spearman = 0.19, 95%CI: [0.10, 0.28]), macrophages (
P
= 7.41e−11, Spearman = 0.30, 95%CI: [0.21, 0.38]), and myeloid dendritic cells (
P
= 9e−3, Spearman = 0.12, 95%CI: [0.03, 0.21]), while B cells had a significant negative correlation (
P
= 3e−2, Spearman = − 0.10, 95%CI: [− 0.19, − 0.01]) (Fig.
7
G–L).
Fig. 7
Potential survival prediction module and immunologic correlation analysis.
A
Correlation plot of risk score and OS-panel.
B
Kaplan–Meier survival analysis of OS-panel.
C
ROC curve of OS-panel.
D–F
Heatmaps of correlation between immune check points and APOE, CXCL10 and SPP1.
G–L
Spearman analyses of correlation between OS-panel and B cell, T cell CD4+, T cell CD8+, neutrophil, macrophage and myeloid dendritic cell. *
P
< 0.05, **
P
< 0.01, **
P
< 0.001, ***
P
< 0.0001
Full size image
Histocyte level analysis of tDEGs
For a deeper exploration of tDEGs, we searched the Human Protein Atlas (HPA) database (
https://www.proteinatlas.org
) for histological and subcellular-level information which was available. Immunohistochemical staining images demonstrated the spatial location of SPP1, APOE, and LYZ in CRC tissue (Fig.
8
A–C). According to the HPA database, SPP1 was detected in the Golgi apparatus and predicted to be secreted (Fig.
8
D). The cell atlas showing fluorescent staining of subcellular structures demonstrated the spatial distribution of the SPP1 protein, nucleus, and microtubules (Fig.
8
E). APOE was detected in vesicles and predicted to be secreted (Fig.
8
F). Fluorescent staining of the APOE protein, nucleus, and microtubules is shown (Fig.
8
G). LYZ was detected in the Golgi apparatus, actin filaments, and nucleoplasms (Fig.
8
H). Subcellular fluorescent staining also demonstrated the distribution characteristics of the LYZ protein and microtubules (Fig.
8
I).
Fig. 8
Histocyte level analysis of tDEGs.
A–C
Immunohistochemical staining (IHC) of SPP1, APOE and LYZ.
D–E
Intracellular distribution and fluorescence staining of SPP1.
F–G
Intracellular distribution and fluorescence staining of APOE.
H–I
Intracellular distribution and fluorescence staining of LYZ
Full size image
Discussions
In this study, we revealed the tissue architecture of stage IIB CRC and thoroughly characterized the heterogeneous spatial landscape of diverse subpopulations within the TME by integrating ST and RNA-seq. The expression landscape of all or selected genes at various spatial locations in the cancer and stromal regions, obtained through consultation with pathologists, was visualized. The identification of different fibroblast subtypes (F1 and F2) may address the potential interactions within the TME underlying CRC proliferation, progression, and metastasis. Immunoassay, methylation, and functional enrichment analyses of tDEGs further characterized the biological heterogeneity between F1 and F2, elucidating the roles of tDEGs in the TME. These findings reveal the tissue architecture and provide novel insights into the management of CRC.
Carcinomas are intricate heterocellular structures comprising epithelial cancer cells, stromal fibroblasts, and diverse immune cell populations. Interactions between these TME and cells facilitate cancer progression and influence the effectiveness of the existing therapies. The TME contributes to systemic inflammation, increases oxidative stress and fibrosis, and affects the cachectic state of CRC patients through inflammatory factors, including tumor necrosis factor alpha (TNFα) and certain chemokines such as interleukin IL-1 and IL-6 [
36
,
37
]. In this study, we annotated epithelial cells and related cells within the TME and illustrated the spatial distribution of diverse cell types. Our analysis revealed that the TME in CRC tissues is predominantly located in the stromal region. An important aspect of the interaction between various components is the inflammatory response pathway that is activated within the TME. We investigated the intricate interplay between epithelial cells and various components of the TME and the inflammatory response pathways. A significant number of genes exhibited differential expressions in response to these factors. According to the analysis of rank-based gene set enrichment, several genes that showed differential expression in various components were functionally enriched in the oxidative phosphorylation pathway. It has been shown that inhibition of oxidative phosphorylation (OXPHOS) can resist the hypoxic state in TME, thereby reducing the inhibitory effect on immune effector cells, increasing oxygenation of hypoxic tumor areas and reactivating the immune response. The prognostic improvement of OXPHOS inhibitors has been observed in cancer immunotherapy and radiation therapy [
38
]. Meanwhile, a previous study investigated the immune subtypes (C1–C6) of CRC and identified novel TME profiles. Among these, C2 exhibits greater activation of pathways associated with immune system function, apoptosis, DNA repair, mTOR signaling, and oxidative phosphorylation [
39
]. These findings contribute to our understanding of the interplay between various pathways and components within the TME of CRC.
To investigate the transformation of stromal cells in the TME by CRC cells, a study conducted single-cell sequencing and discovered that somatic cell copy number alterations (SCNAs) are widespread in immune cells, fibroblasts, and endothelial cells in both TME and normal tissues within each individual. Moreover, the percentage of fibroblasts with SCNAs was considerably greater in tumors (11.1–47.7%) than in adjacent normal tissues (1.1–10.6%) [
40
]. This led to our interest in fibroblasts in the TME of CRC. Fibroblasts can be divided into two parts, F1 and F2, according to the FindClusters function. Analysis of spatial transcriptomics revealed that F1 was mainly expressed in the stroma, whereas a portion of F2 was expressed in the cancer region of CRC. We identified a large number of genes that exhibited significantly different expression levels between F1 and F2 plants. Among these genes, SPP1, APOE, CXCL10, APOC1, and LYZ had the highest weights in the weighted analysis and were the top 5 differentially expressed genes (tDEGs). Spatial transcriptomics was performed to reveal the genetic and spatial heterogeneity of these two groups, demonstrating that tDEGs were mainly expressed in the stromal region of CRC tissue. An emerging study revealed a positive correlation between tumor-specific FAP (+) fibroblasts and SPP1 (+) macrophages, which were tightly localized, as demonstrated by immunofluorescence staining and spatial transcriptomics [
41
]. This interaction may be regulated by TGF-β and interleukin-1, which promote the formation of bridging protein structures that facilitate immunological rejection and restrict T-cell infiltration. Interestingly, they observed that patients with high FAP or SPP1 expression levels exhibited limited therapeutic benefits from anti-PD-L1 therapy. These findings suggest that disrupting the interaction between FAP (+) fibroblasts and SPP1 (+) macrophages may represent a promising therapeutic strategy to improve immunotherapy. APOE was found to be associated with lipid homeostasis and inflammation in the normal brain and could be a therapeutic target for Alzheimer's disease [
42
,
43
]. According to previous studies, CXCL10 is a novel therapeutic target for autoimmune diseases including inflammatory intestinal disease, multiple sclerosis, and rheumatoid arthritis [
44
]. In addition, CXCL10 is involved in the metastasis of colon cancer through activation of the PI3K/Akt pathway by CXCR3, leading to inhibition of GSK-3β phosphorylation and upregulation of Snail expression, thereby regulating epithelial mesenchymal transition in colon cancer cells [
45
]. APOC1 is considered a biomarker that indicates the prognosis of various cancer types [
46
,
47
,
48
,
49
]. A series of studies have revealed an association between LYZ and diseases, including temporal lobe epilepsy and ulcerative colitis [
50
,
51
].
RNA methylation modification is a key mechanism of epigenetic regulation in the immune response and tumorigenesis, and research on RNA methylation has become a hot topic in recent years. The four major RNA adenosine modifications include m(6)A, m(1)A, alternative polyadenylation, and adenosine-to-inosine RNA editing [
52
]. In this study, we examined the correlation between tDEGs and RNA methylation modifications including m1A, m5C, and m6A. tDEGs were shown to be strongly associated with changes in RNA methylation, suggesting a crucial role in regulation within the TME of CRC. Furthermore, we present landscapes that illustrate the immunomodulatory regulation and components within the TME, demonstrating the intricate immune interactions between tDEGs and the TME. Survival analysis, immunohistochemistry, and fluorescence staining further enriched the clinical translational significance and basic researching insight of the tDEGs and provided a deeper understanding at the molecular level. In the survival analysis, the receiver operating characteristic (ROC) value was observed to be lower than 0.7, indicating the predictive performance was not strong, nevertheless, this was one attempt at the clinical translational application of tDEGs and predicting survival status was not the key point of the study. In the Kaplan–Meier analysis, it revealed a significant different survival of High- and Low-score groups with
P
< 0.002, demonstrating a strong performance in predicting long-term survival. In general, more translational applications based on tDEGs in various areas are warranted in the feature.
Despite the use of relatively advanced ST analysis techniques in this study, which integrated multiple analytical methods to illustrate the spatial architecture of CRC and reveal the potential interactions of several TME components, there are still some limitations. First, due to database resource limitations, the sample included in this study was small, which may have led to an underrepresentation of the study. We believe that more patients will be enrolled in the study based on these findings. Second, immune-related cellular interactions within the TME were derived by cell sequencing, thus obtaining expression for correlation analysis as well as pathway analysis, lacking direct demonstration from pathway experiments. Third, although ST arrays can provide spatial transcriptomic data, their coverage and resolution are limited. Owing to the size of the ST array, it may not be possible to cover the entire tissue of interest. Additionally, the resolution may vary around single-cell level (1–10 cell) across different spots on the array. Furthermore, transcriptomic data are only accessible within each spot and information may be lost between adjacent spots. However, the development of ST technology offers the potential for higher resolutions and shorter intervals between spots in the future. This may enable researchers to capture the transcriptional profiles of individual cells within complex tissues more accurately.
Conclusions
This study characterized the tissue architecture of CRC by integrating RNA-seq and ST analyses, demonstrating spatial heterogeneity and potential cross-talk within the TME. Various landscapes have revealed a strong association between tDEGs, immunological regulation, and RNA methylation modification. The findings showing a comprehensive tissue structure and cellular landscape provide novel insights into the mechanism of CRC progression and the discovery of novel therapeutic targets.
Availability of data and materials
The datasets used in the current study are available from the corresponding author upon reasonable request.
References
Sung H, Ferlay J, Siegel RL, Laversanne M, Soerjomataram I, Jemal A, Bray F. Global Cancer Statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin. 2021;71:209–49.
Article
Google Scholar
Sinicrope FA. Increasing incidence of early-onset colorectal cancer. N Engl J Med. 2022;386:1547–58.
Article
Google Scholar
Jin J. Screening for colorectal cancer. JAMA. 2021;325:2026–2026.
Article
Google Scholar
Osumi H, Shinozaki E, Yamaguchi K, Zembutsu H. Clinical utility of circulating tumor DNA for colorectal cancer. Cancer Sci. 2019;110:1148–55.
Article
Google Scholar
Li J, Ma X, Chakravarti D, Shalapour S, DePinho RA. Genetic and biological hallmarks of colorectal cancer. Genes Dev. 2021;35:787–820.
Article
Google Scholar
Arneth B. Tumor microenvironment. Medicina (Kaunas, Lithuania) 2019; 56.
Xiao Y, Yu D. Tumor microenvironment as a therapeutic target in cancer. Pharmacol Ther. 2021;221: 107753.
Article
Google Scholar
Laplane L, Duluc D, Bikfalvi A, Larmonier N, Pradeu T. Beyond the tumour microenvironment. Int J Cancer. 2019;145:2611–8.
Article
Google Scholar
Bilotta MT, Antignani A, Fitzgerald DJ. Managing the TME to improve the efficacy of cancer therapy. Front Immunol. 2022;13: 954992.
Article
Google Scholar
Rao A, Barkley D, França GS, Yanai I. Exploring tissue architecture using spatial transcriptomics. Nature. 2021;596:211–20.
Article
Google Scholar
Longo SK, Guo MG, Ji AL, Khavari PA. Integrating single-cell and spatial transcriptomics to elucidate intercellular tissue dynamics. Nat Rev Genet. 2021;22:627–44.
Article
Google Scholar
Moncada R, Barkley D, Wagner F, Chiodin M, Devlin JC, Baron M, Hajdu CH, Simeone DM, Yanai I. Integrating microarray-based spatial transcriptomics and single-cell RNA-seq reveals tissue architecture in pancreatic ductal adenocarcinomas. Nat Biotechnol. 2020;38:333–42.
Article
Google Scholar
Baccin C, Al-Sabah J, Velten L, Helbling PM, Grünschläger F, Hernández-Malmierca P, Nombela-Arrieta C, Steinmetz LM, Trumpp A, Haas S. Combined single-cell and spatial transcriptomics reveal the molecular, cellular and spatial bone marrow niche organization. Nat Cell Biol. 2020;22:38–48.
Article
Google Scholar
Saviano A, Henderson NC, Baumert TF. Single-cell genomics and spatial transcriptomics: discovery of novel cell states and cellular interactions in liver physiology and disease biology. J Hepatol. 2020;73:1219–30.
Article
Google Scholar
Ji AL, Rubin AJ, Thrane K, Jiang S, Reynolds DL, Meyers RM, Guo MG, George BM, Mollbrink A, Bergenstråhle J, Larsson L, Bai Y, Zhu B, Bhaduri A, Meyers JM, Rovira-Clavé X, Hollmig ST, Aasi SZ, Nolan GP, Lundeberg J, Khavari PA. Multimodal analysis of composition and spatial architecture in human squamous cell carcinoma. Cell. 2020;182:497-514.e22.
Article
Google Scholar
Chen H, Murray E, Sinha A, Laumas A, Li J, Lesman D, Nie X, Hotaling J, Guo J, Cairns BR, Macosko EZ, Cheng CY, Chen F. Dissecting mammalian spermatogenesis using spatial transcriptomics. Cell Rep. 2021;37: 109915.
Article
Google Scholar
Andersson A, Larsson L, Stenbeck L, Salmén F, Ehinger A, Wu SZ, Al-Eryani G, Roden D, Swarbrick A, Borg Å, Frisén J, Engblom C, Lundeberg J. Spatial deconvolution of HER2-positive breast cancer delineates tumor-associated cell type interactions. Nat Commun. 2021;12:6012.
Article
Google Scholar
Larroquette M, Guegan JP, Besse B, Cousin S, Brunet M, Le Moulec S, Le Loarer F, Rey C, Soria JC, Barlesi F, Bessede A, Scoazec JY, Soubeyran I, Italiano A. Spatial transcriptomics of macrophage infiltration in non-small cell lung cancer reveals determinants of sensitivity and resistance to anti-PD1/PD-L1 antibodies. J Immunothera Cancer. 2022;10:e003890.
Article
Google Scholar
Berglund E, Maaskola J, Schultz N, Friedrich S, Marklund M, Bergenstråhle J, Tarish F, Tanoglidi A, Vickovic S, Larsson L, Salmén F, Ogris C, Wallenborg K, Lagergren J, Ståhl P, Sonnhammer E, Helleday T, Lundeberg J. Spatial maps of prostate cancer transcriptomes reveal an unexplored landscape of heterogeneity. Nat Commun. 2018;9:2419.
Article
Google Scholar
Tavares-Ferreira D, Shiers S, Ray PR, Wangzhou A, Jeevakumar V, Sankaranarayanan I, Cervantes AM, Reese JC, Chamessian A, Copits BA, Dougherty PM, Gereau RWT, Burton MD, Dussor G, Price TJ. Spatial transcriptomics of dorsal root ganglia identifies molecular signatures of human nociceptors. Sci Transl Med. 2022;14:e8186.
Article
Google Scholar
Ou Z, Lin S, Qiu J, Ding W, Ren P, Chen D, Wang J, Tong Y, Wu D, Chen A, Deng Y, Cheng M, Peng T, Lu H, Yang H, Wang J, Jin X, Ma D, Xu X, Wang Y, Li J, Wu P. Single-nucleus RNA sequencing and spatial transcriptomics reveal the immunological microenvironment of cervical squamous cell carcinoma. Adv Sci. 2022;9:e2203040.
Article
Google Scholar
Guo W, Zhou B, Yang Z, Liu X, Huai Q, Guo L, Xue X, Tan F, Li Y, Xue Q, Gao S, He J. Integrating microarray-based spatial transcriptomics and single-cell RNA-sequencing reveals tissue architecture in esophageal squamous cell carcinoma. EBioMedicine. 2022;84: 104281.
Article
Google Scholar
Peng Z, Ye M, Ding H, Feng Z, Hu K. Spatial transcriptomics atlas reveals the crosstalk between cancer-associated fibroblasts and tumor microenvironment components in colorectal cancer. J Transl Med. 2022;20:302.
Article
Google Scholar
Hao Y, Hao S, Andersen-Nissen E, Mauck WM 3rd, Zheng S, Butler A, Lee MJ, Wilk AJ, Darby C, Zager M, Hoffman P, Stoeckius M, Papalexi E, Mimitou EP, Jain J, Srivastava A, Stuart T, Fleming LM, Yeung B, Rogers AJ, McElrath JM, Blish CA, Gottardo R, Smibert P, Satija R. Integrated analysis of multimodal single-cell data. Cell. 2021;184:3573-3587.e29.
Article
Google Scholar
Andreatta M, Carmona SJ. UCell: robust and scalable single-cell gene signature scoring. Comput Struct Biotechnol J. 2021;19:3796–8.
Article
Google Scholar
Zeng D, Ye Z, Shen R, Yu G, Wu J, Xiong Y, Zhou R, Qiu W, Huang N, Sun L, Li X, Bin J, Liao Y, Shi M, Liao W. IOBR: multi-omics immuno-oncology biological research to decode tumor microenvironment and signatures. Front Immunol. 2021;12: 687975.
Article
Google Scholar
Mayakonda A, Lin DC, Assenov Y, Plass C, Koeffler HP. Maftools: efficient and comprehensive analysis of somatic variants in cancer. Genome Res. 2018;28:1747–56.
Article
Google Scholar
Gu Z, Eils R, Schlesner M. Complex heatmaps reveal patterns and correlations in multidimensional genomic data. Bioinformatics (Oxford, England). 2016;32:2847–9.
Google Scholar
Yu G, Wang LG, Han Y, He QY. clusterProfiler: an R package for comparing biological themes among gene clusters. OMICS. 2012;16:284–7.
Article
Google Scholar
Engebretsen S, Bohlin J. Statistical predictions with glmnet. Clin Epigenet. 2019;11:123.
Article
Google Scholar
Sedgwick P. Spearman’s rank correlation coefficient. BMJ (Clin Res Ed). 2014;349: g7327.
Article
Google Scholar
Amdisen A. Pearson’s correlation coefficient, p-value, and lithium therapy. Biol Psychiat. 1987;22:926–8.
Article
Google Scholar
Cohen J. A power primer. Psychol Bull. 1992;112:155–9.
Article
Google Scholar
Ranstam J, Cook JA. Kaplan–Meier curve. Br J Surg. 2017;104:442.
Article
Google Scholar
Kang J, Choi YJ, Kim IK, Lee HS, Kim H, Baik SH, Kim NK, Lee KY. LASSO-based machine learning algorithm for prediction of lymph node metastasis in T1 colorectal cancer. Cancer Res Treat. 2021;53:773–83.
Article
Google Scholar
AlMusawi S, Ahmed M, Nateri AS. Understanding cell–cell communication and signaling in the colorectal cancer microenvironment. Clin Transl Med. 2021;11: e308.
Article
Google Scholar
Kasprzak A. The role of tumor microenvironment cells in colorectal cancer (CRC) cachexia. Int J Mol Sci. 2021;22.
Boreel DF, Span PN, Heskamp S, Adema GJ, Bussink J. Targeting oxidative phosphorylation to increase the efficacy of radio- and immune-combination therapy. Clin Cancer Res. 2021;27:2970–8.
Article
Google Scholar
Soldevilla B, Carretero-Puche C, Gomez-Lopez G, Al-Shahrour F, Riesco MC, Gil-Calderon B, Alvarez-Vallina L, Espinosa-Olarte P, Gomez-Esteves G, Rubio-Cuesta B, Sarmentero J, La Salvia A, Garcia-Carbonero R. The correlation between immune subtypes and consensus molecular subtypes in colorectal cancer identifies novel tumour microenvironment profiles, with prognostic and therapeutic implications. Eur J Cancer. 2019;123:118–29.
Article
Google Scholar
Zhou Y, Bian S, Zhou X, Cui Y, Wang W, Wen L, Guo L, Fu W, Tang F. Single-cell multiomics sequencing reveals prevalent genomic alterations in tumor stromal cells of human colorectal cancer. Cancer Cell. 2020;38:818-828.e5.
Article
Google Scholar
Qi J, Sun H, Zhang Y, Wang Z, Xun Z, Li Z, Ding X, Bao R, Hong L, Jia W, Fang F, Liu H, Chen L, Zhong J, Zou D, Liu L, Han L, Ginhoux F, Liu Y, Ye Y, Su B. Single-cell and spatial analysis reveal interaction of FAP(+) fibroblasts and SPP1(+) macrophages in colorectal cancer. Nat Commun. 2022;13:1742.
Article
Google Scholar
Lanfranco MF, Ng CA, Rebeck GW. ApoE lipidation as a therapeutic target in Alzheimer’s disease. Int J Mol Sci. 2020;21:6336.
Article
Google Scholar
Rebeck GW. The role of APOE on lipid homeostasis and inflammation in normal brains. J Lipid Res. 2017;58:1493–9.
Article
Google Scholar
Karin N, Razon H. Chemokines beyond chemo-attraction: CXCL10 and its significant role in cancer and autoimmunity. Cytokine. 2018;109:24–8.
Article
Google Scholar
Wang Z, Ao X, Shen Z, Ao L, Wu X, Pu C, Guo W, Xing W, He M, Yuan H, Yu J, Li L, Xu X. TNF-α augments CXCL10/CXCR3 axis activity to induce epithelial–mesenchymal transition in colon cancer cell. Int J Biol Sci. 2021;17:2683–702.
Article
Google Scholar
Ren H, Chen Z, Yang L, Xiong W, Yang H, Xu K, Zhai E, Ding L, He Y, Song X. Apolipoprotein C1 (APOC1) promotes tumor progression via MAPK signaling pathways in colorectal cancer. Cancer Manage Res. 2019;11:4917–30.
Article
Google Scholar
Guo Q, Liu XL, Jiang N, Zhang WJ, Guo SW, Yang H, Ji YM, Zhou J, Guo JL, Zhang J, Liu HS. Decreased APOC1 expression inhibited cancer progression and was associated with better prognosis and immune microenvironment in esophageal cancer. Am J Cancer Res. 2022;12:4904–29.
Google Scholar
Hao X, Zheng Z, Liu H, Zhang Y, Kang J, Kong X, Rong D, Sun G, Sun G, Liu L, Yu H, Tang W, Wang X. Inhibition of APOC1 promotes the transformation of M2 into M1 macrophages via the ferroptosis pathway and enhances anti-PD1 immunotherapy in hepatocellular carcinoma based on single-cell RNA sequencing. Redox Biol. 2022;56: 102463.
Article
Google Scholar
Ren L, Yi J, Yang Y, Li W, Zheng X, Liu J, Li S, Yang H, Zhang Y, Ge B, Zhang S, Fu W, Dong D, Du G, Wang X, Wang J. Systematic pan-cancer analysis identifies APOC1 as an immunological biomarker which regulates macrophage polarization and promotes tumor metastasis. Pharmacol Res. 2022;183: 106376.
Article
Google Scholar
Chen QL, Xia L, Zhong SP, Wang Q, Ding J, Wang X. Bioinformatic analysis identifies key transcriptome signatures in temporal lobe epilepsy. CNS Neurosci Ther. 2020;26:1266–77.
Article
Google Scholar
Mei F, Meng K, Gu Z, Yun Y, Zhang W, Zhang C, Zhong Q, Pan F, Shen X, Xia G, Chen H. Arecanut (Areca catechu L.) seed polyphenol-ameliorated osteoporosis by altering gut microbiome via LYZ and the immune system in estrogen-deficient rats. J Agric Food Chem. 2021;69:246–58.
Article
Google Scholar
Chen H, Yao J, Bao R, Dong Y, Zhang T, Du Y, Wang G, Ni D, Xun Z, Niu X, Ye Y, Li HB. Cross-talk of four types of RNA modification writers defines tumor microenvironment and pharmacogenomic landscape in colorectal cancer. Mol Cancer. 2021;20:29.
Article
Google Scholar
Download references
Acknowledgements
Not applicable.
Funding
The National Key R&D Program of China (2018YFC1312100).
Author information
Authors and Affiliations
Department of Pancreatic and Gastric Surgical Oncology, National Cancer Center/ National Clinical Research for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, 100021, China
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei & Dongbing Zhao
Authors
Zheng Li
View author publications
You can also search for this author in
PubMed
Google Scholar
Xiaojie Zhang
View author publications
You can also search for this author in
PubMed
Google Scholar
Chongyuan Sun
View author publications
You can also search for this author in
PubMed
Google Scholar
Zefeng Li
View author publications
You can also search for this author in
PubMed
Google Scholar
He Fei
View author publications
You can also search for this author in
PubMed
Google Scholar
Dongbing Zhao
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
(1) Guarantor of study integrity: Dongbing Zhao. (2) Study concept and design: Zheng Li, Xiaojie Zhang, and Dongbing Zhao. (3) Provision of study materials or patients: Zheng Li, Xiaojie Zhang, Chongyuan Sun, He Fei, Zefeng Li, Dongbing Zhao. (4) Data collection and assembly: Zheng Li, Xiaojie Zhang, Chongyuan Sun, He Fei, and Zefeng Li. (5) Statistical analysis: Zheng and Li. (6) Manuscript preparation: All authors. (7) Manuscript editing: All authors.
Corresponding author
Correspondence to
Dongbing Zhao
.
Ethics declarations
Ethics approval and consent to participate
Since this was a retrospective, observational cohort study based on open-access 10 × Genomics Datasets, the requirement for ethics approval and informed consent were waived by the National Cancer Center in China.
Consent for publication
Not applicable.
Competing interests
The authors have declared that no competing interest exists.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Supplementary Information
Supplementary Material 1.
Supplementary Material 2.
Supplementary Material 3.
Supplementary Material 4.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by-nc-nd/4.0/
.
Reprints and permissions
About this article
Cite this article
Li, Z., Zhang, X., Sun, C.
et al.
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer.
J Big Data
11
, 132 (2024). https://doi.org/10.1186/s40537-024-00992-9
Download citation
Received
:
05 May 2023
Accepted
:
26 August 2024
Published
:
17 September 2024
DOI
:
https://doi.org/10.1186/s40537-024-00992-9
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Colorectal cancer
Spatial transcriptomics
RNA-seq
Tumor microenvironment
Fibroblast
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00965-y):
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
12 September 2024
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
Asefeh Asemi
1
,
Adeleh Asemi
2
&
Andrea Ko
1
Journal of Big Data
volume
11
, Article number:
128
(
2024
)
Cite this article
41
Accesses
Metrics
details
Abstract
This article presents an investment recommender system based on an Adaptive Neuro-Fuzzy Inference System (ANFIS) and pre-trained weights from a Multimodal Neural Network (MNN). The model is designed to support the investment process for the customers and takes into consideration seven factors to implement the proposed investment system model through the customer or potential investor data set. The system takes input from a web-based questionnaire that collects data on investors' preferences and investment goals. The data is then preprocessed and clustered using ETL tools, JMP, MATLAB, and Python. The ANFIS-based recommender system is designed with three inputs and one output and trained using a hybrid approach over three epochs with 188 data pairs and 18 fuzzy rules. The system's performance is evaluated using metrics such as RMSE, accuracy, precision, recall, and F1-score. The system is also designed to incorporate expert feedback and opinions from investors to customize and improve investment recommendations. The article concludes that the proposed ANFIS-based investment recommender system is effective and accurate in generating investment recommendations that meet investors' preferences and goals.
Graphical abstract
Introduction
The investment recommender systems (IRSs) have become increasingly important as individual investors face difficulties in making informed investment decisions in today's complex financial markets. This paper proposes the development of a hybrid recommendation system that integrates fuzzy logic and neural networks to provide personalized investment advice based on an individual investor's preferences, risk tolerance, and financial goals. Specifically, the proposed system uses the Adaptive Neuro-Fuzzy Inference System (ANFIS) and multimodal neural network pretraining to improve its accuracy and effectiveness [
1
,
2
]. The research aims to investigate the potential benefits of this approach, answering several research questions related to the system's accuracy and effectiveness, optimal pretraining objectives, data preparation, and training and validation procedures. Overall, the proposed IRS has the potential to provide valuable support to individual investors in making informed investment decisions, ultimately helping them achieve their financial goals.
Literature review
Recommender systems are widely used in investment decision-making to help individual investors choose suitable financial products based on their risk tolerance, financial goals, and investment experience [
3
]. However, traditional recommender systems have limitations, such as the reliance on a limited set of user attributes and the inability to consider the dynamic nature of financial markets or user feedback. To overcome these limitations, recent research has explored the use of multimodal neural network pretraining techniques, such as ANFIS [
4
], that can model complex relationships between inputs and outputs and adapt to changing conditions. A variety of studies have investigated the use of machine learning and artificial intelligence methods, such as genetic algorithms, data clustering, and sentiment analysis, for stock prediction and investment efficiency. For example, Abraham et al. [
5
] explored the use of GA and random forest to predict stock trends, while Aggarwal et al. [
6
] examined data clustering algorithms and their applications in stock prediction. Huang et al. [
6
] investigated neural network models for stock selection based on fundamental analysis, and Faridniya and Faridnia [
7
] provided a model for allocating resources and choosing investment types using Data Envelopment Analysis. Researchers have also explored the impact of factors such as economic policy uncertainty, corporate governance, creative accounting, and customer experience on investment decision-making. Benkraiem et al. [
8
] investigated the impact of economic policy uncertainty, investor protection, and excess cash on stock value in a cross-country comparison, while Aksar et al. [
9
] examined the relationship between cash holding and investment efficiency for financially distressed firms, and the moderating effect of corporate governance. AL-Khafaji et al. [
10
] studied the role of creative accounting in increasing the marketing of shares and profits in the Iraqi stock exchange, and Andajani [
11
] examined customer experience management in retailing. Furthermore, some studies propose novel combined business recommender system models that incorporate customer investment service feedback to provide personalized investment recommendations. Asemi and Ko [
4
] proposed a novel combined business recommender system model using customer investment service feedback, and Chen et al. [
12
] studied user perception of sentiment-integrated critiquing in recommender systems. Chen et al. [
13
] proposed a cluster-based mutual fund classification and price prediction system using machine learning for Robo-advisors, while Chatterjee et al. [
14
] proposed an NLP and LSTM-based stock prediction and recommender system for KOSDAQ and KOSPI. Finally, various studies have applied ANFIS to evaluate dysarthric automatic speech recognition systems [
15
] or to estimate the return rate of blockchain financial products [
16
]. D'lima and Khan [
17
] used ANN and ANFIS to predict FOREX rates, while Davies et al. [
18
] implemented a type-2 fuzzy logic-based prediction system for the Nigerian stock exchange. Ezhilarasi and Sashi Rekha [
19
] proposed a secure recommendation application for environment crops using big data analytics with a fuzzy framework. Asemi et al. [
20
] propose a model for an investment recommender system using ANFIS based on the potential investors' decision key factors. They analyze big data to identify key factors influencing investment decisions and utilize ANFIS to make personalized investment recommendations. In another study, Asemi et al. [
21
] investigate the impact of managerial traits on investor decision prediction using ANFIS, revealing valuable insights into the role of managers in influencing investment outcomes. Additionally, Asemi et al. [
22
] present an adaptive neuro-fuzzy inference system for customizing investment types based on potential investors' demographics and feedback. Their research highlights the importance of incorporating demographic information and feedback into investment recommendations. Finally, Asemi et al. [
23
] conduct a systematic review and propose an ANFIS-based investment-type recommender system that considers investors' demographics. The authors present their findings at the 8th International Congress on Information and Communication Technology, emphasizing the potential of ANFIS-based recommender systems in providing personalized investment advice. These studies collectively contribute to the understanding of ANFIS-based investment recommender systems and their application in the financial domain. In summary, these studies provide a comprehensive examination of various aspects of stock prediction and investment efficiency, utilizing a range of methods and techniques including machine learning, artificial intelligence, and data analysis. The use of multimodal neural network pretraining techniques, such as ANFIS, has helped to overcome the limitations of traditional recommender systems and allowed for the modeling of complex relationships between inputs and outputs while adapting to changing conditions.
Methods
This study proposes a novel approach to developing an ANFIS-based IRS using Multimodal Neural Network Pretraining. ANFIS is a hybrid artificial neural network that combines fuzzy logic and neural networks to perform data analysis and decision-making. Multimodal Neural Network Pretraining is a technique used in deep learning to improve the overall performance of the neural network by allowing it to learn from multiple sources of information simultaneously. The proposed approach jointly pre-trains all modalities using a predictive objective to improve the accuracy and effectiveness of investment recommendations. The implementation of this approach was carried out using MATLAB, Python, Anaconda, and Jupyter, and all codes and data used in this work are presented in this article. Predictive pretraining can help improve the performance of ANFIS models by initializing the weights with a useful representation of the input data, leading to faster learning, better generalization performance, and more accurate investment recommendations (Table
1
).
Table 1 Description of research methodology
Full size table
Experimental results
The experimental results demonstrate the effectiveness of the proposed ANFIS-based IRS in predicting investment types based on a combination of demographic, decision key factors, personality traits, experiences, and financial and managerial traits. The system outperformed traditional methods such as decision trees and logistic regression, highlighting the superiority of ANFIS-based approaches for investment prediction. The results included the following sections.
Preprocessing and clustering data
To develop an ANFIS-based IRS, the dataset used in this study was preprocessed and clustered. The dataset consisted of eight columns, six of which contained clustered data related to types of investors based on demographic characteristics, financial status, management characteristics, and more. Duplicate and infrequent rows were eliminated, resulting in 188 potential investor groups. Three columns related to investment data were clustered using Python and k-means, including financial information, investment experiences, and other features such as personality and management characteristics. These three columns were combined into three inputs for ANFIS, with the output consisting of the combination of clustered data related to investment type preference and current investment type. The final dataset contained 188 data rows in four columns, and ANFIS was built using this dataset after preprocessing and clustering (Table
2
).
Table 2 Description of data preprocessing
Full size table
ANFIS design model
The ANFIS-based IRS is a powerful tool for providing personalized investment recommendations to potential investors.
Figure
1
in MATLAB shows the data imported for the ANFIS, with 3 columns for potential investor clusters and the final column for investing product clusters. The ANFIS model was designed using a Sugeno-type fuzzy function with MFs displayed in the graph. A total of 188 train data pairs were used, with max aggregation and min implication. The MFs are trimf and the output MF type is constant. Aggregation combines fuzzy sets representing rule outputs and occurs once before the final defuzzification stage for each output variable.
Fig. 1
Data and fuzzy function for ANFIS model
Full size image
ANFIS training and testing
Figure
2
displays the trained grid of the ANFIS system, which has three inputs and one output for investment type. The system was trained using a hybrid approach over three epochs, and the error for each epoch is ~ 0.72. The ANFIS info section provides information about the training process of the Combined ANFIS system, including the number of nodes, parameters, and fuzzy rules. The system has been successfully trained using 188 data pairs, with a minimal training root mean squared error of 0.721054. The model achieved an F1-score of 0.6667 and a minimal training RMSE of 0.721054. An F1-score of 0.6667 indicates that the model's performance is reasonably good, as it considers both precision and recall. A perfect F1 score is 1, while an F1 score of 0 indicates that the model's predictions are completely wrong. Therefore, an F1-score of 0.6667 suggests that the model's precision and recall are both reasonably high, although there is room for improvement. Overall, this F1-score indicates that the model can make accurate predictions, but there may be some misclassifications. The trained ANFIS system, which generated a total of 18 rules that are the decision-making mechanisms for investment recommendations. As the following:
Fig. 2
Trained and tested grid of the ANFIS system for investment type prediction with hybrid approach
Full size image
Figure
3
depicts the structure of the ANFIS Model, including fuzzification, implication rules, normalization, defuzzification, and integration, resulting in an investment recommendation for the investor. Overall, the ANFIS-based IRS provides a powerful and customizable tool for personalized investment recommendations.
Fig. 3
Proposed ANFIS structure
Full size image
Multimodal neural network pretraining
Result Test MSE is 0.0011995050086818341. A low test MSE indicates that your model is performing well on the test data, which is a good sign. However, it's important to keep in mind that a low test MSE doesn't necessarily mean that our model is perfect. Thus, the other metrics considered such as accuracy or precision to solve the problem. Now that we have a pre-trained neural network model, we can use it for making predictions on new data. To do this, we can use the prediction method of the Keras model object, which takes an input array of the same shape as the training data and returns the predicted output values. Here, new_data is a numpy array with two new input samples, which we normalize using the same scaler object that was used to normalize the training data. We then reshape the new data to have the same shape as the training data and use the prediction method of the model to obtain the predicted output values. Finally, we print the predictions to the console.
Initializing neural network weights
Model evaluation
Prediction on the new data
Discussion
The investment industry is one of the most important sectors in the global economy, with trillions of dollars in assets under management. Investors face many challenges, including market volatility, changing economic conditions, and increasing amounts of data to analyze. IRSs are becoming increasingly popular to help investors make more informed decisions about where to allocate their funds. Previous studies have utilized ANFIS for investment prediction, such as predicting stock market and real estate investment trust prices. However, these studies did not focus on predicting investment type based on a combination of inputs, as this study does. Other studies proposed ANFIS-based models for stock price prediction or investment type prediction using demographic characteristics and investment behavior. Hybrid systems combining ANFIS with particle swarm optimization or GA have also been proposed for investment type prediction with better performance than traditional methods. However, none of these studies specifically focus on predicting investment type based on a combination of inputs including demographic, decision key factors, personality traits, experiences, and financial and managerial traits as this study does [
4
,
5
,
16
,
20
,
21
,
24
,
25
,
26
,
27
]. In this research, we presented an IRS based on an ANFIS. ANFIS is a type of artificial neural network that combines fuzzy logic and neural networks to create a powerful prediction engine. In this section, we analyze and discuss the results of implementing the proposed investment recommender system framework, focusing on the effectiveness and accuracy of the model across various phases of development (Fig.
4
). Our system takes as input a set of user preferences and investment goals and provides a list of recommended investment products based on these inputs as the following:
Phase 1: Data Collection • Inputs from the Web-based Questionnaire:
○
Data Categories: □ Demographics: Age, Gender, Education, Income Level, etc. □ Financial Information: Income, Assets, Investment Capital, etc. □ Investment Experience: Past investments, success rates, risk tolerance, etc. □ Personality & Managerial Traits: Decision-making style, leadership qualities, etc. □ Investment Preferences: Preferred types of investments, expected returns, investment horizon, etc.
Phase 2: Data Preprocessing & Clustering • Data Preprocessing:
○
Tools Used: ETL Tools, Python, JMP, MATLAB.
○
Objective: Clean and structure the raw data to prepare it for clustering and model training. • Clustering Process:
○
K-Means Clustering (Elbow Curve & Silhouette score): □ Clustered Columns: □ Financial Information: Clusters investors based on their financial profiles. □ Investment Experiences: Clusters investors based on their previous investment experiences and outcomes. □ Personality & Managerial Traits: Clusters investors based on their personal characteristics and management styles. □ Clustering Approach: Use Python and K-Means to identify optimal clusters for each column.
○
Combined Inputs for ANFIS: □ The clustered data from the three columns (Financial Information, Investment Experiences, Personality & Managerial Traits) are combined into three inputs for the ANFIS model.
Phase 3: ANFIS-Based Recommender System • ANFIS Model:
○
Design: □ Inputs: □ Three inputs representing the clustered data: Financial Information, Investment Experiences, Personality & Managerial Traits. □ Output: □ A combination of clustered data related to Investment Type Preference and Current Investment Type.
○
Training: □ Dataset: The final dataset contains 188 rows and four columns after preprocessing and clustering. □ Hybrid Training Approach: □ The ANFIS model is trained using a hybrid approach over three epochs. □ Fuzzy Rules: Incorporates 18 fuzzy rules to drive decision-making and recommendations.
○
Objective: To provide personalized investment recommendations based on the clustered inputs.
Phase 4: Multimodal Neural Network Pretraining • Pretraining the Neural Network:
○
Purpose: Enhance the ANFIS model's accuracy by pretraining the neural network components.
○
Approach: Fine-tune the neural network layers, ensuring optimal performance in recommending investment types.
Phase 5: Model Training and Testing • Training & Performance Evaluation:
○
Training: Continuous refinement of the ANFIS model using the dataset to enhance predictive capabilities.
○
Testing Metrics: □ Root Mean Square Error (RMSE): Measures the prediction error. □ Precision: Assesses the accuracy of the investment recommendations. □ Recall: Evaluates the model's ability to identify relevant investment options. □ F1-Score: Balances precision and recall for overall model assessment.
Phase 6: Expert Feedback Loop • Continuous Improvement:
○
Expert Input: Financial experts provide ongoing feedback to refine fuzzy rules and adjust model parameters.
○
Error Correction: Incorporate expert insights to improve the accuracy and relevance of recommendations.
Phase 7: Final Output & Decision-Making • Final Output:
○
Personalized Investment Recommendations: □ Tailored investment strategies generated based on the ANFIS model's output, reflecting the investor's unique profile. □ The output is influenced by the combined data clusters, ensuring that recommendations are well-aligned with the investor's preferences and current portfolio. o Feedback from Investors: □ Application Layer: The recommendations are implemented, and feedback is gathered to improve the recommender system. □ Objective: To continuously enhance the system's performance and investor satisfaction (Additional file
1
).
Our results show that our ANFIS-based IRS performs well in recommending investment products based on user preferences and investment goals. Our system provides accurate and personalized investment recommendations to investors, allowing them to make more informed decisions about where to allocate their funds. Our system can be used by both novice and experienced investors, making it an effective tool for anyone looking to optimize their investment portfolio. One limitation of our system is that it requires a significant amount of data to train the ANFIS model. Collecting this data can be time-consuming and costly, particularly for smaller investment firms or individual investors. Additionally, our system is designed for retail investors, and may not be suitable for institutional investors or investors with very complex investment portfolios. Overall, our ANFIS-based IRS is an effective tool for investors looking to optimize their investment portfolios. By combining fuzzy logic and neural networks, our system provides personalized investment recommendations based on user preferences and investment goals. Our system is easy to use and can be customized based on expert opinions and feedback from investors. With further development, our system has the potential to revolutionize the investment industry and provide investors with more accurate and effective investment recommendations.
Fig. 4
Comprehensive Framework for the Proposed Investment Recommender System
Full size image
Conclusion
In conclusion, the ANFIS-based IRS has demonstrated promising results in recommending suitable investment types to investors. By using data collected through a web questionnaire, preprocessing it with ETL tools, and training the ANFIS model with a hybrid approach over three epochs, the system achieved a low RMSE and high accuracy in predicting suitable investments. Furthermore, the system's performance was enhanced through multimodal neural network pretraining and expert feedback. The system's results have several practical implications for the financial industry, as it can assist investors in making informed investment decisions based on their preferences and risk tolerance. The system's ability to incorporate expert feedback and customize rules and recommendations based on investor feedback can lead to increased satisfaction and trust in the investment recommendations. However, there are several avenues for future research that can further improve the ANFIS-based IRS. One potential area of research is the integration of alternative data sources, such as social media sentiment analysis or news sentiment analysis, to enhance the system's accuracy and predictive power. Additionally, incorporating more sophisticated machine learning algorithms, such as deep learning, can improve the system's ability to capture complex patterns and relationships in the data. Moreover, future research can investigate the system's scalability and applicability in different investment contexts, such as international investments or real estate investments. Finally, the system's ethical implications and potential biases should be thoroughly examined, as it relies on historical data to make future predictions, which can perpetuate existing biases and inequalities. In summary, the ANFIS-based IRS has the potential to revolutionize the investment decision-making process by providing customized and accurate recommendations to investors. Future research can further enhance the system's performance and applicability, paving the way for more efficient and effective investment decisions while addressing ethical concerns and potential biases.
Availability of data and materials
The original data used in this research was collected through a collaborative effort involving the Corvinus University of Budapest, the Dorsum company, and the Portfolio in the 1.3.1-VKE-2018-00007 project, conducted in the Hungarian language. The consortium agreement and project leader's consent allow for the use of project data in additional research and publications by the authors. The authors have translated, cleaned, and prepared the data specifically for this study. New data can be accessed at [
28
] in title of "Data for Adaptive Neuro-Fuzzy Inference System for Customizing Investment Type based on the Potential Investors’ Demographics", available at Mendeley Data.
References
Hervella ÁS, Rouco J, Novo J, Ortega M. Self-supervised multimodal reconstruction pre-training for retinal computer-aided diagnosis. Expert Syst Appl. 2021;185: 115598.
https://doi.org/10.1016/j.eswa.2021.115598
.
Article
Google Scholar
Jang JSR. ANFIS: Adaptive-network-based fuzzy inference system. IEEE Trans Syst Man Cybern. 1993;23(3):665–85.
https://doi.org/10.1109/21.256541
.
Article
Google Scholar
Chen J. Investment product. Reviewed by Godon Scott, Investopedia.Com.
https://www.investopedia.com/terms/i/investment-product.asp
. 2020.
Asemi A, Ko A. A novel combined business recommender system model using customer investment service feedback. In: 34th Bled EConference Digital Support from Crisis to Progressive Change: Conference Proceedings, 2021; pp. 223–237.
https://doi.org/10.18690/978-961-286-485-9.17
.
Abraham R, Samad ME, Bakhach AM, El-Chaarani H, Sardouk A, Nemar SE, Jaber D. Forecasting a stock trend using genetic algorithm and random forest. J Risk Financ Manage. 2022;15(5):5.
https://doi.org/10.3390/jrfm15050188
.
Article
Google Scholar
Aggarwal A, Hess O, Lockman JL, Smith L, Stevens M, Bruce J, Caruso T. Anesthesiologists with advanced degrees in education: qualitative study of a changing paradigm. JMIR Med Educ. 2022;8(2):e38050.
https://doi.org/10.2196/38050
.
Article
Google Scholar
Faridniya A, Faridnia M. Providing a model for allocating resources and choosing investment type using data envelopment analysis (DEA) (case study: social security organization). J Adv Pharm Educ Res. 2019;9(S2):112–24.
Google Scholar
Benkraiem R, Gaaya S, Lakhal F, Lakhal N. Economic policy uncertainty, investor protection, and the value of excess cash: a cross-country comparison. Financ Res Lett. 2023;52: 103572.
https://doi.org/10.1016/j.frl.2022.103572
.
Article
Google Scholar
Aksar M, Hassan S, Kayani MB, Khan S, Ahmed T. Cash holding and investment efficiency nexus for financially distressed firms: the moderating role of corporate governance. Manage Sci Lett. 2022;12(1):67–74.
https://doi.org/10.5267/j.msl.2021.7.001
.
Article
Google Scholar
AL-Khafaji AAK, Mustangs RF, Alsaalim FHAJ. The role of creative accounting in increasing the marketing of shares and their profits in the Iraqi stock exchange. Period Eng Nat Sci. 2022;10(2):323–35.
https://doi.org/10.21533/pen.v10i2.2886
.
Article
Google Scholar
Andajani E. Understanding customer experience management in retailing. Proc Soc Behav Sci. 2015;211:629–33.
https://doi.org/10.1016/j.sbspro.2015.11.082
.
Article
Google Scholar
Chen L, Yan D, Wang F. User perception of sentiment-integrated critiquing in recommender systems. Int J Hum Comput Stud. 2019;121:4–20.
https://doi.org/10.1016/j.ijhcs.2017.09.005
.
Article
Google Scholar
Chen X, Ye S, Huang C. Cluster-based mutual fund classification and price prediction using machine learning for robo-advisors. Comput Intell Neurosci. 2021;2021: e4984265.
https://doi.org/10.1155/2021/4984265
.
Article
Google Scholar
Chatterjee I, Gwan J, Kim YJ, Lee MS, Cho M. An NLP and LSTM based stock prediction and recommender system for KOSDAQ and KOSPI. In: Singh M, Kang DK, Lee JH, Tiwary US, Singh D, Chung WY, editors. Intelligent human computer interaction, Pt I, vol. 12615. Cham: Springer International Publishing; 2021. p. 403–13.
https://doi.org/10.1007/978-3-030-68449-5_40
.
Chapter
Google Scholar
Asemi A, Salim SSB, Shahamiri SR, Asemi A, Houshangi N. Adaptive neuro-fuzzy inference system for evaluating dysarthric automatic speech recognition (ASR) systems. Soft Comput. 2019;23:3529–44.
https://doi.org/10.1007/s00500-018-3013-4
.
Article
Google Scholar
Birim ŞÖ, Sönmez FE, Liman YS. Estimating return rate of blockchain financial product by ANFIS-PSO method. In: Lecture notes in networks and systems, 504 LNNS, pp. 802–809. Scopus. 2022.
https://doi.org/10.1007/978-3-031-09173-5_92
.
D’lima N, Khan S. FOREX rate prediction using ANN and ANFIS Conference.
https://www.semanticscholar.org/paper/FOREX-rate-prediction-using-ANN-and-ANFIS-D%27lima-Khan/6817d1cc9f7ac35cf28404f0e17e358b54fa16d1
. 2016.
Davies IN, Ene D, Cookey IB, Lenu GF. Implementation of a type-2 fuzzy logic based prediction system for the Nigerian stock exchange. 2022.
Ezhilarasi TP, Sashi Rekha K. Secure recommendation application for environment crop using big data analytics with fuzzy framework. J Green Eng. 2020;10(4):1799–815.
Google Scholar
Asemi A, Asemi A, Ko A. Investment recommender system model based on the potential investors’ key decision factors. Big Data. 2023.
https://doi.org/10.1089/big.2022.0302
.
Article
Google Scholar
Asemi A, Asemi A, Ko A. A systematic review and propose an ANFIS-based investment type recommender system using investors’ demographic. In: A Hybrid Conference 8th International Congress on Information and Communication Technology ICICT 2023, London, UK, 20–23.
https://www.researchgate.net/publication/369019468_Systematic_Review_and_Propose_an_ANFIS-Based_Investment_Type_Recommender_System_using_Investors'_Demographic
. 2023b.
Asemi A, Asemi A, Ko A. Adaptive neuro-fuzzy inference system for customizing investment type based on the potential investors’ demographics and feedback. J Big Data. 2023;10(1):87.
https://doi.org/10.1186/s40537-023-00784-7
.
Article
Google Scholar
Asemi A, Asemi A, Ko A. Unveiling the impact of managerial traits on investor decision prediction: ANFIS approach. Soft Comput. 2023.
https://doi.org/10.1007/s00500-023-08102-2
.
Article
Google Scholar
Asemi A, Asemi A. Intelligent MCDM method for supplier selection under fuzzy environment. Int J Inf Sci Manage (IJISM).
https://ijism.ricest.ac.ir/index.php/ijism/article/view/346
. 2014.
Huang Y, Capretz LF, Ho D. Neural network models for stock selection based on fundamental analysis. IEEE Can Conf Electr Comput Eng (CCECE). 2019;2019:1–4.
https://doi.org/10.1109/CCECE.2019.8861550
.
Article
Google Scholar
Kovács T, Ko A, Asemi A. Exploration of the investment patterns of potential retail banking customers using two-stage cluster analysis. J Big Data, 2021; 8(1).
https://doi.org/10.1186/s40537-021-00529-4
.
Wang Y, Zhang M. Simulation analysis of regional real estate investment risk based on system dynamics. E3S Web Conf. 2021;251:01070.
https://doi.org/10.1051/e3sconf/202125101070
.
Article
Google Scholar
Asemi A. Data for adaptive neuro-fuzzy inference system for customizing investment type based on the potential investors’ demographics. Available at Mendeley Data, V1, 2022.
https://doi.org/10.17632/93dmwj5yhk.1
.
Download references
Acknowledgements
Not applicable.
Funding
No funding was received for this study.
Author information
Authors and Affiliations
Corvinus University of Budapest, Budapest, 1093, Hungary
Asefeh Asemi & Andrea Ko
Universiti Malaya, 50603, Kuala Lumpur, Malaysia
Adeleh Asemi
Authors
Asefeh Asemi
View author publications
You can also search for this author in
PubMed
Google Scholar
Adeleh Asemi
View author publications
You can also search for this author in
PubMed
Google Scholar
Andrea Ko
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
Asefeh Asemi played the role of the main researcher, Adeleh Asemi provided guidance as the research advisor, and Andrea Ko served as the research supervisor.
Corresponding author
Correspondence to
Asefeh Asemi
.
Ethics declarations
Ethics approval and consent to participate
This article does not involve any studies that were conducted on human or animal participants by any of the authors. Not applicable as there were no participants involved in the study.
Competing interests
The authors disclose that their work at the Corvinus University of Budapest involved collaboration with commercial companies (Dorsum and Portfolio) in the design and development of the survey used in this research.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Supplementary Information
Additional file 1: Comprehensive Framework for the Investment Recommender System.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by-nc-nd/4.0/
.
Reprints and permissions
About this article
Cite this article
Asemi, A., Asemi, A. & Ko, A. A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN.
J Big Data
11
, 128 (2024). https://doi.org/10.1186/s40537-024-00965-y
Download citation
Received
:
30 November 2022
Accepted
:
16 July 2024
Published
:
12 September 2024
DOI
:
https://doi.org/10.1186/s40537-024-00965-y
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Adaptive neuro-fuzzy inference system (ANFIS)
Investment recommender system
Multimodal neural network
Clustering
JMP
MATLAB
Python
Fuzzy rules
Investor feedback
Expert feedback
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/most-recent/rss.xml):
Most Recent Articles: Journal of Big Data
https://journalofbigdata.springeropen.com
Most Recent Articles: Journal of Big Data
Optimizing poultry audio signal classification with deep learning and burn layer fusion
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00985-8
This study introduces a novel deep learning-based approach for classifying poultry audio signals, incorporating a custom Burn Layer to enhance model robustness. The methodology integrates digital audio signal ...
Research
Wed, 18 Sep 2024 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00985-8
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
2024-09-18T00:00:00Z
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00991-w
In late 2023, the United Nations conference on climate change (COP28), which was held in Dubai, encouraged a quick move from fossil fuels to renewable energy. Solar energy is one of the most promising forms of...
Research
Wed, 18 Sep 2024 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00991-w
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
2024-09-18T00:00:00Z
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00994-7
The frequent usage of computer networks and the Internet has made computer networks vulnerable to numerous attacks, highlighting the critical need to enhance the precision of security mechanisms. One of the mo...
Research
Wed, 18 Sep 2024 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00994-7
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
2024-09-18T00:00:00Z
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00992-9
The tumor microenvironment (TME) provides a region for intricate interactions within or between immune and non-immune cells. We aimed to reveal the tissue architecture and comprehensive landscape of cells with...
Research
Tue, 17 Sep 2024 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00992-9
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
2024-09-17T00:00:00Z
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00965-y
This article presents an investment recommender system based on an Adaptive Neuro-Fuzzy Inference System (ANFIS) and pre-trained weights from a Multimodal Neural Network (MNN). The model is designed to support...
Research
Thu, 12 Sep 2024 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00965-y
Asefeh Asemi, Adeleh Asemi and Andrea Ko
2024-09-12T00:00:00Z
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles):
Articles | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Articles
Collections
Articles
Search by keyword
Search by citation
for results from
All volumes
Volume 11 (2024)
Volume 10 (2023)
Volume 9 (2022)
Volume 8 (2021)
Volume 7 (2020)
Volume 6 (2019)
Volume 5 (2018)
Volume 4 (2017)
Volume 3 (2016)
Volume 2 (2015)
Volume 1 (2014)
Search
Show results from
Select a volume
Volume 11 (2024)
Volume 10 (2023)
Volume 9 (2022)
Volume 8 (2021)
Volume 7 (2020)
Volume 6 (2019)
Volume 5 (2018)
Volume 4 (2017)
Volume 3 (2016)
Volume 2 (2015)
Volume 1 (2014)
Search
Page 1 of 20
Sort by
Relevance
Newest first
Oldest first
Submit
Optimizing poultry audio signal classification with deep learning and burn layer fusion
This study introduces a novel deep learning-based approach for classifying poultry audio signals, incorporating a custom Burn Layer to enhance model robustness. The methodology integrates digital audio signal ...
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Citation:
Journal of Big Data
2024
11
:135
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
In late 2023, the United Nations conference on climate change (COP28), which was held in Dubai, encouraged a quick move from fossil fuels to renewable energy. Solar energy is one of the most promising forms of...
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Citation:
Journal of Big Data
2024
11
:134
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
The frequent usage of computer networks and the Internet has made computer networks vulnerable to numerous attacks, highlighting the critical need to enhance the precision of security mechanisms. One of the mo...
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Citation:
Journal of Big Data
2024
11
:133
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
The tumor microenvironment (TME) provides a region for intricate interactions within or between immune and non-immune cells. We aimed to reveal the tissue architecture and comprehensive landscape of cells with...
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Citation:
Journal of Big Data
2024
11
:132
Content type:
Research
Published on:
17 September 2024
View
Full Text
View
PDF
Development and evaluation of a deep learning model for automatic segmentation of non-perfusion area in fundus fluorescein angiography
Diabetic retinopathy (DR) is the most prevalent cause of preventable vision loss worldwide, imposing a significant economic and medical burden on society today, of which early identification is the cornerstone...
Authors:
Wei Feng, Bingjie Wang, Dan Song, Mengda Li, Anming Chen, Jing Wang, Siyong Lin, Yiran Zhao, Bin Wang, Zongyuan Ge, Shuyi Xu and Yuntao Hu
Citation:
Journal of Big Data
2024
11
:131
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Evolutionary computation-based self-supervised learning for image processing: a big data-driven approach to feature extraction and fusion for multispectral object detection
The image object recognition and detection technology are widely used in many scenarios. In recent years, big data has become increasingly abundant, and big data-driven artificial intelligence models have attr...
Authors:
Xiaoyang Shen, Haibin Li, Achyut Shankar, Wattana Viriyasitavat and Vinay Chamola
Citation:
Journal of Big Data
2024
11
:130
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Leveraging large-scale genetic data to assess the causal impact of COVID-19 on multisystemic diseases
The long-term impacts of COVID-19 on human health are a major concern, yet comprehensive evaluations of its effects on various health conditions are lacking.
Authors:
Xiangyang Zhang, Zhaohui Jiang, Jiayao Ma, Yaru Qi, Yin Li, Yan Zhang, Yihan Liu, Chaochao Wei, Yihong Chen, Ping Liu, Yinghui Peng, Jun Tan, Ying Han, Shan Zeng, Changjing Cai and Hong Shen
Citation:
Journal of Big Data
2024
11
:129
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
This article presents an investment recommender system based on an Adaptive Neuro-Fuzzy Inference System (ANFIS) and pre-trained weights from a Multimodal Neural Network (MNN). The model is designed to support...
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Citation:
Journal of Big Data
2024
11
:128
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Inhibitory neuron links the causal relationship from air pollution to psychiatric disorders: a large multi-omics analysis
Psychiatric disorders are severe health challenges that exert a heavy public burden. Air pollution has been widely reported as related to psychiatric disorder risk, but their casual association and pathologica...
Authors:
Xisong Liang, Jie Wen, Chunrun Qu, Nan Zhang, Ziyu Dai, Hao Zhang, Peng Luo, Ming Meng, Zhixiong Liu, Fan Fan and Quan Cheng
Citation:
Journal of Big Data
2024
11
:127
Content type:
Research
Published on:
11 September 2024
View
Full Text
View
PDF
Enhancing oil palm segmentation model with GAN-based augmentation
In digital agriculture, accurate crop detection is fundamental to developing automated systems for efficient plantation management. For oil palm, the main challenge lies in developing robust models that perfor...
Authors:
Qi Bin Kwong, Yee Thung Kon, Wan Rusydiah W. Rusik, Mohd Nor Azizi Shabudin, Shahirah Shazana A. Rahman, Harikrishna Kulaveerasingam and David Ross Appleton
Citation:
Journal of Big Data
2024
11
:126
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
AI sees beyond humans: automated diagnosis of myopia based on peripheral refraction map using interpretable deep learning
The question of whether artificial intelligence (AI) can surpass human capabilities is crucial in the application of AI in clinical medicine. To explore this, an interpretable deep learning (DL) model was deve...
Authors:
Yong Tang, Zhenghua Lin, Linjing Zhou, Weijia Wang, Longbo Wen, Yongli Zhou, Zongyuan Ge, Zhao Chen, Weiwei Dai, Zhikuan Yang, He Tang and Weizhong Lan
Citation:
Journal of Big Data
2024
11
:125
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
Modeling the impact of BDA-AI on sustainable innovation ambidexterity and environmental performance
Data has evolved into one of the principal resources for contemporary businesses. Moreover, corporations have undergone digitalization; consequently, their supply chains generate substantial amounts of data. T...
Authors:
Chin-Tsu Chen, Asif Khan and Shih-Chih Chen
Citation:
Journal of Big Data
2024
11
:124
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
Efficient microservices offloading for cost optimization in diverse MEC cloud networks
In recent years, mobile applications have proliferated across domains such as E-banking, Augmented Reality, E-Transportation, and E-Healthcare. These applications are often built using microservices, an archit...
Authors:
Abdul Rasheed Mahesar, Xiaoping Li and Dileep Kumar Sajnani
Citation:
Journal of Big Data
2024
11
:123
Content type:
Research
Published on:
4 September 2024
View
Full Text
View
PDF
Predicting startup success using two bias-free machine learning: resolving data imbalance using generative adversarial networks
The success of newly established companies holds significant implications for community development and economic growth. However, startups often grapple with heightened vulnerability to market volatility, whic...
Authors:
Jungryeol Park, Saesol Choi and Yituo Feng
Citation:
Journal of Big Data
2024
11
:122
Content type:
Research
Published on:
3 September 2024
View
Full Text
View
PDF
CTGAN-ENN: a tabular GAN-based hybrid sampling method for imbalanced and overlapped data in customer churn prediction
Class imbalance is one of many problems of customer churn datasets. One of the common problems is class overlap, where the data have a similar instance between classes. The prediction task of customer churn be...
Authors:
I Nyoman Mahayasa Adiputra and Paweena Wanchai
Citation:
Journal of Big Data
2024
11
:121
Content type:
Research
Published on:
2 September 2024
View
Full Text
View
PDF
Cartographies of warfare in the Indian subcontinent: Contextualizing archaeological and historical analysis through big data approaches
Some of the most notable human behavioral palimpsests result from warfare and its durable traces in the form of defensive architecture and strategic infrastructure. For premodern periods, this architecture is ...
Authors:
Monica L. Smith and Connor Newton
Citation:
Journal of Big Data
2024
11
:120
Content type:
Case Study
Published on:
29 August 2024
View
Full Text
View
PDF
Automated subway touch button detection using image process
Subway button detection is paramount for passenger safety, yet the occurrence of inadvertent touches poses operational threats. Camera-based detection is indispensable for identifying touch occurrences, ascert...
Authors:
Junfeng An, Mengmeng Lu, Gang Li, Jiqiang Liu and Chongqing Wang
Citation:
Journal of Big Data
2024
11
:119
Content type:
Research
Published on:
29 August 2024
View
Full Text
View
PDF
Cybersecurity vulnerabilities and solutions in Ethiopian university websites
This study investigates the causes and countermeasures of cybercrime vulnerabilities, specifically focusing on selected 16 Ethiopian university websites. This study uses a cybersecurity awareness survey, and a...
Authors:
Ali Yimam Eshetu, Endris Abdu Mohammed and Ayodeji Olalekan Salau
Citation:
Journal of Big Data
2024
11
:118
Content type:
Research
Published on:
23 August 2024
View
Full Text
View
PDF
Crude oil price forecasting using K-means clustering and LSTM model enhanced by dense-sparse-dense strategy
Crude oil is an essential energy source that affects international trade, transportation, and manufacturing, highlighting its importance to the economy. Its future price prediction affects consumer prices and ...
Authors:
Alireza Jahandoost, Farhad Abedinzadeh Torghabeh, Seyyed Abed Hosseini and Mahboobeh Houshmand
Citation:
Journal of Big Data
2024
11
:117
Content type:
Research
Published on:
17 August 2024
View
Full Text
View
PDF
Rs-net: Residual Sharp U-Net architecture for pavement crack segmentation and severity assessment
U-net, a fully convolutional network-based image segmentation method, has demonstrated widespread adaptability in the crack segmentation task. The combination of the semantically dissimilar features of the enc...
Authors:
Luqman Ali, Hamad AlJassmi, Mohammed Swavaf, Wasif Khan and Fady Alnajjar
Citation:
Journal of Big Data
2024
11
:116
Content type:
Research
Published on:
17 August 2024
View
Full Text
View
PDF
Internet of things and ensemble learning-based mental and physical fatigue monitoring for smart construction sites
The construction industry substantially contributes to the economic growth of a country. However, it records a large number of workplace injuries and fatalities annually due to its hesitant adoption of automat...
Authors:
Bubryur Kim, K. R. Sri Preethaa, Sujeen Song, R. R. Lukacs, Jinwoo An, Zengshun Chen, Euijung An and Sungho Kim
Citation:
Journal of Big Data
2024
11
:115
Content type:
Research
Published on:
16 August 2024
View
Full Text
View
PDF
Toward a globally lunar calendar: a machine learning-driven approach for crescent moon visibility prediction
This paper presents a comprehensive approach to harmonizing lunar calendars across different global regions, addressing the long-standing challenge of variations in new crescent Moon sightings that mark the be...
Authors:
Samia Loucif, Murad Al-Rajab, Raed Abu Zitar and Mahmoud Rezk
Citation:
Journal of Big Data
2024
11
:114
Content type:
Research
Published on:
12 August 2024
View
Full Text
View
PDF
Enhancing K-nearest neighbor algorithm: a comprehensive review and performance analysis of modifications
The k-Nearest Neighbors (kNN) method, established in 1951, has since evolved into a pivotal tool in data mining, recommendation systems, and Internet of Things (IoT), among other areas. This paper presents a c...
Authors:
Rajib Kumar Halder, Mohammed Nasir Uddin, Md. Ashraf Uddin, Sunil Aryal and Ansam Khraisat
Citation:
Journal of Big Data
2024
11
:113
Content type:
Survey
Published on:
11 August 2024
View
Full Text
View
PDF
Deep SqueezeNet learning model for diagnosis and prediction of maize leaf diseases
The maize leaf diseases create severe yield reductions and critical problems. The maize leaf disease should be discovered early, perfectly identified, and precisely diagnosed to make greater yield. This work s...
Authors:
Prasannavenkatesan Theerthagiri, A. Usha Ruby, J. George Chellin Chandran, Tanvir Habib Sardar and Ahamed Shafeeq B. M.
Citation:
Journal of Big Data
2024
11
:112
Content type:
Research
Published on:
10 August 2024
View
Full Text
View
PDF
An aspect sentiment analysis model with Aspect Gated Convolution and Dual-Feature Filtering layers
Aspect level sentiment analysis is a basic task to determine the sentiment bias based on the contextual information near the aspect words. Some sentences contain many confusing feature words due to incomplete ...
Authors:
Hongfang Gong and Siyu Zhang
Citation:
Journal of Big Data
2024
11
:111
Content type:
Methodology
Published on:
9 August 2024
View
Full Text
View
PDF
Context-aware prediction of active and passive user engagement: Evidence from a large online social platform
The success of online social platforms hinges on their ability to predict and understand user behavior at scale. Here, we present data suggesting that context-aware modeling approaches may offer a holistic yet...
Authors:
Heinrich Peters, Yozen Liu, Francesco Barbieri, Raiyan Abdul Baten, Sandra C. Matz and Maarten W. Bos
Citation:
Journal of Big Data
2024
11
:110
Content type:
Research
Published on:
8 August 2024
View
Full Text
View
PDF
Analysis of Graeco-Latin square designs in the presence of uncertain data
This paper addresses the Graeco-Latin square design (GLSD) under neutrosophic statistics. In this work, we propose a novel approach for analyzing Graeco-Latin square designs using uncertain observations.
Authors:
Abdulrahman AlAita, Muhammad Aslam, Khaled Al Sultan and Muhammad Saleem
Citation:
Journal of Big Data
2024
11
:109
Content type:
Research
Published on:
7 August 2024
View
Full Text
View
PDF
Memetic multilabel feature selection using pruned refinement process
With the growing complexity of data structures, which include high-dimensional and multilabel datasets, the significance of feature selection has become more emphasized. Multilabel feature selection endeavors ...
Authors:
Wangduk Seo, Jaegyun Park, Sanghyuck Lee, A-Seong Moon, Dae-Won Kim and Jaesung Lee
Citation:
Journal of Big Data
2024
11
:108
Content type:
Research
Published on:
6 August 2024
View
Full Text
View
PDF
Sentiment-based predictive models for online purchases in the era of marketing 5.0: a systematic review
The convergence of artificial intelligence (AI), big data (DB), and Internet of Things (IoT) in Society 5.0, has given rise to Marketing 5.0, revolutionizing personalized customer experiences. In this study, a...
Authors:
Veerajay Gooljar, Tomayess Issa, Sarita Hardin-Ramanan and Bilal Abu-Salih
Citation:
Journal of Big Data
2024
11
:107
Content type:
Survey
Published on:
5 August 2024
View
Full Text
View
PDF
Unlocking the potential of Naive Bayes for spatio temporal classification: a novel approach to feature expansion
Prediction processes in areas ranging from climate and disease spread to disasters and air pollution rely heavily on spatial–temporal data. Understanding and forecasting the distribution patterns of disease ca...
Authors:
Sri Suryani Prasetiyowati and Yuliant Sibaroni
Citation:
Journal of Big Data
2024
11
:106
Content type:
Research
Published on:
5 August 2024
View
Full Text
View
PDF
Advancing cybersecurity: a comprehensive review of AI-driven detection techniques
As the number and cleverness of cyber-attacks keep increasing rapidly, it's more important than ever to have good ways to detect and prevent them. Recognizing cyber threats quickly and accurately is crucial be...
Authors:
Aya H. Salem, Safaa M. Azzam, O. E. Emam and Amr A. Abohany
Citation:
Journal of Big Data
2024
11
:105
Content type:
Survey
Published on:
4 August 2024
View
Full Text
View
PDF
Interpolation-split: a data-centric deep learning approach with big interpolated data to boost airway segmentation performance
The morphology and distribution of airway tree abnormalities enable diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critic...
Authors:
Wing Keung Cheung, Ashkan Pakzad, Nesrin Mogulkoc, Sarah Helen Needleman, Bojidar Rangelov, Eyjolfur Gudmundsson, An Zhao, Mariam Abbas, Davina McLaverty, Dimitrios Asimakopoulos, Robert Chapman, Recep Savas, Sam M. Janes, Yipeng Hu, Daniel C. Alexander, John R. Hurst…
Citation:
Journal of Big Data
2024
11
:104
Content type:
Research
Published on:
4 August 2024
View
Full Text
View
PDF
DiabSense: early diagnosis of non-insulin-dependent diabetes mellitus using smartphone-based human activity recognition and diabetic retinopathy analysis with Graph Neural Network
Non-Insulin-Dependent Diabetes Mellitus (NIDDM) is a chronic health condition caused by high blood sugar levels, and if not treated early, it can lead to serious complications i.e. blindness. Human Activity Re...
Authors:
Md Nuho Ul Alam, Ibrahim Hasnine, Erfanul Hoque Bahadur, Abdul Kadar Muhammad Masum, Mercedes Briones Urbano, Manuel Masias Vergara, Jia Uddin, Imran Ashraf and Md. Abdus Samad
Citation:
Journal of Big Data
2024
11
:103
Content type:
Research
Published on:
3 August 2024
View
Full Text
View
PDF
An adaptive composite time series forecasting model for short-term traffic flow
Short-term traffic flow forecasting is a hot issue in the field of intelligent transportation. The research field of traffic forecasting has evolved greatly in past decades. With the rapid development of deep ...
Authors:
Qitan Shao, Xinglin Piao, Xiangyu Yao, Yuqiu Kong, Yongli Hu, Baocai Yin and Yong Zhang
Citation:
Journal of Big Data
2024
11
:102
Content type:
Methodology
Published on:
3 August 2024
View
Full Text
View
PDF
Fitcam: detecting and counting repetitive exercises with deep learning
Physical fitness is one of the most important traits a person could have for health longevity. Conducting regular exercise is fundamental to maintaining physical fitness, but with the caveat of occurring injur...
Authors:
Ferdinandz Japhne, Kevin Janada, Agustinus Theodorus and Andry Chowanda
Citation:
Journal of Big Data
2024
11
:101
Content type:
Research
Published on:
3 August 2024
View
Full Text
View
PDF
Tc-llama 2: fine-tuning LLM for technology and commercialization applications
This paper introduces TC-Llama 2, a novel application of large language models (LLMs) in the technology-commercialization field. Traditional methods in this field, reliant on statistical learning and expert kn...
Authors:
Jeyoon Yeom, Hakyung Lee, Hoyoon Byun, Yewon Kim, Jeongeun Byun, Yunjeong Choi, Sungjin Kim and Kyungwoo Song
Citation:
Journal of Big Data
2024
11
:100
Content type:
Research
Published on:
2 August 2024
View
Full Text
View
PDF
An ensemble machine learning model for predicting one-year mortality in elderly coronary heart disease patients with anemia
This study was designed to develop and validate a robust predictive model for one-year mortality in elderly coronary heart disease (CHD) patients with anemia using machine learning methods.
Authors:
Longcan Cheng, Yan Nie, Hongxia Wen, Yan Li, Yali Zhao, Qian Zhang, Mingxing Lei and Shihui Fu
Citation:
Journal of Big Data
2024
11
:99
Content type:
Research
Published on:
24 July 2024
View
Full Text
View
PDF
Predictive modelling of MapReduce job performance in cloud environments using machine learning techniques
Within the Hadoop ecosystem, MapReduce stands as a cornerstone for managing, processing, and mining large-scale datasets. Yet, the absence of efficient solutions for precise estimation of job execution times p...
Authors:
Mohammed Bergui, Soufiane Hourri, Said Najah and Nikola S. Nikolov
Citation:
Journal of Big Data
2024
11
:98
Content type:
Research
Published on:
23 July 2024
View
Full Text
View
PDF
Hate speech detection in the Bengali language: a comprehensive survey
The detection of hate speech (HS) in online platforms has become extremely important for maintaining a safe and inclusive environment. While significant progress has been made in English-language HS detection,...
Authors:
Abdullah Al Maruf, Ahmad Jainul Abidin, Md. Mahmudul Haque, Zakaria Masud Jiyad, Aditi Golder, Raaid Alubady and Zeyar Aung
Citation:
Journal of Big Data
2024
11
:97
Content type:
Survey
Published on:
23 July 2024
View
Full Text
View
PDF
Introducing Mplots: scaling time series recurrence plots to massive datasets
Time series similarity matrices (informally, recurrence plots or dot-plots), are useful tools for time series data mining. They can be used to guide data exploration, and various useful features can be derived...
Authors:
Maryam Shahcheraghi, Ryan Mercer, João Manuel de Almeida Rodrigues, Audrey Der, Hugo Filipe Silveira Gamboa, Zachary Zimmerman, Kerry Mauck and Eamonn Keogh
Citation:
Journal of Big Data
2024
11
:96
Content type:
Research
Published on:
20 July 2024
View
Full Text
View
PDF
Text summarization based on semantic graphs: an abstract meaning representation graph-to-text deep learning approach
Nowadays, due to the constantly growing amount of textual information, automatic text summarization constitutes an important research area in natural language processing. In this work, we present a novel frame...
Authors:
Panagiotis Kouris, Georgios Alexandridis and Andreas Stafylopatis
Citation:
Journal of Big Data
2024
11
:95
Content type:
Research
Published on:
14 July 2024
View
Full Text
View
PDF
Examining ALS: reformed PCA and random forest for effective detection of ALS
ALS (Amyotrophic Lateral Sclerosis) is a fatal neurodegenerative disease of the human motor system. It is a group of progressive diseases that affects the nerve cells in the brain and spinal cord that control ...
Authors:
Abdullah Alqahtani, Shtwai Alsubai, Mohemmed Sha and Ashit Kumar Dutta
Citation:
Journal of Big Data
2024
11
:94
Content type:
Research
Published on:
10 July 2024
View
Full Text
View
PDF
Emotion AWARE: an artificial intelligence framework for adaptable, robust, explainable, and multi-granular emotion analysis
Emotions are fundamental to human behaviour. How we feel, individually and collectively, determines how humanity evolves and advances into our shared future. The rapid digitalisation of our personal, social an...
Authors:
Gihan Gamage, Daswin De Silva, Nishan Mills, Damminda Alahakoon and Milos Manic
Citation:
Journal of Big Data
2024
11
:93
Content type:
Methodology
Published on:
10 July 2024
View
Full Text
View
PDF
Exploring AI-driven approaches for unstructured document analysis and future horizons
In the current industrial landscape, a significant number of sectors are grappling with the challenges posed by unstructured data, which incurs financial losses amounting to millions annually. If harnessed eff...
Authors:
Supriya V. Mahadevkar, Shruti Patil, Ketan Kotecha, Lim Way Soong and Tanupriya Choudhury
Citation:
Journal of Big Data
2024
11
:92
Content type:
Survey
Published on:
5 July 2024
View
Full Text
View
PDF
New custom rating for improving recommendation system performance
Recommendation system is currently attracting the interest of many explorers. Various new businesses have surfaced with the rise of online marketing (E-Commerce) in response to Covid-19 pandemic. This phenomen...
Authors:
Tora Fahrudin and Dedy Rahman Wijaya
Citation:
Journal of Big Data
2024
11
:91
Content type:
Research
Published on:
2 July 2024
View
Full Text
View
PDF
Optimization-based convolutional neural model for the classification of white blood cells
White blood cells (WBCs) are one of the most significant parts of the human immune system, and they play a crucial role in diagnosing the characteristics of pathologists and blood-related diseases. The charact...
Authors:
Tulasi Gayatri Devi and Nagamma Patil
Citation:
Journal of Big Data
2024
11
:90
Content type:
Research
Published on:
26 June 2024
View
Full Text
View
PDF
Advanced RIME architecture for global optimization and feature selection
The article introduces an innovative approach to global optimization and feature selection (FS) using the RIME algorithm, inspired by RIME-ice formation. The RIME algorithm employs a soft-RIME search strategy ...
Authors:
Ruba Abu Khurma, Malik Braik, Abdullah Alzaqebah, Krishna Gopal Dhal, Robertas Damaševičius and Bilal Abu-Salih
Citation:
Journal of Big Data
2024
11
:89
Content type:
Research
Published on:
18 June 2024
View
Full Text
View
PDF
Feature reduction for hepatocellular carcinoma prediction using machine learning algorithms
Hepatocellular carcinoma (HCC) is a highly prevalent form of liver cancer that necessitates accurate prediction models for early diagnosis and effective treatment. Machine learning algorithms have demonstrated...
Authors:
Ghada Mostafa, Hamdi Mahmoud, Tarek Abd El-Hafeez and Mohamed E. ElAraby
Citation:
Journal of Big Data
2024
11
:88
Content type:
Research
Published on:
18 June 2024
View
Full Text
View
PDF
Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering
The classification of imbalanced datasets is a prominent task in text mining and machine learning. The number of samples in each class is not uniformly distributed; one class contains a large number of samples...
Authors:
Muhammad Mujahid, EROL Kına, Furqan Rustam, Monica Gracia Villar, Eduardo Silva Alvarado, Isabel De La Torre Diez and Imran Ashraf
Citation:
Journal of Big Data
2024
11
:87
Content type:
Research
Published on:
17 June 2024
View
Full Text
View
PDF
Advancing machine learning with OCR2SEQ: an innovative approach to multi-modal data augmentation
OCR2SEQ represents an innovative advancement in Optical Character Recognition (OCR) technology, leveraging a multi-modal generative augmentation strategy to overcome traditional limitations in OCR systems. Thi...
Authors:
Michael Lowe, Joseph D. Prusa, Joffrey L. Leevy and Taghi M. Khoshgoftaar
Citation:
Journal of Big Data
2024
11
:86
Content type:
Research
Published on:
13 June 2024
View
Full Text
View
PDF
Previous
page
1
2
3
4
5
…
20
Next
page
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0):
A survey on Image Data Augmentation for Deep Learning | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
A survey on Image Data Augmentation for Deep Learning
Download PDF
Download ePub
Download PDF
Download ePub
Survey paper
Open access
Published:
06 July 2019
A survey on Image Data Augmentation for Deep Learning
Connor Shorten
ORCID:
orcid.org/0000-0001-6253-6861
1
&
Taghi M. Khoshgoftaar
1
Journal of Big Data
volume
6
, Article number:
60
(
2019
)
Cite this article
534k
Accesses
6133
Citations
102
Altmetric
Metrics
details
Abstract
Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.
Introduction
Deep Learning models have made incredible progress in discriminative tasks. This has been fueled by the advancement of deep network architectures, powerful computation, and access to big data. Deep neural networks have been successfully applied to Computer Vision tasks such as image classification, object detection, and image segmentation thanks to the development of convolutional neural networks (CNNs). These neural networks utilize parameterized, sparsely connected kernels which preserve the spatial characteristics of images. Convolutional layers sequentially downsample the spatial resolution of images while expanding the depth of their feature maps. This series of convolutional transformations can create much lower-dimensional and more useful representations of images than what could possibly be hand-crafted. The success of CNNs has spiked interest and optimism in applying Deep Learning to Computer Vision tasks.
There are many branches of study that hope to improve current benchmarks by applying deep convolutional networks to Computer Vision tasks. Improving the generalization ability of these models is one of the most difficult challenges. Generalizability refers to the performance difference of a model when evaluated on previously seen data (training data) versus data it has never seen before (testing data). Models with poor generalizability have overfitted the training data. One way to discover overfitting is to plot the training and validation accuracy at each epoch during training. The graph below depicts what overfitting might look like when visualizing these accuracies over training epochs (Fig.
1
).
Fig. 1
The plot on the left shows an inflection point where the validation error starts to increase as the training rate continues to decrease. The increased training has caused the model to overfit to the training data and perform poorly on the testing set relative to the training set. In contrast, the plot on the right shows a model with the desired relationship between training and testing error
Full size image
To build useful Deep Learning models, the validation error must continue to decrease with the training error. Data Augmentation is a very powerful method of achieving this. The augmented data will represent a more comprehensive set of possible data points, thus minimizing the distance between the training and validation set, as well as any future testing sets.
Data Augmentation, the focus of this survey, is not the only technique that has been developed to reduce overfitting. The following few paragraphs will introduce other solutions available to avoid overfitting in Deep Learning models. This listing is intended to give readers a broader understanding of the context of Data Augmentation.
Many other strategies for increasing generalization performance focus on the model’s architecture itself. This has led to a sequence of progressively more complex architectures from AlexNet [
1
] to VGG-16 [
2
], ResNet [
3
], Inception-V3 [
4
], and DenseNet [
5
]. Functional solutions such as dropout regularization, batch normalization, transfer learning, and pretraining have been developed to try to extend Deep Learning for application on smaller datasets. A brief description of these overfitting solutions is provided below. A complete survey of regularization methods in Deep Learning has been compiled by Kukacka et al. [
6
]. Knowledge of these overfitting solutions will inform readers about other existing tools, thus framing the high-level context of Data Augmentation and Deep Learning.
Dropout [
7
] is a regularization technique that zeros out the activation values of randomly chosen neurons during training. This constraint forces the network to learn more robust features rather than relying on the predictive capability of a small subset of neurons in the network. Tompson et al. [
8
] extended this idea to convolutional networks with Spatial Dropout, which drops out entire feature maps rather than individual neurons.
Batch normalization [
9
] is another regularization technique that normalizes the set of activations in a layer. Normalization works by subtracting the batch mean from each activation and dividing by the batch standard deviation. This normalization technique, along with standardization, is a standard technique in the preprocessing of pixel values.
Transfer Learning [
10
,
11
] is another interesting paradigm to prevent overfitting. Transfer Learning works by training a network on a big dataset such as ImageNet [
12
] and then using those weights as the initial weights in a new classification task. Typically, just the weights in convolutional layers are copied, rather than the entire network including fully-connected layers. This is very effective since many image datasets share low-level spatial characteristics that are better learned with big data. Understanding the relationship between transferred data domains is an ongoing research task [
13
]. Yosinski et al. [
14
] find that transferability is negatively affected primarily by the specialization of higher layer neurons and difficulties with splitting co-adapted neurons.
Pretraining [
15
] is conceptually very similar to transfer learning. In Pretraining, the network architecture is defined and then trained on a big dataset such as ImageNet [
12
]. This differs from Transfer Learning because in Transfer Learning, the network architecture such as VGG-16 [
2
] or ResNet [
3
] must be transferred as well as the weights. Pretraining enables the initialization of weights using big datasets, while still enabling flexibility in network architecture design.
One-shot and Zero-shot learning [
16
,
17
] algorithms represent another paradigm for building models with extremely limited data. One-shot learning is commonly used in facial recognition applications [
18
]. An approach to one-shot learning is the use of siamese networks [
19
] that learn a distance function such that image classification is possible even if the network has only been trained on one or a few instances. Another very popular approach to one-shot learning is the use of memory-augmented networks [
20
]. Zero-shot learning is a more extreme paradigm in which a network uses input and output vector embeddings such as Word2Vec [
21
] or GloVe [
22
] to classify images based on descriptive attributes.
In contrast to the techniques mentioned above, Data Augmentation approaches overfitting from the root of the problem, the training dataset. This is done under the assumption that more information can be extracted from the original dataset through augmentations. These augmentations artificially inflate the training dataset size by either data warping or oversampling. Data warping augmentations transform existing images such that their label is preserved. This encompasses augmentations such as geometric and color transformations, random erasing, adversarial training, and neural style transfer. Oversampling augmentations create synthetic instances and add them to the training set. This includes mixing images, feature space augmentations, and generative adversarial networks (GANs). Oversampling and Data Warping augmentations do not form a mutually exclusive dichotomy. For example, GAN samples can be stacked with random cropping to further inflate the dataset. Decisions around final dataset size, test-time augmentation, curriculum learning, and the impact of resolution are covered in this survey under the “
Design considerations for image Data Augmentation
” section. Descriptions of individual augmentation techniques will be enumerated in the “
Image Data Augmentation techniques
” section. A quick taxonomy of the Data Augmentations is depicted below in Fig.
2
.
Fig. 2
A taxonomy of image data augmentations covered; the colored lines in the figure depict which data augmentation method the corresponding meta-learning scheme uses, for example, meta-learning using Neural Style Transfer is covered in neural augmentation [
36
]
Full size image
Before discussing image augmentation techniques, it is useful to frame the context of the problem and consider what makes image recognition such a difficult task in the first place. In classic discriminative examples such as cat versus dog, the image recognition software must overcome issues of viewpoint, lighting, occlusion, background, scale, and more. The task of Data Augmentation is to bake these translational invariances into the dataset such that the resulting models will perform well despite these challenges.
It is a generally accepted notion that bigger datasets result in better Deep Learning models [
23
,
24
]. However, assembling enormous datasets can be a very daunting task due to the manual effort of collecting and labeling data. Limited datasets is an especially prevalent challenge in medical image analysis. Given big data, deep convolutional networks have been shown to be very powerful for medical image analysis tasks such as skin lesion classification as demonstrated by Esteva et al. [
25
]. This has inspired the use of CNNs on medical image analysis tasks [
26
] such as liver lesion classification, brain scan analysis, continued research in skin lesion classification, and more. Many of the images studied are derived from computerized tomography (CT) and magnetic resonance imaging (MRI) scans, both of which are expensive and labor-intensive to collect. It is especially difficult to build big medical image datasets due to the rarity of diseases, patient privacy, the requirement of medical experts for labeling, and the expense and manual effort needed to conduct medical imaging processes. These obstacles have led to many studies on image Data Augmentation, especially GAN-based oversampling, from the application perspective of medical image classification.
Many studies on the effectiveness of Data Augmentation utilize popular academic image datasets to benchmark results. These datasets include MNIST hand written digit recognition, CIFAR-10/100, ImageNet, tiny-imagenet-200, SVHN (street view house numbers), Caltech-101/256, MIT places, MIT-Adobe 5K dataset, Pascal VOC, and Stanford Cars. The datasets most frequently discussed are CIFAR-10, CIFAR-100, and ImageNet. The expansion of open-source datasets has given researchers a wide variety of cases to compare performance results of Data Augmentation techniques. Most of these datasets such as ImageNet would be classified as big data. Many experiments constrain themselves to a subset of the dataset to simulate limited data problems.
In addition to our focus on limited datasets, we will also consider the problem of class imbalance and how Data Augmentation can be a useful oversampling solution. Class imbalance describes a dataset with a skewed ratio of majority to minority samples. Leevy et al. [
27
] describe many of the existing solutions to high-class imbalance across data types. Our survey will show how class-balancing oversampling in image data can be done with Data Augmentation.
Many aspects of Deep Learning and neural network models draw comparisons with human intelligence. For example, a human intelligence anecdote of transfer learning is illustrated in learning music. If two people are trying to learn how to play the guitar, and one already knows how to play the piano, it seems likely that the piano-player will learn to play the guitar faster. Analogous to learning music, a model that can classify ImageNet images will likely perform better on CIFAR-10 images than a model with random weights.
Data Augmentation is similar to imagination or dreaming. Humans imagine different scenarios based on experience. Imagination helps us gain a better understanding of our world. Data Augmentation methods such as GANs and Neural Style Transfer can ‘imagine’ alterations to images such that they have a better understanding of them. The remainder of the paper is organized as follows: A brief “
Background
” is provided to give readers a historical context of Data Augmentation and Deep Learning. “
Image Data Augmentation techniques
” discusses each image augmentation technique in detail along with experimental results. “
Design considerations for image Data Augmentation
” discusses additional characteristics of augmentation such as test-time augmentation and the impact of image resolution. The paper concludes with a “
Discussion
” of the presented material, areas of “
Future work
”, and “
Conclusion
”.
Background
Image augmentation in the form of data warping can be found in LeNet-5 [
28
]. This was one of the first applications of CNNs on handwritten digit classification. Data augmentation has also been investigated in oversampling applications. Oversampling is a technique used to re-sample imbalanced class distributions such that the model is not overly biased towards labeling instances as the majority class type. Random Oversampling (ROS) is a naive approach which duplicates images randomly from the minority class until a desired class ratio is achieved. Intelligent oversampling techniques date back to SMOTE (Synthetic Minority Over-sampling Technique), which was developed by Chawla et al. [
29
]. SMOTE and the extension of Borderline-SMOTE [
30
] create new instances by interpolating new points from existing instances via k-Nearest Neighbors. The primary focus of this technique was to alleviate problems due to class imbalance, and SMOTE was primarily used for tabular and vector data.
The AlexNet CNN architecture developed by Krizhevsky et al. [
1
] revolutionized image classification by applying convolutional networks to the ImageNet dataset. Data Augmentation is used in their experiments to increase the dataset size by a magnitude of 2048. This is done by randomly cropping 224 × 224 patches from the original images, flipping them horizontally, and changing the intensity of the RGB channels using PCA color augmentation. This Data Augmentation helped reduce overfitting when training a deep neural network. The authors claim that their augmentations reduced the error rate of the model by over 1%.
Since then, GANs were introduced in 2014 [
31
], Neural Style Transfer [
32
] in 2015, and Neural Architecture Search (NAS) [
33
] in 2017. Various works on GAN extensions such as DCGANs, CycleGANs and Progressively-Growing GANs [
34
] were published in 2015, 2017, and 2017, respectively. Neural Style Transfer was sped up with the development of Perceptual Losses by Johnson et al. [
35
] in 2016. Applying meta-learning concepts from NAS to Data Augmentation has become increasingly popular with works such as Neural Augmentation [
36
], Smart Augmentation [
37
], and AutoAugment [
38
] published in 2017, 2017, and 2018, respectively.
Applying Deep Learning to medical imaging has been a popular application for CNNs since they became so popular in 2012. Deep Learning and medical imaging became increasingly popular with the demonstration of dermatologist-level skin cancer detection by Esteva et al. [
25
] in 2017.
The use of GANs in medical imaging is well documented in a survey by Yi et al. [
39
]. This survey covers the use of GANs in reconstruction such as CT denoising [
40
], accelerated magnetic resonance imaging [
41
], PET denoising [
42
], and the application of super-resolution GANs in retinal vasculature segmentation [
43
]. Additionally, Yi et al. [
39
] cover the use of GAN image synthesis in medical imaging applications such as brain MRI synthesis [
44
,
45
], lung cancer diagnosis [
46
], high-resolution skin lesion synthesis [
47
], and chest x-ray abnormality classification [
48
]. GAN-based image synthesis Data Augmentation was used by Frid-Adar et al. [
49
] in 2018 for liver lesion classification. This improved classification performance from 78.6% sensitivity and 88.4% specificity using classic augmentations to 85.7% sensitivity and 92.4% specificity using GAN-based Data Augmentation.
Most of the augmentations covered focus on improving Image Recognition models. Image Recognition is when a model predicts an output label such as ‘dog’ or ‘cat’ given an input image.
However, it is possible to extend results from image recognition to other Computer Vision tasks such as Object Detection led by the algorithms YOLO [
50
], R-CNN [
51
], fast R-CNN [
52
], and faster R-CNN [
53
] or Semantic Segmentation [
54
] including algorithms such as U-Net [
55
].
Image Data Augmentation techniques
The earliest demonstrations showing the effectiveness of Data Augmentations come from simple transformations such as horizontal flipping, color space augmentations, and random cropping. These transformations encode many of the invariances discussed earlier that present challenges to image recognition tasks. The augmentations listed in this survey are geometric transformations, color space transformations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, GAN-based augmentation, neural style transfer, and meta-learning schemes. This section will explain how each augmentation algorithm works, report experimental results, and discuss disadvantages of the augmentation technique.
Data Augmentations based on basic image manipulations
Geometric transformations
This section describes different augmentations based on geometric transformations and many other image processing functions. The class of augmentations discussed below could be characterized by their ease of implementation. Understanding these transformations will provide a useful base for further investigation into Data Augmentation techniques.
We will also describe the different geometric augmentations in the context of their ‘safety’ of application. The safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation. For example, rotations and flips are generally safe on ImageNet challenges such as cat versus dog, but not safe for digit recognition tasks such as 6 versus 9. A non-label preserving transformation could potentially strengthen the model’s ability to output a response indicating that it is not confident about its prediction. However, achieving this would require refined labels [
56
] post-augmentation. If the label of the image after a non-label preserving transformation is something like [0.5 0.5], the model could learn more robust confidence predictions. However, constructing refined labels for every non-safe Data Augmentation is a computationally expensive process.
Due to the challenge of constructing refined labels for post-augmented data, it is important to consider the ‘safety’ of an augmentation. This is somewhat domain dependent, providing a challenge for developing generalizable augmentation policies, (see AutoAugment [
38
] for further exploration into finding generalizable augmentations). There is no image processing function that cannot result in a label changing transformation at some distortion magnitude. This demonstrates the data-specific design of augmentations and the challenge of developing generalizable augmentation policies. This is an important consideration with respect to the geometric augmentations listed below.
Flipping
Horizontal axis flipping is much more common than flipping the vertical axis. This augmentation is one of the easiest to implement and has proven useful on datasets such as CIFAR-10 and ImageNet. On datasets involving text recognition such as MNIST or SVHN, this is not a label-preserving transformation.
Color space
Digital image data is usually encoded as a tensor of the dimension (height × width × color channels). Performing augmentations in the color channels space is another strategy that is very practical to implement. Very simple color augmentations include isolating a single color channel such as R, G, or B. An image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels. Additionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image. More advanced color augmentations come from deriving a color histogram describing the image. Changing the intensity values in these histograms results in lighting alterations such as what is used in photo editing applications.
Cropping
Cropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image. Additionally, random cropping can also be used to provide an effect very similar to translations. The contrast between random cropping and translations is that cropping will reduce the size of the input such as (256,256) → (224, 224), whereas translations preserve the spatial dimensions of the image. Depending on the reduction threshold chosen for cropping, this might not be a label-preserving transformation.
Rotation
Rotation augmentations are done by rotating the image right or left on an axis between 1° and 359°. The safety of rotation augmentations is heavily determined by the rotation degree parameter. Slight rotations such as between 1 and 20 or − 1 to − 20 could be useful on digit recognition tasks such as MNIST, but as the rotation degree increases, the label of the data is no longer preserved post-transformation.
Translation
Shifting images left, right, up, or down can be a very useful transformation to avoid positional bias in the data. For example, if all the images in a dataset are centered, which is common in face recognition datasets, this would require the model to be tested on perfectly centered images as well. As the original image is translated in a direction, the remaining space can be filled with either a constant value such as 0 s or 255 s, or it can be filled with random or Gaussian noise. This padding preserves the spatial dimensions of the image post-augmentation.
Noise injection
Noise injection consists of injecting a matrix of random values usually drawn from a Gaussian distribution. Noise injection is tested by Moreno-Barea et al. [
57
] on nine datasets from the UCI repository [
58
]. Adding noise to images can help CNNs learn more robust features.
Geometric transformations are very good solutions for positional biases present in the training data. There are many potential sources of bias that could separate the distribution of the training data from the testing data. If positional biases are present, such as in a facial recognition dataset where every face is perfectly centered in the frame, geometric transformations are a great solution. In addition to their powerful ability to overcome positional biases, geometric transformations are also useful because they are easily implemented. There are many imaging processing libraries that make operations such as horizontal flipping and rotation painless to get started with. Some of the disadvantages of geometric transformations include additional memory, transformation compute costs, and additional training time. Some geometric transformations such as translation or random cropping must be manually observed to make sure they have not altered the label of the image. Finally, in many of the application domains covered such as medical image analysis, the biases distancing the training data from the testing data are more complex than positional and translational variances. Therefore, the scope of where and when geometric transformations can be applied is relatively limited.
Color space transformations
Image data is encoded into 3 stacked matrices, each of size height × width. These matrices represent pixel values for an individual RGB color value. Lighting biases are amongst the most frequently occurring challenges to image recognition problems. Therefore, the effectiveness of color space transformations, also known as photometric transformations, is fairly intuitive to conceptualize. A quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value. Another quick color space manipulation is to splice out individual RGB color matrices. Another transformation consists of restricting pixel values to a certain min or max value. The intrinsic representation of color in digital images lends itself to many strategies of augmentation.
Color space transformations can also be derived from image-editing apps. An image’s pixel values in each RGB color channel is aggregated to form a color histogram. This histogram can be manipulated to apply filters that change the color space characteristics of an image.
There is a lot of freedom for creativity with color space augmentations. Altering the color distribution of images can be a great solution to lighting challenges faced by testing data (Figs.
3
,
4
).
Fig. 3
Examples of Color Augmentations provided by Mikolajczyk and Grochowski [
72
] in the domain of melanoma classification
Full size image
Fig. 4
Examples of color augmentations tested by Wu et al. [
127
]
Full size image
Image datasets can be simplified in representation by converting the RGB matrices into a single grayscale image. This results in smaller images, height × width × 1, resulting in faster computation. However, this has been shown to reduce performance accuracy. Chatifled et al. [
59
] found a ~ 3% classification accuracy drop between grayscale and RGB images with their experiments on ImageNet [
12
] and the PASCAL [
60
] VOC dataset. In addition to RGB versus grayscale images, there are many other ways of representing digital color such as HSV (Hue, Saturation, and Value). Jurio et al. [
61
] explore the performance of Image Segmentation on many different color space representations from RGB to YUV, CMY, and HSV.
Similar to geometric transformations, a disadvantage of color space transformations is increased memory, transformation costs, and training time. Additionally, color transformations may discard important color information and thus are not always a label-preserving transformation. For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image. Another indirect example of non-label preserving color transformations is in Image Sentiment Analysis [
62
]. In this application, CNNs try to visually predict the sentiment score of an image such as: highly negative, negative, neutral, positive, or highly positive. One indicator of a negative/highly negative image is the presence of blood. The dark red color of blood is a key component to distinguish blood from water or paint. If color space transforms repeatedly change the color space such that the model cannot recognize red blood from green paint, the model will perform poorly on Image Sentiment Analysis. In effect, color space transformations will eliminate color biases present in the dataset in favor of spatial characteristics. However, for some tasks, color is a very important distinctive feature.
Geometric versus photometric transformations
Taylor and Nitschke [
63
] provide a comparative study on the effectiveness of geometric and photometric (color space) transformations. The geometric transformations studied were flipping, − 30° to 30° rotations, and cropping. The color space transformations studied were color jittering, (random color manipulation), edge enhancement, and PCA. They tested these augmentations with 4-fold cross-validation on the Caltech101 dataset filtered to 8421 images of size 256 × 256 (Table
1
).
Table 1 Results of Taylor and Nitschke’s Data Augmentation experiments on Caltech101 [
63
]
Full size table
Kernel filters
Kernel filters are a very popular technique in image processing to sharpen and blur images. These filters work by sliding an
n
×
n
matrix across an image with either a Gaussian blur filter, which will result in a blurrier image, or a high contrast vertical or horizontal edge filter which will result in a sharper image along edges. Intuitively, blurring images for Data Augmentation could lead to higher resistance to motion blur during testing. Additionally, sharpening images for Data Augmentation could result in encapsulating more details about objects of interest.
Sharpening and blurring are some of the classical ways of applying kernel filters to images. Kang et al. [
64
] experiment with a unique kernel filter that randomly swaps the pixel values in an
n
×
n
sliding window. They call this augmentation technique PatchShuffle Regularization. Experimenting across different filter sizes and probabilities of shuffling the pixels at each step, they demonstrate the effectiveness of this by achieving a 5.66% error rate on CIFAR-10 compared to an error rate of 6.33% achieved without the use of PatchShuffle Regularization. The hyperparameter settings that achieved this consisted of 2 × 2 filters and a 0.05 probability of swapping. These experiments were done using the ResNet [
3
] CNN architecture (Figs.
5
,
6
).
Fig. 5
Examples of applying the PatchShuffle regularization technique [
64
]
Full size image
Fig. 6
Pixels in a n × n window are randomly shifted with a probability parameter p
Full size image
Kernel filters are a relatively unexplored area for Data Augmentation. A disadvantage of this technique is that it is very similar to the internal mechanisms of CNNs. CNNs have parametric kernels that learn the optimal way to represent images layer-by-layer. For example, something like PatchShuffle Regularization could be implemented with a convolution layer. This could be achieved by modifying the standard convolution layer parameters such that the padding parameters preserve spatial resolution and the subsequent activation layer keeps pixel values between 0 and 255, in contrast to something like a sigmoid activation which maps pixels to values between 0 and 1. Therefore kernel filters can be better implemented as a layer of the network rather than as an addition to the dataset through Data Augmentation.
Mixing images
Mixing images together by averaging their pixel values is a very counterintuitive approach to Data Augmentation. The images produced by doing this will not look like a useful transformation to a human observer. However, Ionue [
65
] demonstrated how the pairing of samples could be developed into an effective augmentation strategy. In this experiment, two images are randomly cropped from 256 × 256 to 224 × 224 and randomly flipped horizontally. These images are then mixed by averaging the pixel values for each of the RGB channels. This results in a mixed image which is used to train a classification model. The label assigned to the new image is the same as the first randomly selected image (Fig.
7
).
Fig. 7
SamplePairing augmentation strategy [
65
]
Full size image
On the CIFAR-10 dataset, Ionue reported a reduction in error rate from 8.22 to 6.93% when using the SamplePairing Data Augmentation technique. The researcher found even better results when testing a reduced size dataset, reducing CIFAR-10 to 1000 total samples with 100 in each class. With the reduced size dataset, SamplePairing resulted in an error rate reduction from 43.1 to 31.0%. The reduced CIFAR-10 results demonstrate the usefulness of the SamplePairing technique in limited data applications (Fig.
8
).
Fig. 8
Results on the reduced CIFAR-10 dataset. Experimental results demonstrated with respect to sampling pools for image mixing [
65
]
Full size image
Another detail found in the study is that better results were obtained when mixing images from the entire training set rather than from instances exclusively belonging to the same class. Starting from a training set of size N, SamplePairing produces a dataset of size N
2
+ N. In addition, Sample Pairing can be stacked on top of other augmentation techniques. For example, if using the augmentations demonstrated in the AlexNet paper by Krizhevsky et al. [
1
], the 2048 × dataset increase can be further expanded to (2048 × N)
2
.
The concept of mixing images in an unintuitive way was further investigated by Summers and Dinneen [
66
]. They looked at using non-linear methods to combine images into new training instances. All of the methods they used resulted in better performance compared to the baseline models (Fig.
9
).
Fig. 9
Non-linearly mixing images [
66
]
Full size image
Amongst these non-linear augmentations tested, the best technique resulted in a reduction from 5.4 to 3.8% error on CIFAR-10 and 23.6% to 19.7% on CIFAR-100. In like manner, Liang et al. [
67
] used GANs to produce mixed images. They found that the inclusion of mixed images in the training data reduced training time and increased the diversity of GAN-samples. Takahashi and Matsubara [
68
] experiment with another approach to mixing images that randomly crops images and concatenates the croppings together to form new images as depicted below. The results of their technique, as well as SamplePairing and mixup augmentation, demonstrate the sometimes unreasonable effectiveness of big data with Deep Learning models (Fig.
10
).
Fig. 10
Mixing images through random image cropping and patching [
68
]
Full size image
An obvious disadvantage of this technique is that it makes little sense from a human perspective. The performance boost found from mixing images is very difficult to understand or explain. One possible explanation for this is that the increased dataset size results in more robust representations of low-level characteristics such as lines and edges. Testing the performance of this in comparisons to transfer learning and pretraining methods is an interesting area for future work. Transfer learning and pretraining are other techniques that learn low-level characteristics in CNNs. Additionally, it will be interesting to see how the performance changes if we partition the training data such that the first 100 epochs are trained with original and mixed images and the last 50 with original images only. These kinds of strategies are discussed further in Design Considerations of Data Augmentation with respect to curriculum learning [
69
]. Additionally, the paper will cover a meta-learning technique developed by Lemley et al. [
37
] that uses a neural network to learn an optimal mixing of images.
Random erasing
Random erasing [
70
] is another interesting Data Augmentation technique developed by Zhong et al. Inspired by the mechanisms of dropout regularization, random erasing can be seen as analogous to dropout except in the input data space rather than embedded into the network architecture. This technique was specifically designed to combat image recognition challenges due to occlusion. Occlusion refers to when some parts of the object are unclear. Random erasing will stop this by forcing the model to learn more descriptive features about an image, preventing it from overfitting to a certain visual feature in the image. Aside from the visual challenge of occlusion, in particular, random erasing is a promising technique to guarantee a network pays attention to the entire image, rather than just a subset of it.
Random erasing works by randomly selecting an n × m patch of an image and masking it with either 0 s, 255 s, mean pixel values, or random values. On the CIFAR-10 dataset this resulted in an error rate reduction from 5.17 to 4.31%. The best patch fill method was found to be random values. The fill method and size of the masks are the only parameters that need to be hand-designed during implementation (Figs.
11
,
12
).
Fig. 11
Example of random erasing on image recognition tasks [
70
]
Full size image
Fig. 12
Example of random erasing on object detection tasks [
70
]
Full size image
Random erasing is a Data Augmentation method that seeks to directly prevent overfitting by altering the input space. By removing certain input patches, the model is forced to find other descriptive characteristics. This augmentation method can also be stacked on top of other augmentation techniques such as horizontal flipping or color filters. Random erasing produced one of the highest accuracies on the CIFAR-10 dataset. DeVries and Taylor [
71
] conducted a similar study called Cutout Regularization. Like the random erasing study, they experimented with randomly masking regions of the image (Table
2
).
Table 2 Results of Cutout Regularization [
104
], plus denotes using traditional augmentation methods, horizontal flipping and cropping
Full size table
Mikolajcyzk and Grochowski [
72
] presented an interesting idea to combine random erasing with GANs designed for image inpainting. Image inpainting describes the task of filling in a missing piece of an image. Using a diverse collection of GAN inpainters, the random erasing augmentation could seed very interesting extrapolations. It will be interesting to see if better results can be achieved by erasing different shaped patches such as circles rather than
n
×
m
rectangles. An extension of this will be to parameterize the geometries of random erased patches and learn an optimal erasing configuration.
A disadvantage to random erasing is that it will not always be a label-preserving transformation. In handwritten digit recognition, if the top part of an ‘8’ is randomly cropped out, it is not any different from a ‘6’. In many fine-grained tasks such as the Stanford Cars dataset [
73
], randomly erasing sections of the image (logo, etc.) may make the car brand unrecognizable. Therefore, some manual intervention may be necessary depending on the dataset and task.
A note on combining augmentations
Of the augmentations discussed, geometric transformations, color space transformations, kernel filters, mixing images, and random erasing, nearly all of these transformations come with an associated distortion magnitude parameter as well. This parameter encodes the distortional difference between a 45° rotation and a 30° rotation. With a large list of potential augmentations and a mostly continuous space of magnitudes, it is easy to conceptualize the enormous size of the augmentation search space. Combining augmentations such as cropping, flipping, color shifts, and random erasing can result in massively inflated dataset sizes. However, this is not guaranteed to be advantageous. In domains with very limited data, this could result in further overfitting. Therefore, it is important to consider search algorithms for deriving an optimal subset of augmented data to train Deep Learning models with. More on this topic will be discussed in Design Considerations of Data Augmentation.
Data Augmentations based on Deep Learning
Feature space augmentation
All of the augmentation methods discussed above are applied to images in the input space. Neural networks are incredibly powerful at mapping high-dimensional inputs into lower-dimensional representations. These networks can map images to binary classes or to
n
× 1 vectors in flattened layers. The sequential processing of neural networks can be manipulated such that the intermediate representations can be separated from the network as a whole. The lower-dimensional representations of image data in fully-connected layers can be extracted and isolated. Konno and Iwazume [
74
] find a performance boost on CIFAR-100 from 66 to 73% accuracy by manipulating the modularity of neural networks to isolate and refine individual layers after training. Lower-dimensional representations found in high-level layers of a CNN are known as the feature space. DeVries and Taylor [
75
] presented an interesting paper discussing augmentation in this feature space. This opens up opportunities for many vector operations for Data Augmentation.
SMOTE is a popular augmentation used to alleviate problems with class imbalance. This technique is applied to the feature space by joining the
k
nearest neighbors to form new instances. DeVries and Taylor discuss adding noise, interpolating, and extrapolating as common forms of feature space augmentation (Figs.
13
,
14
).
Fig. 13
Architecture diagram of the feature space augmentation framework presented by DeVries and Taylor [
75
]
Full size image
Fig. 14
Examples of interpolated instances in the feature space on the handwritten ‘@’ character [
75
]
Full size image
The use of auto-encoders is especially useful for performing feature space augmentations on data. Autoencoders work by having one half of the network, the encoder, map images into low-dimensional vector representations such that the other half of the network, the decoder, can reconstruct these vectors back into the original image. This encoded representation is used for feature space augmentation.
DeVries and Taylor [
75
] tested their feature space augmentation technique by extrapolating between the 3 nearest neighbors per sample to generate new data and compared their results against extrapolating in the input space and using affine transformations in the input space (Table
3
).
Table 3 Performance results of the experiment with feature vs. input space extrapolation on MNIST and CIFAR-10 [
75
]
Full size table
Feature space augmentations can be implemented with auto-encoders if it is necessary to reconstruct the new instances back into input space. It is also possible to do feature space augmentation solely by isolating vector representations from a CNN. This is done by cutting off the output layer of the network, such that the output is a low-dimensional vector rather than a class label. Vector representations are then found by training a CNN and then passing the training set through the truncated CNN. These vector representations can be used to train any machine learning model from Naive Bayes, Support Vector Machine, or back to a fully-connected multilayer network. The effectiveness of this technique is a subject for future work.
A disadvantage of feature space augmentation is that it is very difficult to interpret the vector data. It is possible to recover the new vectors into images using an auto-encoder network; however, this requires copying the entire encoding part of the CNN being trained. For deep CNNs, this results in massive auto-encoders which are very difficult and time-consuming to train. Finally, Wong et al. [
76
] find that when it is possible to transform images in the data-space, data-space augmentation will outperform feature space augmentation.
Adversarial training
One of the solutions to search the space of possible augmentations is adversarial training. Adversarial training is a framework for using two or more networks with contrasting objectives encoded in their loss functions. This section will discuss using adversarial training as a search algorithm as well as the phenomenon of adversarial attacking. Adversarial attacking consists of a rival network that learns augmentations to images that result in misclassifications in its rival classification network. These adversarial attacks, constrained to noise injections, have been surprisingly successful from the perspective of the adversarial network. This is surprising because it completely defies intuition about how these models represent images. The adversarial attacks demonstrate that representations of images are much less robust than what might have been expected. This is well demonstrated by Moosavi-Dezfooli et al. [
77
] using DeepFool, a network that finds the minimum possible noise injection needed to cause a misclassification with high confidence. Su et al. [
78
] show that 70.97% of images can be misclassified by changing just one pixel. Zajac et al. [
79
] cause misclassifications with adversarial attacks limited to the border of images. The success of adversarial attacks is especially exaggerated as the resolution of images increases.
Adversarial attacking can be targeted or untargeted, referring to the deliberation in which the adversarial network is trying to cause misclassifications. Adversarial attacks can help to illustrate weak decision boundaries better than standard classification metrics can.
In addition to serving as an evaluation metric, defense to adversarial attacks, adversarial training can be an effective method for searching for augmentations.
By constraining the set of augmentations and distortions available to an adversarial network, it can learn to produce augmentations that result in misclassifications, thus forming an effective search algorithm. These augmentations are valuable for strengthening weak spots in the classification model. Therefore, adversarial training can be an effective search technique for Data Augmentation. This is in heavy contrast to the traditional augmentation techniques described previously. Adversarial augmentations may not represent examples likely to occur in the test set, but they can improve weak spots in the learned decision boundary.
Engstrom et al. [
80
] showed that simple transformations such as rotations and translations can easily cause misclassifications by deep CNN models. The worst out of the random transformations reduced the accuracy of MNIST by 26%, CIFAR10 by 72% and ImageNet (Top 1) by 28%. Goodfellow et al. [
81
] generate adversarial examples to improve performance on the MNIST classification task. Using a technique for generating adversarial examples known as the “fast gradient sign method”, a maxout network [
82
] misclassified 89.4% of adversarial examples with an average confidence of 97.6%. This test is done on the MNIST dataset. With adversarial training, the error rate of adversarial examples fell from 89.4% to 17.9% (Fig.
15
).
Fig. 15
Adversarial misclassification example [
81
]
Full size image
Li et al. [
83
] experiment with a novel adversarial training approach and compare the performance on original testing data and adversarial examples. The results displayed below show how anticipation of adversarial attacks in the training process can dramatically reduce the success of attacks.
As shown in Table
4
, the adversarial training in their experiment did not improve the test accuracy. However, it does significantly improve the test accuracy of adversarial examples. Adversarial defense is a very interesting subject for evaluating security and robustness of Deep Learning models. Improving on the Fast Gradient Sign Method, DeepFool, developed by Moosavi-Dezfooli et al. [
77
], uses a neural network to find the smallest possible noise perturbation that causes misclassifications.
Table 4 Test accuracies showing the impact of adversarial training, clean refers to the original testing data, FGSM refers to adversary examples derived from Fast Gradient Sign Method and PGD refers to adversarial examples derived from Projected Gradient Descent [
83
]
Full size table
Another interesting framework that could be used in an adversarial training context is to have an adversary change the labels of training data. Xie et al. [
84
] presented DisturbLabel, a regularization technique that randomly replaces labels at each iteration. This is a rare example of adding noise to the loss layer, whereas most of the other augmentation methods discussed add noise into the input or hidden representation layers. On the MNIST dataset with LeNet [
28
] CNN architecture, DisturbLabel produced a 0.32% error rate compared to a baseline error rate of 0.39%. DisturbLabel combined with Dropout Regularization produced a 0.28% error rate compared to the 0.39% baseline. To translate this to the context of adversarial training, one network takes in the classifier’s training data as input and learns which labels to flip to maximize the error rate of the classification network.
The effectiveness of adversarial training in the form of noise or augmentation search is still a relatively new concept that has not been widely tested and understood. Adversarial search to add noise has been shown to improve performance on adversarial examples, but it is unclear if this is useful for the objective of reducing overfitting. Future work seeks to expand on the relationship between resistance to adversarial attacks and actual performance on test datasets.
GAN-based Data Augmentation
Another exciting strategy for Data Augmentation is generative modeling. Generative modeling refers to the practice of creating artificial instances from a dataset such that they retain similar characteristics to the original set. The principles of adversarial training discussed above have led to the very interesting and massively popular generative modeling framework known as GANs. Bowles et al. [
85
] describe GANs as a way to “unlock” additional information from a dataset. GANs are not the only generative modeling technique that exists; however they are dramatically leading the way in computation speed and quality of results.
Another useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders [
86
]. Variational auto-encoders learn a low-dimensional representation of data points. In the image domain, this translates an image tensor of size
height
×
width
×
color
channels down into a vector of size
n
× 1, identical to what was discussed with respect to feature space augmentation. Low-dimensional constraints in vector representations will result in a poorer representation, although these constraints are better for visualization using methods such as t-SNE [
87
]. Imagine a vector representation of size 5 × 1 created by an autoencoder. These autoencoders can take in a distribution of labeled data and map them into this space. These classes could include ‘head turned left’, ‘centered head’, and ‘head turned right’. The auto-encoder learns a low-dimensional representation of these data points such that vector operations such as adding and subtracting can be used to simulate a front view-3D rotation of a new instance. Variational auto-encoder outputs can be further improved by inputting them into GANs [
31
]. Additionally, a similar vector manipulation process can be done on the noise vector inputs to GANs through the use of Bidirectional GANs [
88
].
The impressive performance of GANs has resulted in increased attention on how they can be applied to the task of Data Augmentation. These networks have the ability to generate new training data that results in better performing classification models. The GAN architecture first proposed by Ian Goodfellow [
31
] is a framework for generative modeling through adversarial training. The best anecdote for understanding GANs is the analogy of a cop and a counterfeiter. The counterfeiter (generator network) takes in some form of input. This could be a random vector, another image, text, and many more. The counterfeiter learns to produce money such that the cop (discriminator network) cannot tell if the money is real or fake. The real or fake dichotomy is analogous to whether or not the generated instance is from the training set or if it was created by the generator network (Fig.
16
).
Fig. 16
Illustration of GAN concept provided by Mikolajczyk and Grochowski [
72
]
Full size image
The counterfeiter versus robber analogy is a seamless bridge to understand GANs in the context of network intrusion detection. Lin et al. [
89
] use a generator network to learn how to fool a black-box detection system. This highlights one of the most interesting characteristics of GANs. Analysis tools derived from game theory such as minimax strategy and the Nash Equilibrium [
90
] suggest that the generator will eventually fool the discriminator. The success of the generator to overcome the discriminator makes it very powerful for generative modeling. GANs are the most promising generative modeling technique for use in Data Augmentation.
The vanilla GAN architecture uses multilayer perceptron networks in the generator and discriminator networks. This is able to produce acceptable images on a simple image dataset such as the MNIST handwritten digits. However, it fails to produce quality results for higher resolution, more complicated datasets. In the MNIST dataset, each image is only 28 × 28 × 1 for a total of 784 pixels. GANs applied to the MNIST data are able to produce convincing results. However, MNIST images are far less challenging than other image datasets due to low intra-class variance and resolution, to name a couple differences of many. This is in heavy contrast with other datasets studied in most academic Computer Vision papers such as ImageNet or CIFAR-10. For immediate reference, an ImageNet image is of resolution 256 × 256 × 3, totaling 196,608 pixels, a 250× increase in pixel count compared with MNIST.
Many research papers have been published that modify the GAN framework through different network architectures, loss functions, evolutionary methods, and many more. This research has significantly improved the quality of samples created by GANs. There have been many new architectures proposed for expanding on the concept of GANs and producing higher resolution output images, many of which are out of the scope of this paper. Amongst these new architectures, DCGANs, Progressively Growing GANs, CycleGANs, and Conditional GANs seem to have the most application potential in Data Augmentation.
The DCGAN [
91
] architecture was proposed to expand on the internal complexity of the generator and discriminator networks. This architecture uses CNNs for the generator and discriminator networks rather than multilayer perceptrons. The DCGAN was tested to generate results on the LSUN interior bedroom image dataset, each image being 64 × 64 × 3, for a total of 12,288 pixels, (compared to 784 in MNIST). The idea behind DCGAN is to increase the complexity of the generator network to project the input into a high dimensional tensor and then add deconvolutional layers to go from the projected tensor to an output image. These deconvolutional layers will expand on the spatial dimensions, for example, going from 14 × 14 × 6 to 28 × 28 × 1, whereas a convolutional layer will decrease the spatial dimensions such as going from 14 × 14 × 32 to 7 × 7 × 64. The DCGAN architecture presents a strategy for using convolutional layers in the GAN framework to produce higher resolution images (Figs.
17
,
18
).
Fig. 17
DCGAN, generator architecture presented by Radford et al. [
91
]
Full size image
Fig. 18
Complete DCGAN architecture used by Frid-Adar et al. [
49
] to generate liver lesion images
Full size image
Frid-Adar et al. [
49
] tested the effectiveness of using DCGANs to generate liver lesion medical images. They use the architecture pictured above to generate 64 × 64 × 1 size images of liver lesion CT scans. Their original dataset contains 182 CT scans, (53 Cysts, 64 Metastases, and 65 Hemangiomas). After using classical augmentations to achieve 78.6% sensitivity and 88.4% specificity, they observed an increase to 85.7% sensitivity and 92.4% specificity once they added the DCGAN-generated samples.
Another architecture of interest is known as Progressively Growing GANs [
34
]. This architecture trains a series of networks with progressive resolution complexity. These resolutions range from 4 × 4 to 8 × 8 and so on until outputs of size 1024 × 1024 are achieved. This is built on the concept that GANs can accept images as input as well as random vectors. Therefore, the series of GANs work by passing samples from a lower resolution GAN up to higher-resolution GANs. This has produced very amazing results on facial images.
In addition to improving the resolution size of GANs, another interesting architecture that increases the quality of outputs is the CycleGAN [
92
] proposed by Zhu et al. CycleGAN introduces an additional Cycle-Consistency loss function to help stabilize GAN training. This is applied to image-to-image translation. Neural Style Transfer [
32
], discussed further in the section below, learns a single image to single image translation. However, CycleGAN learns to translate from a domain of images to another domain, such as horses to zebras. This is implemented via forward and backward consistency loss functions. A generator takes in images of horses and learns to map them to zebras such that the discriminator cannot tell if they were originally a part of the zebra set or not, as discussed above. After this, the generated zebras from horse images are passed through a network which translates them back into horses. A second discriminator determines if this re-translated image belongs to the horse set or not. Both of these discriminator losses are aggregated to form the cycle-consistency loss.
The use of CycleGANs was tested by Zhu et al. [
93
] in the task of Emotion Classification. Using the emotion recognition dataset, FER2013 [
94
], Facial Expression Recognition Database, they build a CNN classifier to recognize 7 different emotions: angry, disgust, fear, happy, sad, surprise, and neutral. These classes are imbalanced and the CycleGAN is used as a method of intelligent oversampling.
CycleGANs learned an unpaired image-to-image translation between domains. An example of the domains in this problem is neutral to disgust. The CycleGAN learns to translate an image representing a neutral image into an image representing the disgust emotion (Figs.
19
,
20
).
Fig. 19
Some examples of synthetic data created with CycleGANs for emotion classification
Full size image
Fig. 20
Architecture overview: G and F consist of two separate GANs composing the CycleGAN. Two images are taken from the reference and target class and used to generate new data in the target class
Full size image
Using CycleGANs to translate images from the other 7 classes into the minority classes was very effective in improving the performance of the CNN model on emotion recognition. Employing these techniquess, accuracy improved 5–10%. To further understand the effectiveness of adding GAN-generated instances, a t-SNE visualization is used. t-SNE [
87
] is a visualization technique that learns to map between high-dimensional vectors into a low-dimensional space to facilitate the visualization of decision boundaries (Fig.
21
).
Fig. 21
t-SNE visualization demonstrating the improved decision boundaries when using CycleGAN-generated samples.
a
original CNN model,
b
adding GAN-generated disgust images,
c
adding GAN-generated sad images,
d
adding both GAN-generated disgust and sad images [
93
]
Full size image
Another interesting GAN architecture for use in Data Augmentation is Conditional GANs [
95
]. Conditional GANs add a conditional vector to both the generator and the discriminator in order to alleviate problems with mode collapse. In addition to inputting a random vector
z
to the generator, Conditional GANs also input a
y
vector which could be something like a one-hot encoded class label, e.g. [0 0 0 1 0]. This class label targets a specific class for the generator and the discriminator (Fig.
22
).
Fig. 22
Illustration from Mirza and Osindero [
95
] showing how the conditional y vector is integrated into the GAN framework
Full size image
Lucic et al. [
96
] sought out to compare newly developed GAN loss functions. They conducted a series of tests that determined most loss functions can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that increased computational power is a more promising area of focus than algorithmic changes in the generator versus discriminator loss function.
Most of the research done in applying GANs to Data Augmentation and reporting the resulting classification performance has been done in biomedical image analysis [
39
]. These papers have shown improved classification boundaries derived from training with real and generated data from GAN models. In addition, some papers measure the quality of GAN outputs by a visual Turing test. In these tests, the study asks two experts to distinguish between real and artificial images in medical image tasks such as skin lesion classification and liver cancer detection. Table
5
shows that the first and second experts were only able to correctly label 62.5% and 58.6% of the GAN-generated liver lesion images as fake. Labeling images as fake refers to their origin coming from the generator rather than an actual liver lesion image (Table
6
; Fig.
23
).
Table 5 Results of ‘Visual Turing Test’ on DCGAN-generated liver lesion images presented by Frid-Adar et al. [
139
]
Full size table
Table 6 Results of ‘Visual Turing Test’ on different DCGAN- and WGAN [
104
]—generated brain tumor MR images presented by Han et al. [
140
]
Full size table
Fig. 23
Trends in applying GANs to medical image analysis [
39
]
Full size image
GAN samples can be used as an oversampling technique to solve problems with class imbalance. Lim et al. [
97
] show how GAN samples can be used for unsupervised anomaly detection. By oversampling rare normal samples, which are samples that occur with small probability, GANs are able to reduce the false positive rate of anomaly detection. They do this using the Adversarial Autoencoder framework proposed by Makhzani et al. [
98
] (Fig.
24
).
Fig. 24
Adversarial autoencoder framework used in DOPING [
97
]
Full size image
As exciting as the potential of GANs is, it is very difficult to get high-resolution outputs from the current cutting-edge architectures. Increasing the output size of the images produced by the generator will likely cause training instability and non-convergence. Another drawback of GANs is that they require a substantial amount of data to train. Thus, depending on how limited the initial dataset is, GANs may not be a practical solution. Salimans et al. [
99
] provide a more complete description of the problems with training GANs.
Neural Style Transfer
Neural Style Transfer [
32
] is one of the flashiest demonstrations of Deep Learning capabilities. The general idea is to manipulate the representations of images created in CNNs. Neural Style Transfer is probably best known for its artistic applications, but it also serves as a great tool for Data Augmentation. The algorithm works by manipulating the sequential representations across a CNN such that the style of one image can be transferred to another while preserving its original content. A more detailed explanation of the gram matrix operation powering Neural Style Transfer can be found by Li et al. [
100
] (Fig.
25
).
Fig. 25
Illustration of style and content reconstructions in Neural Style Transfer [
32
]
Full size image
It is important to also recognize an advancement of the original algorithm from Gatys et al. known as Fast Style Transfer [
35
]. This algorithm extends the loss function from a per-pixel loss to a perceptual loss and uses a feed-forward network to stylize images. This perceptual loss is reasoned about through the use of another pre-trained net. The use of perceptual loss over per-pixel loss has also shown great promise in the application of super-resolution [
101
] as well as style transfer. This loss function enhancement enables style transfer to run much faster, increasing interest in practical applications. Additionally, Ulyanov et al. [
102
] find that replacing batch normalization with instance normalization results in a significant improvement for fast stylization (Fig.
26
).
Fig. 26
Illustration of the Fast neural style algorithm by Johnson et al. [
35
]
Full size image
For the purpose of Data Augmentation, this is somewhat analogous to color space lighting transformations. Neural Style Transfer extends lighting variations and enables the encoding of different texture and artistic styles as well. This leaves practitioners of Data Augmentation with the decision of which styles to sample from when deriving new images via Neural Style Transfer.
Choosing which styles to sample from can be a challenging task. For applications such as self-driving cars it is fairly intuitive to think of transferring training data into a night-to-day scale, winter-to-summer, or rainy-to-sunny scale. However, in other application domains, the set of styles to transfer into is not so obvious. For ease of implementation, data augmentation via Neural Style Transfer could be done by selecting a set of
k
styles and applying them to all images in the training set. The work of Style Augmentation [
103
], avoids introducing a new form of style bias into the dataset by deriving styles at random from a distribution of 79,433 artistic images. Transferring style in training data has been tested on the transition from simulated environments to the real-world. This is very useful for robotic manipulation tasks using Reinforcement Learning because of potential damages to hardware when training in the real-world. Many constraints such as low-fidelity cameras cause these models to generalize poorly when trained in physics simulations and deployed in the real-world.
Tobin et al. [
104
] explore the effectiveness of using different styles in training simulation and achieve within 1.5 cm accuracy in the real-world on the task of object localization. Their experiments randomize the position and texture of the objects to be detected on the table in the simulation, as well as the texture, lighting, number of lights, and random noise in the background. They found that with enough variability in the training data style, the real-world simply appears as another variation to the model. Interestingly, they found that diversity in styles was more effective than simulating in as realistic of an environment as possible. This is in contrast to the work of Shrivastava et al. [
105
] who used GANs to make their simulated data as realistic as possible (Fig.
27
).
Fig. 27
Examples of different styles simulated by Tobin et al. [
104
]
Full size image
Using simulated data to build Computer Vision models has been heavily investigated. One example of this is from Richter et al. [
106
]. They use computer graphics from modern open-world games such as Grand Theft Auto to produce semantic segmentation datasets. The authors highlight anecdotes of the manual annotation costs required to build these pixel-level datasets. They mention the CamVid dataset [
107
] requires 60 min per image to manually annotate, and the Cityscapes dataset [
108
] requires 90 min per image. This high labor and time cost motivates the use and development of synthetic datasets. Neural Style Transfer is a very interesting strategy to improve the generalization ability of simulated datasets.
A disadvantage of Neural Style Transfer Data Augmentation is the effort required to select styles to transfer images into. If the style set is too small, further biases could be introduced into the dataset. Trying to replicate the experiments of Tobin et al. [
104
] will require a massive amount of additional memory and compute to transform and store 79,433 new images from each image. The original algorithm proposed by Gatys et al. [
32
] has a very slow running time and is therefore not practical for Data Augmentation. The algorithm developed by Johnson et al. [
35
] is much faster, but limits transfer to a pre-trained set of styles.
Meta learning Data Augmentations
The concept of meta-learning in Deep Learning research generally refers to the concept of optimizing neural networks with neural networks. This approach has become very popular since the publication of NAS [
33
] from Zoph and Le. Real et al. [
109
,
110
] also show the effectiveness of evolutionary algorithms for architecture search. Salimans et al. [
111
] directly compare evolutionary strategies with Reinforcement Learning. Another interesting alternative to Reinforcement Learning is simple random search [
112
]. Utilizing evolutionary and random search algorithms is an interesting area of future work, but the meta-learning schemes reviewed in this survey are all neural-network, gradient-based.
The history of Deep Learning advancement from feature engineering such as SIFT [
113
] and HOG [
114
] to architecture design such as AlexNet [
1
], VGGNet [
2
], and Inception-V3 [
4
], suggest that meta-architecture design is the next paradigm shift. NAS takes a novel approach to meta-learning architectures by using a recurrent network trained with Reinforcement Learning to design architectures that result in the best accuracy. On the CIFAR-10 dataset, this achieved an error rate of 3.65 (Fig.
28
).
Fig. 28
Concept behind Neural Architecture Search [
33
]
Full size image
This section will introduce three experiments using meta-learning for Data Augmentation. These methods use a prepended neural network to learn Data Augmentations via mixing images, Neural Style Transfer, and geometric transformations.
Neural augmentation
The Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang [
36
] presented an algorithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation. The Neural Augmentation approach takes in two random images from the same class. The prepended augmentation net maps them into a new image through a CNN with 5 layers, each with 16 channels, 3 × 3 filters, and ReLU activation functions. The image outputted from the augmentation is then transformed with another random image via Neural Style Transfer. This style transfer is carried out via the CycleGAN [
92
] extension of the GAN [
31
] framework. These images are then fed into a classification model and the error from the classification model is backpropagated to update the Neural Augmentation net. The Neural Augmentation network uses this error to learn the optimal weighting for content and style images between different images as well as the mapping between images in the CNN (Fig.
29
).
Fig. 29
Illustration of augmentation network [
36
]
Full size image
Perez and Wang tested their algorithm on the MNIST and Tiny-imagenet-200 datasets on binary classification tasks such as cat versus dog. The Tiny-imagenet-200 dataset is used to simulate limited data. The Tiny-imagenet-200 dataset contains only 500 images in each of the classes, with 100 set aside for validation. This problem limits this dataset to 2 classes. Thus there are only 800 images for training. Each of the Tiny-imagenet-200 images is 64 × 64 × 3, and the MNIST images are 28 × 28 × 1. The experiment compares their proposed Neural Augmentation [
36
] approach with traditional augmentation techniques such as cropping and rotation, as well as with a style transfer approach with a predetermined set of styles such as Night/Day and Winter/Summer.
The traditional baseline study transformed images by choosing an augmentation from a set (shifted, zoomed in/out, rotated, flipped, distorted, or shaded with a hue). This was repeated to increase the dataset size from N to 2 N. The GAN style transfer baseline uses 6 different styles to transform images (Cezanne, Enhance, Monet, Ukiyoe, Van Gogh and Winter). The Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Content loss, Style loss via gram matrix, and no loss computer at this layer). All experiments are tested with a convolutional network consisting of 3 convolutional layers each followed by max pooling and batch normalization, followed by 2 fully-connected layers. Each experiment runs for 40 epochs at a learning rate of 0.0001 with the Adam optimization technique (Table
7
).
Table 7 Results comparing augmentations [
36
]
Full size table
The results of the experiment are very promising. The Neural Augmentation technique performs significantly better on the Dogs versus Goldfish study and only slightly worse on Dogs versus Cats. The technique does not have any impact on the MNIST problem. The paper suggests that the likely best strategy would be to combine the traditional augmentations and the Neural Augmentations.
Smart Augmentation
The Smart Augmentation [
37
] approach utilizes a similar concept as the Neural Augmentation technique presented above. However, the combination of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm.
Smart Augmentation is another approach to meta-learning augmentations. This is done by having two networks,
Network
-
A
and
Network
-
B
.
Network
-
A
is an augmentation network that takes in two or more input images and maps them into a new image or images to train
Network
-
B
. The change in the error rate in
Network
-
B
is then backpropagated to update
Network
-
A
. Additionally another loss function is incorporated into
Network
-
A
to ensure that its outputs are similar to others within the class.
Network
-
A
uses a series of convolutional layers to produce the augmented image. The conceptual framework of
Network
-
A
can be expanded to use several Networks trained in parallel. Multiple
Network
-
As
could be very useful for learning class-specific augmentations via meta-learning (Fig.
30
).
Fig. 30
Illustration of the Smart Augmentation architecture [
37
]
Full size image
Smart Augmentation is similar to SamplePairing [
65
] or mixed-examples in the sense that a combination of existing examples produces new ones. However, the mechanism of Smart Augmentation is much more sophisticated, using an adaptive CNN to derive new images rather than averaging pixels or hand-engineered image combinations.
The Smart Augmentation technique was tested on the task of gender recognition. On the Feret dataset, accuracy improved from 83.52 to 88.46%. The audience dataset responded with an improvement of 70.02% to 76.06%. Most interestingly, results from another face dataset increased from 88.15 to 95.66%. This was compared with traditional augmentation techniques which increased the accuracy from 88.15 to 89.08%. Additionally, this experiment derived the same accuracy when using two
Network
-
As
in the augmentation framework as was found with one
Network
-
A
. This experiment demonstrates the significant performance increase with the Smart Augmentation meta-learning strategy (Fig.
31
).
Fig. 31
On the gender recognition task, the image to the left is an example of an instance produced by Network-A in Smart Augmentation given the right images as input [
37
]
Full size image
AutoAugment
AutoAugment [
38
], developed by Cubuk et al., is a much different approach to meta-learning than Neural Augmentation or Smart Augmentation. AutoAugment is a Reinforcement Learning algorithm [
115
] that searches for an optimal augmentation policy amongst a constrained set of geometric transformations with miscellaneous levels of distortions. For example, ‘translateX 20 pixels’ could be one of the transformations in the search space (Table
8
).
Table 8 AutoAugment augmentation policy found on the reduced CIFAR-10 dataset [
38
]
Full size table
In Reinforcement Learning algorithms, a policy is analogous to the strategy of the learning algorithm. This policy determines what actions to take at given states to achieve some goal. The AutoAugment approach learns a policy which consists of many sub-policies, each sub-policy consisting of an image transformation and a magnitude of transformation. Reinforcement Learning is thus used as a discrete search algorithm of augmentations. The authors also suggest that evolutionary algorithms or random search would be effective search algorithms as well.
AutoAugment found policies which achieved a 1.48% error rate on CIFAR-10. AutoAugment also achieved an 83.54% Top-1 accuracy on the ImageNet dataset. Very interestingly as well, the policies learned on the ImageNet dataset were successful when transferred to the Stanford Cars and FGVC Aircraft image recognition tasks. In this case, the ImageNet policy applied to these other datasets reduced error rates by 1.16% and 1.76% respectively.
Geng et al. [
116
] expanded on AutoAugment by replacing the Reinforcement Learning search algorithm with Augmented Random Search (ARS) [
112
]. The authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space. They convert the probability and magnitude of augmentations into a continuous space and search for sub-policies with ARS. With this, they achieve lower error rates on CIFAR-10, CIFAR-100, and ImageNet (Table
9
).
Table 9 The performance of ARS on continuous space vs. AutoAugment on discrete space [
116
]
Full size table
Minh et al. [
117
] also experimented with using Reinforcement Learning [
115
] to search for Data Augmentations. They further explore the effectiveness of learning transformations for individual instances rather than the entire dataset. They find classification accuracy differences of 70.18% versus 74.42% on the CIFAR-10 dataset and 74.61% versus 80.35% on the problem of classifying dogs versus cats. Further, they explore the robustness of classifiers with respect to test-time augmentation and find that the model trained with Reinforcement Learning augmentation search performs much better. On the CIFAR-10 dataset this results in 50.99% versus 70.06% accuracy when the models are evaluated on augmented test data.
A disadvantage to meta-learning is that it is a relatively new concept and has not been heavily tested. Additionally, meta-learning schemes can be difficult and time-consuming to implement. Practitioners of meta-learning will have to solve problems primarily with vanishing gradients [
118
], amongst others, to train these networks.
Comparing Augmentations
As shown throughout “
Design considerations for image Data Augmentation
” section, possibilities for Data Augmentation. However, there are not many comparative studies that show the performance differences of these different augmentations. One such study was conducted by Shijie et al. [
119
] which compared GANs, WGANs, flipping, cropping, shifting, PCA jittering, color jittering, adding noise, rotation, and some combinations on the CIFAR-10 and ImageNet datasets. Additionally, the comparative study ranged across dataset sizes with the small set consisting of 2 k samples with 200 in each class, tthe medium set consisting of 10 k samples with 1 k in each class, and the large set consisting of 50 k samples with 5 k in each class. They also tested with 3 levels of augmentation, no augmentation, original plus same size of generated samples, and original plus double size of generated samples. They found that cropping, flipping, WGAN, and rotation generally performed better than others. The combinations of flipping + cropping and flipping + WGAN were the best overall, improving classification performance on CIFAR-10 by + 3% and + 3.5%, respectively.
Design considerations for image Data Augmentation
This section will briefly describe some additional design decisions with respect to Data Augmentation techniques on image data.
Test-time augmentation
In addition to augmenting training data, many research reports have shown the effectiveness of augmenting data at test-time as well. This can be seen as analogous to ensemble learning techniques in the data space. By taking a test image and augmenting it in the same way as the training images, a more robust prediction can be derived. This comes at a computational cost depending on the augmentations performed, and it can restrict the speed of the model. This could be a very costly bottleneck in models that require real-time prediction. However, test-time augmentation is a promising practice for applications such as medical image diagnosis. Radosavovic et al. [
120
] denote test-time augmentation as data distillation to describe the use of ensembled predictions to get a better representation of the image.
Wang et al. [
121
] sought out to develop a mathematical framework to formulate test-time augmentation. Testing their test-time augmentation scheme on medical image segmentation, they found that it outperformed the single-prediction baseline and dropout-based multiple predictions. They also found better uncertainty estimation when using test-time augmentation, reducing highly confident but incorrect predictions. Their test-time augmentation method uses a Monte Carlo simulation in order to obtain parameters for different augmentations such as flipping, scaling, rotation, and translations, as well as noise injections.
Test-time augmentation can be found in the Alexnet paper [
1
], which applies CNNs to the ImageNet dataset. In their experiments, they average the predictions on ten randomly cropped patches. These patches consist of one extracted from the center, four corner croppings, and the equivalent regions on the horizontally flipped images. These predictions are averaged to form the final output. He et al. [
3
] use the same 10-crop testing procedure to evaluate their ResNet CNN architecture (Fig.
32
).
Fig. 32
Impact of test-time data augmentation for skin lesion classification [
122
]
Full size image
Perez et al. [
122
] present a study on the effectiveness of test-time augmentation with many augmentation techniques. These augmentations tested include color augmentation, rotation, shearing, scaling, flipping, random cropping, random erasing, elastic, mixing, and combinations between the techniques. Table
9
shows the higher performance achieved when augmenting test images as well as training images. Matsunaga et al. [
123
] also demonstrate the effectiveness of test-time augmentation on skin lesion classification, using geometric transformations such as rotation, translation, scaling, and flipping.
The impact of test-time augmentation on classification accuracy is another mechanism for measuring the robustness of a classifier. A robust classifier is thus defined as having a low variance in predictions across augmentations. For example, a prediction of an image should not be much different when that same image is rotated 20°. In their experiments searching for augmentations with Reinforcement Learning, Minh et al. [
117
] measure robustness by distorting test images with a 50% probability and contrasting the accuracy on un-augmented data with the augmented data. In this study, the performance of the baseline model decreases from 74.61 to 66.87% when evaluated on augmented test images.
Some classification models lie on the fence in terms of their necessity for speed. This suggests promise in developing methods that incrementally upgrade the confidence of prediction. This could be done by first outputting a prediction with little or no test-time augmentation and then incrementally adding test-time augmentations to increase the confidence of the prediction. Different Computer Vision tasks require certain constraints on the test-time augmentations that can be used. For example, image recognition can easily aggregate predictions across warped images. However, it is difficult to aggregate predictions on geometrically transformed images in object detection and semantic segmentation.
Curriculum learning
Aside from the study of Data Augmentation, many researchers have been interested in trying to find a strategy for selecting training data that beats random selection. In the context of Data Augmentation, research has been published investigating the relationship between original and augmented data across training epochs. Some research suggests that it is best to initially train with the original data only and then finish training with the original and augmented data, although there is no clear consensus.
In the SamplePairing [
65
] study, one epoch on ImageNet and 100 epochs on other datasets are completed without SamplePairing before mixed image data is added to the training. Once the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without. Jaderberg et al. [
124
] train exclusively with synthetic data for natural scene text recognition. The synthetic data produced the training data by enumerating through different fonts and augmentations. This produced sets of training images for size 50 k and 90 k lexicons. Mikolajczyk and Grochowski [
72
] draw comparisons from transfer learning. They suggest that training on augmented data to learn the initial weights of a deep convolutional network is similar to transferring weights trained on other datasets such as ImageNet. These weights are then fine-tuned only with the original training data.
Curriculum learning decisions are especially important for One-Shot Learning systems such as FaceNet, presented by Schroff et al. [
125
]. It is important to find faces which are somewhat similar to the new face such that the learned distance function is actually useful. In this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples.
Curriculum learning, a term originally coined by Bengio et al. [
126
], is an applicable concept for all Deep Learning models, not just those constrained with limited data. Plotting out training accuracy over time across different initial training subsets could help reveal patterns in the data that dramatically speed up training time. Data Augmentation constructs massively inflated training from combinations such as flipping, translating, and randomly erasing. It is highly likely that a subset exists in this set such that training will be faster and more accurate.
Resolution impact
Another interesting discussion about Data Augmentation in images is the impact of resolution. Higher resolution images such as HD (1920 × 1080 × 3) or 4 K (3840 × 2160 × 3) require much more processing and memory to train deep CNNs. However, it seems intuitive that next-generation models would be trained on higher resolution images. Many current models downsample images from their original resolution to make the classification problem computationally more feasible. However, sometimes this downsampling causes information loss within the image, making image recognition more difficult (Table
10
).
Table 10 Comparison of resolution across three very popular open-source image datasets
Full size table
It is interesting to investigate the nature of this downsampling and resulting performance comparison. Wu et al. [
127
] compare the tradeoff between accuracy and speed when downsampling images to different resolutions. The researchers found that composing an ensemble of models trained with high and low-resolution images performed better than any one model individually. This ensemble prediction is found by averaging the softmax predictions. The models trained on 256 × 256 images and 512 × 512 images achieve 7.96% and 7.42% top-5 error rates, respectively. When aggregated they achieved a lower top-5 error rate of 6.97%. Therefore, different downsampled images can be viewed as another Data Augmentation scheme (Fig.
33
).
Fig. 33
Classifications of the Image to the right by different resolution models trained by Wu et al. [
127
]
Full size image
With the advance of Super-Resolution Convolutional Neural Networks presented by Chong et al. [
128
] or SRGANs, Super-Resolution Generative Adversarial Networks, presented by Ledig et al. [
129
], it is interesting to consider if upsampling images to an even higher resolution would result in better models. Quality upsampling on CIFAR-10 images from even 32 × 32 × 3 to 64 × 64 × 3 could lead to better and more robust image classifiers.
Resolution is also a very important topic with GANs. Producing high resolution outputs from GANs is very difficult due to issues with training stability and mode collapse. Many of the newer GAN architectures such as StackGAN [
130
] and Progressively-Growing GANs [
34
] are designed to produce higher resolution images. In addition to these architectures, the use of super-resolution networks such as SRGAN could be an effective technique for improving the quality of outputs from a DCGAN [
91
] model. Once it is practical to produce high resolution outputs from GAN samples, these outputs will be very useful for Data Augmentation.
Final dataset size
A necessary component of Data Augmentation is the determination of the final dataset size. For example, if all images are horizontally flipped and added to the dataset, the resulting dataset size changes from N to 2N. One of the main considerations with respect to final dataset size is the additional memory and compute constraints associated with augmenting data. Practitioners have the choice between using generators which transform data on the fly during training or transforming the data beforehand and storing it in memory. Transforming data on the fly can save memory, but will result in slower training. Storing datasets in memory can be extremely problematic depending on how heavily the dataset size has been inflated. Storing augmented datasets in memory is especially problematic when augmenting big data. This decision is generally categorized as online or offline data augmentation, (with online augmentation referring to on the fly augmentations and offline augmentation referring to editing and storing data on the disk).
In the design of a massively distributed training system, Chilimbi et al. [
131
] augment images before training to speed up image serving. By augmenting images in advance, the distributed system is able to request and pre-cache training batches. Augmentations can also be built into the computational graph used to construct Deep Learning models and facilitate fast differentiation. These augmentations process images immediately after the input image tensor.
Additionally, it is also interesting to explore a subset of the inflated data that will result in higher or similar performance to the entire training set. This is a similar concept to curriculum learning, since the central idea is to find an optimal ordering of training data. This idea is also very related to final dataset size and the considerations of transformation compute and available memory for storing augmented images.
Alleviating class imbalance with Data Augmentation
Class imbalance is a common problem in which a dataset is primarily composed of examples from one class. This could manifest itself in a binary classification problem such that there is a clear majority-minority class distinction, or in multi-class classification in which there is one or multiple majority classes and one or multiple minority classes. Imbalanced datasets are harmful because they bias models towards majority class predictions. Imbalanced datasets also render accuracy as a deceitful performance metric. Buda et al. [
132
] provide a systematic study specifically investigating the impact of imbalanced data in CNNs processing image data. Leevy et al. [
27
] cover many Data-level and Algorithm-level solutions to class imbalance in big data in general. Data Augmentation falls under a Data-level solution to class imbalance and there are many different strategies for implementation.
A naive solution to oversampling with Data Augmentation would be a simple random oversampling with small geometric transformations such as a 30° rotation. Other simple image manipulations such as color augmentations, mixing images, kernel filters, and random erasing can also be extended to oversample data in the same manner as geometric augmentations. This can be useful for ease of implementation and quick experimentation with different class ratios. One problem of oversampling with basic image transformations is that it could cause overfitting on the minority class which is being oversampled. The biases present in the minority class are more prevalent post-sampling with these techniques.
Oversampling methods based on Deep Learning such as adversarial training, Neural Style Transfer, GANs, and meta-learning schemes can also be used as a more intelligent oversampling strategy. Neural Style Transfer is an interesting way to create new images. These new images can be created either through extrapolating style with a foreign style or by interpolating styles amongst instances within the dataset. Using GANs to oversample data could be another effective way to increase the minority class size while preserving the extrinsic distribution. Oversampling with GANs can be done using the entire minority class as “real” examples, or by using subsets of the minority class as inputs to GANs. The use of evolutionary sampling [
133
] to find these subsets to input to GANs for class sampling is a promising area for future work.
Discussion
The interesting ways to augment image data fall into two general categories: data warping and oversampling. Many of these augmentations elucidate how an image classifier can be improved, while others do not. It is easy to explain the benefit of horizontal flipping or random cropping. However, it is not clear why mixing pixels or entire images together such as in PatchShuffle regularization or SamplePairing is so effective. Additionally, it is difficult to interpret the representations learned by neural networks for GAN-based augmentation, variational auto-encoders, and meta-learning. CNN visualization has been led by Yosinski et al. [
134
] with their deep visualization method. Having a human-level understanding of convolutional networks features could greatly help guide the augmentation process.
Manipulating the representation power of neural networks is being used in many interesting ways to further the advancement of augmentation techniques. Traditional hand-crafted augmentation techniques such as cropping, flipping, and altering the color space are being extended with the use of GANs, Neural Style Transfer, and meta-learning search algorithms.
Image-to-image translation has many potential uses in Data Augmentation. Neural Style Transfer uses neural layers to translate images into new styles. This technique not only utilizes neural representations to separate ‘style’ and ‘content’ from images, but also uses neural transformations to transfer the style of one image into another. Neural Style Transfer is a much more powerful augmentation technique than traditional color space augmentations, but even these methods can be combined together.
An interesting characteristic of these augmentation methods is their ability to be combined together. For example, the random erasing technique can be stacked on top of any of these augmentation methods. The GAN framework possesses an intrinsic property of recursion which is very interesting. Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation or Neural Augmentation to create even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset. The extensibility of the GAN framework is amongst many reasons they are so interesting to Deep Learning researchers.
Test-time augmentation is analogous to ensemble learning in the data space. Instead of aggregating the predictions of different learning algorithms, we aggregate predictions across augmented images. We can even extend the solution algorithm to parameterize prediction weights from different augmentations. This seems like a good solution for systems concerned with achieving very high performance scores, more so than prediction speed. Determining the effectiveness of test-time augmentation by primarily exploring test-time geometric transformations and Neural Style Transfer, is an area of future work.
An interesting question for practical Data Augmentation is how to determine post-augmented dataset size. There is no consensus as to which ratio of original to final dataset size will result in the best performing model. However, imagine using color augmentations exclusively. If the initial training dataset consists of 50 dogs and 50 cats, and each image is augmented with 100 color filters to produce 5000 dogs and 5000 cats, this dataset will be heavily biased towards the spatial characteristics of the original 50 dogs and 50 cats. This over-extensive color-augmented data will cause a deep model to overfit even worse than the original. From this anecdote, we can conceptualize the existence of an optimal size for post-augmented data.
Additionally, there is no consensus about the best strategy for combining data warping and oversampling techniques. One important consideration is the intrinsic bias in the initial, limited dataset. There are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data. All these augmentation algorithms perform best under the assumption that the training data and testing data are both drawn from the same distribution. If this is not true, it is very unlikely that these methods will be useful.
Future work
Future work in Data Augmentation will be focused on many different areas such as establishing a taxonomy of augmentation techniques, improving the quality of GAN samples, learning new ways to combine meta-learning and Data Augmentation, discovering relationships between Data Augmentation and classifier architecture, and extending these principles to other data types. We are interested in seeing how the time-series component in video data impacts the use of static image augmentation techniques. Data Augmentation is not limited to the image domain and can be useful for text, bioinformatics, tabular records, and many more.
Our future work intends to explore performance benchmarks across geometric and color space augmentations across several datasets from different image recognition tasks. These datasets will be constrained in size to test the effectiveness with respect to limited data problems. Zhang et al. [
135
] test their novel GAN augmentation technique on the SVHN dataset across 50, 80, 100, 200, and 500 training instances. Similar to this work, we will look to further establish benchmarks for different levels of limited data.
Improving the quality of GAN samples and testing their effectiveness on a wide range of datasets is another very important area for future work. We would like to further explore the combinatorics of GAN samples with other augmentation techniques such as applying a range of style transfers to GAN-generated samples.
Super-resolution networks through the use of SRCNNs, Super-Resolution Convolutional Neural Networks, and SRGANs are also very interesting areas for future work in Data Augmentation. We want to explore the performance differences across architectures with upsampled images such as expanding CIFAR-10 images from 32 × 32 to 64 × 64 to 128 × 128 and so on. One of the primary difficulties with GAN samples is trying to achieve high-resolution outputs. Therefore, it will be interesting to see how we can use super-resolution networks to achieve high-resolution such as DCGAN samples inputted into an SRCNN or SRGAN. The result of this strategy will be compared with the performance of the Progressively Growing GAN architecture.
Test-time augmentation has the potential to make a massive difference in Computer Vision performance and has not been heavily explored. We want to establish benchmarks for different ensembles of test-time augmentations and investigate the solution algorithms used. Currently, majority voting seems to be the dominant solution algorithm for test-time augmentation. It seems highly likely that test-time augmentation can be further improved if the weight of each augmented images prediction is further parameterized and learned. Additionally, we will explore the effectiveness of test-time augmentation on object detection, comparing color space augmentations and the Neural Style Transfer algorithm.
Meta-learning GAN architectures is another exciting area of interest. Using Reinforcement Learning algorithms such as NAS on the generator and discriminator architectures seem very promising. Another interesting area of further research is to use an evolutionary approach to speed up the training of GANs through parallelization and cluster computing.
Another important area of future work for practical integration of Data Augmentation into Deep Learning workflows is the development of software tools. Similar to how the Tensorflow [
136
] system automates the back-end processes of gradient-descent learning, Data Augmentation libraries will automate preprocessing functions. The Keras [
137
] library provides an ImageDataGenerator class that greatly facilitates the implementation of geometric augmentations. Buslaev et al. presented another augmentation tool they called Albumentations [
138
]. The development of Neural Style Transfer, adversarial training, GANs, and meta-learning APIs will help engineers utilize the performance power of advanced Data Augmentation techniques much faster and more easily.
Conclusion
This survey presents a series of Data Augmentation solutions to the problem of overfitting in Deep Learning models due to limited data. Deep Learning models rely on big data to avoid overfitting. Artificially inflating datasets using the methods discussed in this survey achieves the benefit of big data in the limited data domain. Data Augmentation is a very useful technique for constructing better datasets. Many augmentations have been proposed which can generally be classified as either a data warping or oversampling technique.
The future of Data Augmentation is very bright. The use of search algorithms combining data warping and oversampling methods has enormous potential. The layered architecture of deep neural networks presents many opportunities for Data Augmentation. Most of the augmentations surveyed operate in the input layer. However, some are derived from hidden layer representations, and one method, DisturbLabel [
28
], is even manifested in the output layer. The space of intermediate representations and the label space are under-explored areas of Data Augmentation with interesting results. This survey focuses on applications for image data, although many of these techniques and concepts can be expanded to other data domains.
Data Augmentation cannot overcome all biases present in a small dataset. For example, in a dog breed classification task, if there are only bulldogs and no instances of golden retrievers, no augmentation method discussed, from SamplePairing to AutoAugment to GANs, will create a golden retriever. However, several forms of biases such as lighting, occlusion, scale, background, and many more are preventable or at least dramatically lessened with Data Augmentation. Overfitting is generally not as much of an issue with access to big data. Data Augmentation prevents overfitting by modifying limited datasets to possess the characteristics of big data.
Availability of data and materials
Not applicable.
Abbreviations
GAN:
generative adversarial network
CNN:
convolutional neural network
DCGAN:
deep convolutional generative adversarial network
NAS:
neural architecture search
SRCNN:
super-resolution convolutional neural network
SRGAN:
super-resolution generative adversarial network
CT:
computerized tomography
MRI:
magnetic resonance imaging
PET:
positron emission tomography
ROS:
random oversampling
SMOTE:
synthetic minority oversampling technique
RGB:
red-green–blue
PCA:
principal components analysis
UCI:
University of California Irvine
MNIST:
Modified National Institute of Standards and Technology
CIFAR:
Canadian Institute for Advanced Research
t-SNE:
t-distributed stochastic neighbor embedding
References
Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. Adv Neural Inf Process Syst. 2012;25:1106–14.
Google Scholar
Karen S, Andrew Z. Very deep convolutional networks for large-scale image recognition. arXiv e-prints. 2014.
Kaiming H, Xiangyu Z, Shaoqing R, Jian S. Deep residual learning for image recognition. In: CVPR, 2016.
Christian S, Vincent V, Sergey I, Jon S, Zbigniew W. Rethinking the inception architecture for computer vision. arXiv e-prints, 2015.
Gao H, Zhuang L, Laurens M, Kilian QW. Densely connected convolutional networks. arXiv preprint. 2016.
Jan K, Vladimir G, Daniel C. Regularization for deep learning: a taxonomy. arXiv preprint. 2017.
Nitish S, Geoffrey H, Alex K, Ilya S, Ruslan S. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929–58.
MathSciNet
MATH
Google Scholar
Jonathan T, Ross G, Arjun J, Yann L, Christoph B. Efficient object localization using convolutional networks. In: CVPR’15. 2015.
Sergey I, Christan S. Batch normalization: accelerating deep network training by reducing internal covariate shift. In: ICML; 2015.
Karl W, Taghi MK, DingDing W. A survey of transfer learning. J Big Data. 2016;3:9.
Article
Google Scholar
Shao L. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2015;26(5):1019–34.
Article
MathSciNet
Google Scholar
Jia D, Wei D, Richard S, Li-Jia L, Kai L, Li F-F. ImageNet: a large-scale hierarchical image database. In: CVPR09, 2009.
Amir Z, Alexander S, William S, Leonidas G, Jitendra M, Silvio S. Taskonomy: disentangling task transfer learning. In: CVPR ‘18. 2018.
Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? Adv Neural Inf Process Syst. 2014;27:3320–8.
Google Scholar
Erhan D, Bengio Y, Courville A, Manzagol PA, Vincent P. Why does unsupervised pre-training help deep learning? J Mach Learn Res. 2010;11:625–60.
MathSciNet
MATH
Google Scholar
Mark P, Dean P, Geoffrey H, Tom MM. Zero-shot learning with semantic output codes. In: NIPS; 2009.
Yongqin X, Christoph HL, Bernt S, Zeynep A. Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly. arXiv preprint. 2018.
Yaniv T, Ming Y, Marc’ AR, Lior W. DeepFace: closing the gap to human-level performance in face verification. In: CVPR ’14; 2014.
Gregory K, Richard Z, Ruslan S. Siamese neural networks for one-shot image recognition. In: ICML Deep Learning workshop; 2015.
Adam S, Sergey B, Matthew B, Dean W, Timothy L. One-shot learning with memory-augmented neural networks. arXiv preprint. 2016.
Tomas M, Ilya S, Kai C, Greg C, Jeffrey D. Distributed representations of words and phrases and their compositionality. Accepted to NIPS 2013.
Jeffrey P, Richard S, Christopher DM. GloVe: global vectors for word representation. In: Proceedings of the empirical methods in natural language processing (EMNLP 2014) 12. 2014.
Halevy A, Norvig P, Pereira F. The unreasonable effectiveness of data. IEEE Intell Syst. 2009;24:8–12.
Article
Google Scholar
Chen S, Abhinav S, Saurabh S, Abhinav G. Revisting unreasonable effectivness of data in deep learning era. In: ICCV; 2017. p. 843–52.
Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer with deep neural networks. Nature. 2017;542:115–8.
Article
Google Scholar
Geert L, Thijs K, Babak EB, Arnaud AAS, Francesco C, Mohsen G, Jeroen AWM, van Bram G, Clara IS. A survey on deep learning in medical image analysis. Med Image Anal. 2017;42:60–88.
Article
Google Scholar
Joffrey LL, Taghi MK, Richard AB, Naeem S. A survey on addressing high-class imbalance in big data. Springer J Big Data. 2018;5:42.
Article
Google Scholar
LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proc IEEE. 1998;86(11):2278–324.
Article
Google Scholar
Nitesh VC, Kevin WB, Lawrence OH, Kegelmeyer W. SMOTE: synthetic minority over-sampling technique. J Artif Intellig Res. 2002;16:321–57.
Article
Google Scholar
Hui H, Wen-Yuan W, Bing-Huan M. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. In: Proceedings of ICIC, vol. 3644, Lecture Notes in Computer Science, New York. 2005, p. 878–87.
Ian JG, Jean PA, Mehdi M, Bing X, David WF, Sherjil O, Aaron C, Yoshua B. Generative adversarial nets. NIPS. 2014.
Leon AG, Alexander SE, Matthias B. A neural algorithm of artistic style. ArXiv. 2015.
Barret Z, Quoc VL. Neural architecture search with reinforcement learning. In: International conference on learning representatoins, 2017.
Tero K, Timo A, Samuli L, Jaakko L. Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017.
Justin J, Alexandre A, Li FF. Perceptual losses for real-time style transfer and super-resolution. ECCV. 2016;2016:694–711.
Google Scholar
Luis P, Jason W. The effectiveness of data augmentation in image classification using deep learning. In: Stanford University research report, 2017.
Lemley J, Barzrafkan S, Corcoran P. Smart augmentation learning an optimal data augmentation strategy. In: IEEE Access. 2017.
Ekin DC, Barret Z, Dandelion M, Vijay V, Quoc VL. AutoAugment: learning augmentation policies from data. ArXiv preprint. 2018.
Xin Y, Paul SB, Ekta W. Generative adversarial network in medical imaging: a review. arXiv preprint. 2018.
Jelmer MW, Tim L, Max AV, Ivana I. Generative adversarial networks for noise reduction in low-dose CT. In: IEEE Transactions on Medical Imaging. 2017.
Ohad S, Tammy RR. Accelerated magnetic resonance imaging by adversarial neural network. In: DLMIA/ML-CDS@MICCAI, 2017.
Wang Y, Biting Y, Wang L, Chen Z, Lalush DS, Lin W, Xi W, Zhou J, Shen D, Zhou L. 3D conditional generative adversarial networks for high-quality PET image estimation at low dose. NeuroImage. 2018;174:550–62.
Article
Google Scholar
Dwarikanath M, Behzad B. Retinal vasculature segmentation using local saliency maps and generative adversarial networks for image super resolution. arXiv preprint. 2017.
Francesco C, Aldo M, Claudio S, Giorgio T. Biomedical data augmentation using generative adversarial neural networks. In: International conference on artificial neural networks. Berlin: Springer; 2017. P. 626–34.
Camilo B, Andrew JP, Larry TD, Allen TN, Susan MR, Bennett AL. Learning implicit brain MRI manifolds with deep learning. Int Soc Opt Photonics. 2018;10574:105741.
Google Scholar
Maria JMC, Sarfaraz H, Jeremy B, Ulas B. How to fool radiologists with generative adversarial networks? A visual turing test for lung cancer diagnosis. arXiv preprint. 2017.
Baur C, Albarqouni S, Navab N. MelanoGANs: high resolution skin lesion synthesis with GANs. arXiv preprint. 2018.
Madani A, Moradi M, Karargyris A, Syeda-Mahmood T. Chest x-ray generation and data augmentation for cardiovascular abnormality classification. In: Medical imaging 2018. Image Processing 2018;10574:105741.
Maayan F-A, Eyal K, Jacob G, Hayit G. GAN-based data augmentation for improved liver lesion classification. arXiv preprint. 2018.
Joseph R, Santosh D, Ross G, Ali F. You only look once: unified, real-time object detection. In: CVPR‘16. 2016.
Ross G, Jeff D, Trevor D, Jitendra M. Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR ‘14. 2014.
Ross G. Fast R-CNN. CoRR, abs/1504.08083. 2015.
Shaoqing R, Kaiming H, Ross G, Jian S. Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS, 2015.
Jonathan L, Evan S, Trevor D. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038. 2014.
Olaf R, Philipp F, Thomas B. U-Net: convolutional networks for biomedical image segmentation. In: MICCAI. Springer; 2015, p. 234–41.
Hessam B, Maxwell H, Mohammad R, Ali F. Label refinery: improving imagenet classification through label progression. arXiv preprint. 2018.
Francisco JM-B, Fiammetta S, Jose MJ, Daniel U, Leonardo F. Forward noise adjustment scheme for data augmentation. arXiv preprints. 2018.
Dua D, Karra TE. UCI machine learning repository [
http://archive.ics.uci.edu/ml
]. Irvine, CA: University of California, School of Information and Computer Science; 2017.
Ken C, Karen S, Andrea V, Andrew Z. Return of the devil in the details: delving deep into convolutional nets. In: Proceedings of BMVC. 2014.
Mark E, Luc VG, Christopher KIW, John W, Andrew Z. The pascal visual object classes (VOC) challenge.
http://www.pascal-network.org/challenges/VOC/voc2008/workshop/
. 2008.
Aranzazu J, Miguel P, Mikel G, Carlos L-M, Daniel P. A comparison study of different color spaces in clustering based image segmentation. IPMU; 2010.
Quanzeng Y, Jiebo L, Hailin J, Jianchao Y. Robust image sentiment analysis using progressively trained and domain transferred deep networks. In: AAAI. 2015, p. 381–8.
Luke T, Geoff N. Improving deep learning using generic data augmentation. arXiv preprint. 2017.
Guoliang K, Xuanyi D, Liang Z, Yi Y. PatchShuffle regularization. arXiv preprint. 2017.
Hiroshi I. Data augmentation by pairing samples for images classification. ArXiv e-prints. 2018.
Cecilia S, Michael JD. Improved mixed-example data augmentation. ArXiv preprint. 2018.
Daojun L, Feng Y, Tian Z, Peter Y. Understanding mixup training methods. In: IEEE access. 2018. p. 1.
Ryo T, Takashi M. Data augmentation using random image cropping and patches for deep CNNs. arXiv preprints. 2018.
Yoshua B, Jerome L, Ronan C, Jason W. Curriculum learning. In: Proceedings of the 26th annual international conference on machine learning, ACM. 2009, p. 41–8.
Zhun Z, Liang Z, Guoliang K, Shaozi L, Yi Y. Random erasing data augmentation. ArXiv e-prints. 2017.
Terrance V, Graham WT. Improved regularization of convolutional neural networks with cutout. arXiv preprint. 2017.
Agnieszka M, Michal G. Data augmentation for improving deep learning in image classification problem. In: IEEE 2018 international interdisciplinary Ph.D. Workshop, 2018.
Jonathan K, Michael S, Jia D, Li F-F. 3D object representations for fine-grained categorization. In: 4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013.
Tomohiko K, Michiaki I. Icing on the cake: an easy and quick post-learning method you can try after deep learning. arXiv preprints. 2018.
Terrance V, Graham WT. Dataset augmentation in feature space. In: Proceedings of the international conference on machine learning (ICML), workshop track, 2017.
Sebastien CW, Adam G, Victor S, Mark DM. Understanding data augmentation for classification: when to warp? CoRR, abs/1609.08764, 2016.
Seyed-Mohsen MD, Alhussein F, Pascal F. DeepFool: a simple and accurate method to fool deep neural networks. arXiv preprint. 2016.
Jiawei S, Danilo VV, Sakurai K. One pixel attack for fooling deep neural networoks. arXiv preprints. 2018.
Michal Z, Konrad Z, Negar R, Pedro OP. Adversarial framing for image and video classification. arXiv preprints. 2018.
Logan E, Brandon T, Dimitris T, Ludwig S, Aleksander M. A rotation and a translation suffice: fooling CNNs with simple transformations. ArXiv preprint. 2018.
Goodfellow I, Shlens J, Szegedy C. Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations, 2015.
Ian JG, David W-F, Mehdi M, Aaron C, Yoshua B. Maxout networks. arXiv preprint. 2013.
Shuangtao L, Yuanke C, Yanlin P, Lin B. Learning more robust features with adversarial training. ArXiv preprints. 2018.
Lingxi X, Jingdong W, Zhen W, Meng W, Qi T. DisturbLabel: regularizing CNN on the loss layer. arXiv preprint. 2016.
Christopher B, Liang C, Ricardo GPB, Roger G, Alexander H, David AD, Maria VH, Joanna W, Daniel R. GAN augmentation: augmenting training data using generative adversarial networks. arXiv preprint. 2018.
Doersch C. Tutorial on Variational Autoencoders. ArXiv e-prints. 2016.
Laurens M, Geoffrey H. Visualizing data using t-SNE. J Mach Learn Res. 2008;9:2431–56.
MathSciNet
MATH
Google Scholar
Jeff D, Philipp K, Trevor D. Adversarial feature learning. In: CVPR’16. 2016.
Lin Z, Shi Y, Xue Z. IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection. arXiv preprint; 2018.
William F, Mihaela R, Balaji L, Andrew MD, Shakir M, Ian G. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In: International conference on learning representations (ICLR); 2017.
Alec R, Luke M, Soumith C. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016.
Jun-Yan Z, Taesung P, Phillip I, Alexei AE. Unpaired image-to-image translation using cycle-consistent adversarial networks. In: International conference on cmoputer vision (ICCV), 2017.
Xinyue Z, Yifan L, Zengchang Q, Jiahong L. Emotion classification with data augmentation using generative adversarial networks. CoRR, vol. abs/1711.00648. 2017.
Goodfellow IJ, Erhan D, Carrier PL, Courville A, Mirza M, Hamner B, Cukierski W, Tang Y, Thaler D, Lee DH, et al. Challenges in representation learning: A report on three machine learning contests. In: NIPS. Berlin: Springer; 2013. p.117–24.
Mehdi M, Simon O. Conditional generative adversarial nets. arXiv preprint. 2014.
Mario L, Karol K, Marcin M, Olivier B, Sylvain G. Are GANs created equal? A large-scale study. arXiv preprint. 2018.
Swee KL, Yi L, Ngoc-Trung T, Ngai-Man C, Gemma R, Yuval E. DOPING: generative data augmentation for unsupervised anomaly detection with GAN. arXiv preprint. 2018.
Alireza M, Jonathon S, Navdeep J, Ian G, Brendan F. Adversarial autoencoders. arXiv preprint. 2015.
Tim S, Ian G, Wojciech Z, Vicki C, Alec R, Xi C. Improved techniques for training GANs. arXiv preprint. 2016.
Yanghao L, Naiyan W, Jiaying L, Xiaodi H. Demistifying neural style transfer. arXiv preprint. 2017.
Khizar H. Super-resolution via deep learning. arXiv preprint. 2017.
Dmitry U, Andrea V, Victor L. Instance normalization: the missing ingredient for fast stylization. arXiv preprint. 2016.
Philip TJ, Amir AA, Stephen B, Toby B, Boguslaw O. Style augmentation: data augmentation via style randomization. arXiv e-prints. 2018.
Josh T, Rachel F, Alex R, Jonas S, Wojciech Z, Pieter A. Domain randomization for transferring deep neural networks from simulation to the real world. arXiv preprint. 2017.
Ashish S, Tomas P, Oncel T, Josh S, Wenda W, Russ W. Learning from simulated and unsupervised images through adversarial training. In: Conference on computer vision and pattern recognition, 2017.
Stephan RR, Vibhav V, Stefan R, Vladlen K. Playing for data: ground truth from computer games. In: European conference on computer vision (ECCV); 2016.
Brostow Gabriel J, Fauqueur Julien, Cipolla Roberto. Semantic object classes in video: a high-definition ground truth database. Pattern Recogn Lett. 2008;30(2):88–97.
Article
Google Scholar
Marius C, Mohamed O, Sebastian R, Timo R, Markus E, Rodrigo B, Uwe F, Stefan R, Bernt S. The cityscape dataset for semantic urban scene understanding. In: CVPR; 2016.
Esteban R, Sherry M, Andrew S, Saurabh S, Yutaka LS, Jie T, Quoc VL, Alexey K. Large-scale evolution of image classifiers. In: Proceedings of the 34th international conference on machine learning (ICML ‘17). 2017.
Esteban R, Alok A, Yanping H, Quoc VL. Regularized evolution for image classifier architecture search. arXiv preprint. 2018.
Tim S, Jonathan H, Xi C, Szymon S, Ilya S. Evolution strategies as a scalable alternative to reinforcement learning. arXiv e-prints. 2017.
Horia M, Aurelia G, Benjamin R. Simple random search provides a competitive approach to reinforcement learning. In: Advances in neural information processing systems (NIPS); 2018.
David GL. Distinctive image features from scale-invariant keypoints. Int J Comput Vis. 2004;2004:91–110.
Google Scholar
Navneet D, Bill T. Histograms of oriented gradients for human detection. In: CVPR, 2005.
Sutton RS, Reinforcement AG. Learning: an introduction. New York: MIT Press; 1998.
Google Scholar
Mingyang G, Kele X, Bo D, Huaimin W, Lei Z. Learning data augmentation policies using augmented random search. arXiv preprint. 2018.
Tran NM, Mathieu S, Hoang TL, Martin W. Automated image data preprocessing with deep reinforcement learning. arXiv preprints. 2018.
Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int J Uncertain Fuzzin Know Based Syst. 1998;6(02):107–16.
Article
Google Scholar
Jia S, Wang P, Jia P, Hu S. Research on data augmentation for image classification based on convolutional neural networks. In: 2017 Chinese automation congress (CAC), 2017. p. 4165–70.
Ilija R, Piotr D, Ross G, Georgia G, Kaiming H. Data distillation: towards omni-supervised learning. In: CVPR ’18; 2018.
Guotai W, Michael A, Sebastien O, Wenqi L, Jan D, Tom V. Test-time augmentation with uncertainty estimation for deep learning-based medical image segmentation. OpenReview.net. 2018.
Fabio P, Christina V, Sandra A, Eduardo V. Data augmentation for skin lesion analysis. In: ISIC skin image analysis workshop and challenge @ MICCAI 2018. 2018.
Karzuhisa M, Akira H, Akane M, Hiroshi K. Image classification of melanoma, nevus and seborrheic keratosis by deep neural network ensemble. In: International skin imaging collaboration (ISIC) 2017 challenge at the international symposium on biomedical imaging (ISBI). 2017.
Max J, Karen S, Andrea V, Andrew Z. Synthetic data and artificial neural networks for natural scene text recognition. arXiv preprint. 2014.
Florian S, Dmitry K, James P. FaceNet: a unified embedding for face recognition and clustering. In: CVPR ‘15. 2015.
Xudong M, Qing L, Haoran X, Raymond YKL, Zhen W, Stephen PS. Least squares generative adversarial networks. In: International conference on computer vision (ICCV), 2017.
Ren W, Shengen Y, Yi S, Qingqing D, Gang S. Deep image: scaling up image recognition. CoRR, abs/1501.02876, 2015.
Chao D, Chen CL, Kaiming H, Ziaoou T. Learning a deep convolutional network for image super-resolution. In: ECCV. Berlin: Springer; 2014. , p. 184–99.
Christian L, Lucas T, Ferenc H, Jose C, Andrew C, Alejandro A, Andrew A, Alykhan T, Johannes T, Zehan W, Wenzhe S. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint. 2016.
Han Z, Tao X, Hongsheng L, Shaoting Z, Xiaogang W, Xiaolei H, Dimitris M. StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV, 2017.
Trishul C, Yutaka S, Johnson A, Karthik K. Project adam: building an efficient and scalable deep learning training system. In: Proceedings of OSDI. 2014. P. 571–82.
Buda Mateusz, Maki Atsuto, Mazurowski Maciej A. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks. 2018;106:249–59.
Article
Google Scholar
Drown DJ, Khoshgoftaar TM, Seliya N. Evolutionary sampling and software quality modeling of high-assurance systems. IEEE Trans Syst. 2009;39(5):1097–107.
Google Scholar
Jason Y, Jeff C, Anh N, Thomas F, Hod L. Understanding neural networks through deep visualization. In: European conference on computer vision (ECCV). Berlin: Springer; 2015. p. 818–33.
Xiaofeng Z, Zhangyang W, Dong L, Qing L. DADA: deep adversarial data augmentation for extremely low data regime classification. arXiv preprint. 2018.
Martin A, Paul B, Jianmin C, Zhifeng C, Andy D, Jeffrey D, Matthieu D, Sanjay G, Geoffrey I, Michael I, Manjunath K, Josh L, Rajat M, Sherry M, Derek GM, Benoit S, Paul T, Vijay V, Pete W, Matrin W, Yuan Y, Xiaoqiang Z. TensorFlow: a system for large-scale machine learning. In: Proceedings of the 12th USENIX symposium on operating system design and implementation (OSDI ‘16), 2016.
Keras
https://keras.io/
. 2015.
Alexander B, Alex P, Eugene K, Vladimir II, Alexandr AK. Albumentations: fast and flexible image augmentations. ArXiv preprints. 2018.
Maayan F-A, Idit D, Eyal K, Michal A, Jacob G, Hayit G. GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification. arXiv preprint. 2018.
Changhee H, Hideaki H, Leonardo R, Ryosuke A, Wataru S, Shinichi M, Yujiro F, Giancarlo M, Hideki N. GAN-based synthetic brain mr image generation. In: 2018 IEEE 15th International Symposium on biomedical imaging (ISBI 2018). IEEE, 2011. P. 734–8.
Download references
Acknowledgements
We would like to thank the reviewers in the Data Mining and Machine Learning Laboratory at Florida Atlantic University. Additionally, we acknowledge partial support by the NSF (CNS-1427536). Opinions, findings, conclusions, or recommendations in this paper are solely of the authors’ and do not reflect the views of the NSF.
Funding
Not applicable.
Author information
Authors and Affiliations
Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, USA
Connor Shorten & Taghi M. Khoshgoftaar
Authors
Connor Shorten
View author publications
You can also search for this author in
PubMed
Google Scholar
Taghi M. Khoshgoftaar
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
CS performed the primary literature review and analysis for this work, and also drafted the manuscript. TMK, JLL, RAB, RZ, KW, NS, and RK worked with CS to develop the article’s framework and focus. TMK introduced this topic to CS, and helped to complete and finalize this work. All authors read and approved the final manuscript.
Corresponding author
Correspondence to
Connor Shorten
.
Ethics declarations
Competing interests
The authors declare that they have no competing interests.
Consent for publication
Not applicable.
Ethics approval and consent to participate
Not applicable.
Additional information
Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (
http://creativecommons.org/licenses/by/4.0/
), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.
Reprints and permissions
About this article
Cite this article
Shorten, C., Khoshgoftaar, T.M. A survey on Image Data Augmentation for Deep Learning.
J Big Data
6
, 60 (2019). https://doi.org/10.1186/s40537-019-0197-0
Download citation
Received
:
09 January 2019
Accepted
:
22 April 2019
Published
:
06 July 2019
DOI
:
https://doi.org/10.1186/s40537-019-0197-0
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Data Augmentation
Big data
Image data
Deep Learning
GANs
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0):
Big data in healthcare: management, analysis and future prospects | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Big data in healthcare: management, analysis and future prospects
Download PDF
Download ePub
Download PDF
Download ePub
Survey paper
Open access
Published:
19 June 2019
Big data in healthcare: management, analysis and future prospects
Sabyasachi Dash
1
na1
,
Sushil Kumar Shakyawar
2
,
3
na1
,
Mohit Sharma
4
,
5
&
…
Sandeep Kaushik
6
Show authors
Journal of Big Data
volume
6
, Article number:
54
(
2019
)
Cite this article
474k
Accesses
803
Citations
102
Altmetric
Metrics
details
Abstract
‘Big data’ is massive amounts of information that can work wonders. It has become a topic of special interest for the past two decades because of a great potential that is hidden in it. Various public and private sector industries generate, store, and analyze big data with an aim to improve the services they provide. In the healthcare industry, various sources for big data include hospital records, medical records of patients, results of medical examinations, and devices that are a part of internet of things. Biomedical research also generates a significant portion of big data relevant to public healthcare. This data requires proper management and analysis in order to derive meaningful information. Otherwise, seeking solution by analyzing big data quickly becomes comparable to finding a needle in the haystack. There are various challenges associated with each step of handling big data which can only be surpassed by using high-end computing solutions for big data analysis. That is why, to provide relevant solutions for improving public health, healthcare providers are required to be fully equipped with appropriate infrastructure to systematically generate and analyze big data. An efficient management, analysis, and interpretation of big data can change the game by opening new avenues for modern healthcare. That is exactly why various industries, including the healthcare industry, are taking vigorous steps to convert this potential into better services and financial advantages. With a strong integration of biomedical and healthcare data, modern healthcare organizations can possibly revolutionize the medical therapies and personalized medicine.
Introduction
Information has been the key to a better organization and new developments. The more information we have, the more optimally we can organize ourselves to deliver the best outcomes. That is why data collection is an important part for every organization. We can also use this data for the prediction of current trends of certain parameters and future events. As we are becoming more and more aware of this, we have started producing and collecting more data about almost everything by introducing technological developments in this direction. Today, we are facing a situation wherein we are flooded with tons of data from every aspect of our life such as social activities, science, work, health, etc. In a way, we can compare the present situation to a data deluge. The technological advances have helped us in generating more and more data, even to a level where it has become unmanageable with currently available technologies. This has led to the creation of the term ‘big data’ to describe data that is large and unmanageable. In order to meet our present and future social needs, we need to develop new strategies to organize this data and derive meaningful information. One such special social need is healthcare. Like every other industry, healthcare organizations are producing data at a tremendous rate that presents many advantages and challenges at the same time. In this review, we discuss about the basics of big data including its management, analysis and future prospects especially in healthcare sector.
The data overload
Every day, people working with various organizations around the world are generating a massive amount of data. The term “digital universe” quantitatively defines such massive amounts of data created, replicated, and consumed in a single year. International Data Corporation (IDC) estimated the approximate size of the digital universe in 2005 to be 130 exabytes (EB). The digital universe in 2017 expanded to about 16,000 EB or 16 zettabytes (ZB). IDC predicted that the digital universe would expand to 40,000 EB by the year 2020. To imagine this size, we would have to assign about 5200 gigabytes (GB) of data to all individuals. This exemplifies the phenomenal speed at which the digital universe is expanding. The internet giants, like Google and Facebook, have been collecting and storing massive amounts of data. For instance, depending on our preferences, Google may store a variety of information including user location, advertisement preferences, list of applications used, internet browsing history, contacts, bookmarks, emails, and other necessary information associated with the user. Similarly, Facebook stores and analyzes more than about 30 petabytes (PB) of user-generated data. Such large amounts of data constitute ‘
big data
’. Over the past decade, big data has been successfully used by the IT industry to generate critical information that can generate significant revenue.
These observations have become so conspicuous that has eventually led to the birth of a new field of science termed ‘
Data Science
’. Data science deals with various aspects including data management and analysis, to extract deeper insights for improving the functionality or services of a system (for example, healthcare and transport system). Additionally, with the availability of some of the most creative and meaningful ways to visualize big data post-analysis, it has become easier to understand the functioning of any complex system. As a large section of society is becoming aware of, and involved in generating big data, it has become necessary to define what big data is. Therefore, in this review, we attempt to provide details on the impact of big data in the transformation of global healthcare sector and its impact on our daily lives.
Defining big data
As the name suggests, ‘big data’ represents large amounts of data that is unmanageable using traditional software or internet-based platforms. It surpasses the traditionally used amount of storage, processing and analytical power. Even though a number of definitions for big data exist, the most popular and well-accepted definition was given by Douglas Laney. Laney observed that (big) data was growing in three different dimensions namely, volume, velocity and variety (known as the 3 Vs) [
1
]. The ‘big’ part of big data is indicative of its large volume. In addition to volume, the big data description also includes velocity and variety. Velocity indicates the speed or rate of data collection and making it accessible for further analysis; while, variety remarks on the different types of organized and unorganized data that any firm or system can collect, such as transaction-level data, video, audio, text or log files. These three Vs have become the standard definition of big data. Although, other people have added several other Vs to this definition [
2
], the most accepted 4th V remains ‘veracity’.
The term “
big data
” has become extremely popular across the globe in recent years. Almost every sector of research, whether it relates to industry or academics, is generating and analyzing
big data
for various purposes. The most challenging task regarding this huge heap of data that can be organized and unorganized, is its management. Given the fact that big data is unmanageable using the traditional software, we need technically advanced applications and software that can utilize fast and cost-efficient high-end computational power for such tasks. Implementation of artificial intelligence (AI) algorithms and novel fusion algorithms would be necessary to make sense from this large amount of data. Indeed, it would be a great feat to achieve automated decision-making by the implementation of machine learning (ML) methods like neural networks and other AI techniques. However, in absence of appropriate software and hardware support, big data can be quite hazy. We need to develop better techniques to handle this ‘endless sea’ of data and smart web applications for efficient analysis to gain workable insights. With proper storage and analytical tools in hand, the information and insights derived from big data can make the critical social infrastructure components and services (like healthcare, safety or transportation) more aware, interactive and efficient [
3
]. In addition, visualization of big data in a user-friendly manner will be a critical factor for societal development.
Healthcare as a big-data repository
Healthcare is a multi-dimensional system established with the sole aim for the prevention, diagnosis, and treatment of health-related issues or impairments in human beings. The major components of a healthcare system are the health professionals (physicians or nurses), health facilities (clinics, hospitals for delivering medicines and other diagnosis or treatment technologies), and a financing institution supporting the former two. The health professionals belong to various health sectors like dentistry, medicine, midwifery, nursing, psychology, physiotherapy, and many others. Healthcare is required at several levels depending on the urgency of situation. Professionals serve it as the first point of consultation (for primary care), acute care requiring skilled professionals (secondary care), advanced medical investigation and treatment (tertiary care) and highly uncommon diagnostic or surgical procedures (quaternary care). At all these levels, the health professionals are responsible for different kinds of information such as patient’s medical history (diagnosis and prescriptions related data), medical and clinical data (like data from imaging and laboratory examinations), and other private or personal medical data. Previously, the common practice to store such medical records for a patient was in the form of either handwritten notes or typed reports [
4
]. Even the results from a medical examination were stored in a paper file system. In fact, this practice is really old, with the oldest case reports existing on a papyrus text from Egypt that dates back to 1600 BC [
5
]. In Stanley Reiser’s words, the clinical case records freeze the episode of illness as a story in which patient, family and the doctor are a part of the plot” [
6
].
With the advent of computer systems and its potential, the digitization of all clinical exams and medical records in the healthcare systems has become a standard and widely adopted practice nowadays. In 2003, a division of the National Academies of Sciences, Engineering, and Medicine known as Institute of Medicine chose the term “
electronic health records
” to represent records maintained for improving the health care sector towards the benefit of patients and clinicians. Electronic health records (EHR) as defined by Murphy, Hanken and Waters are computerized medical records for patients any information relating to the past, present or future physical/mental health or condition of an individual which resides in electronic system(s) used to capture, transmit, receive, store, retrieve, link and manipulate multimedia data for the primary purpose of providing healthcare and health-related services” [
7
].
Electronic health records
It is important to note that the National Institutes of Health (NIH) recently announced the “All of Us” initiative (
https://allofus.nih.gov/
) that aims to collect one million or more patients’ data such as EHR, including medical imaging, socio-behavioral, and environmental data over the next few years. EHRs have introduced many advantages for handling modern healthcare related data. Below, we describe some of the characteristic advantages of using EHRs. The first advantage of EHRs is that healthcare professionals have an improved access to the entire medical history of a patient. The information includes medical diagnoses, prescriptions, data related to known allergies, demographics, clinical narratives, and the results obtained from various laboratory tests. The recognition and treatment of medical conditions thus is time efficient due to a reduction in the lag time of previous test results. With time we have observed a significant decrease in the redundant and additional examinations, lost orders and ambiguities caused by illegible handwriting, and an improved care coordination between multiple healthcare providers. Overcoming such logistical errors has led to reduction in the number of drug allergies by reducing errors in medication dose and frequency. Healthcare professionals have also found access over web based and electronic platforms to improve their medical practices significantly using automatic reminders and prompts regarding vaccinations, abnormal laboratory results, cancer screening, and other periodic checkups. There would be a greater continuity of care and timely interventions by facilitating communication among multiple healthcare providers and patients. They can be associated to electronic authorization and immediate insurance approvals due to less paperwork. EHRs enable faster data retrieval and facilitate reporting of key healthcare quality indicators to the organizations, and also improve public health surveillance by immediate reporting of disease outbreaks. EHRs also provide relevant data regarding the quality of care for the beneficiaries of employee health insurance programs and can help control the increasing costs of health insurance benefits. Finally, EHRs can reduce or absolutely eliminate delays and confusion in the billing and claims management area. The EHRs and internet together help provide access to millions of health-related medical information critical for patient life.
Digitization of healthcare and big data
Similar to EHR, an electronic medical record (EMR) stores the standard medical and clinical data gathered from the patients. EHRs, EMRs, personal health record (PHR), medical practice management software (MPM), and many other healthcare data components collectively have the potential to improve the quality, service efficiency, and costs of healthcare along with the reduction of medical errors. The big data in healthcare includes the healthcare payer-provider data (such as EMRs, pharmacy prescription, and insurance records) along with the genomics-driven experiments (such as genotyping, gene expression data) and other data acquired from the smart web of internet of things (IoT) (Fig.
1
). The adoption of EHRs was slow at the beginning of the 21st century however it has grown substantially after 2009 [
7
,
8
]. The management and usage of such healthcare data has been increasingly dependent on information technology. The development and usage of wellness monitoring devices and related software that can generate alerts and share the health related data of a patient with the respective health care providers has gained momentum, especially in establishing a real-time biomedical and health monitoring system. These devices are generating a huge amount of data that can be analyzed to provide real-time clinical or medical care [
9
]. The use of big data from healthcare shows promise for improving health outcomes and controlling costs.
Fig. 1
Workflow of Big data Analytics. Data warehouses store massive amounts of data generated from various sources. This data is processed using analytic pipelines to obtain smarter and affordable healthcare options
Full size image
Big data in biomedical research
A biological system, such as a human cell, exhibits molecular and physical events of complex interplay. In order to understand interdependencies of various components and events of such a complex system, a biomedical or biological experiment usually gathers data on a smaller and/or simpler component. Consequently, it requires multiple simplified experiments to generate a wide map of a given biological phenomenon of interest. This indicates that more the data we have, the better we understand the biological processes. With this idea, modern techniques have evolved at a great pace. For instance, one can imagine the amount of data generated since the integration of efficient technologies like next-generation sequencing (NGS) and Genome wide association studies (GWAS) to decode human genetics. NGS-based data provides information at depths that were previously inaccessible and takes the experimental scenario to a completely new dimension. It has increased the resolution at which we observe or record biological events associated with specific diseases in a real time manner. The idea that large amounts of data can provide us a good amount of information that often remains unidentified or hidden in smaller experimental methods has ushered-in the ‘-
omics
’ era. The ‘
omics
’ discipline has witnessed significant progress as instead of studying a single ‘
gene
’ scientists can now study the whole ‘
genome
’ of an organism in ‘
genomics
’ studies within a given amount of time. Similarly, instead of studying the expression or ‘
transcription
’ of single gene, we can now study the expression of all the genes or the entire ‘
transcriptome
’ of an organism under ‘
transcriptomics
’ studies. Each of these individual experiments generate a large amount of data with more depth of information than ever before. Yet, this depth and resolution might be insufficient to provide all the details required to explain a particular mechanism or event. Therefore, one usually finds oneself analyzing a large amount of data obtained from multiple experiments to gain novel insights. This fact is supported by a continuous rise in the number of publications regarding big data in healthcare (Fig.
2
). Analysis of such big data from medical and healthcare systems can be of immense help in providing novel strategies for healthcare. The latest technological developments in data generation, collection and analysis, have raised expectations towards a revolution in the field of personalized medicine in near future.
Fig. 2
Publications associated with big data in healthcare. The numbers of publications in PubMed are plotted by year
Full size image
Big data from omics studies
NGS has greatly simplified the sequencing and decreased the costs for generating whole genome sequence data. The cost of complete genome sequencing has fallen from millions to a couple of thousand dollars [
10
]. NGS technology has resulted in an increased volume of biomedical data that comes from genomic and transcriptomic studies. According to an estimate, the number of human genomes sequenced by 2025 could be between 100 million to 2 billion [
11
]. Combining the genomic and transcriptomic data with proteomic and metabolomic data can greatly enhance our knowledge about the individual profile of a patient—an approach often ascribed as “individual, personalized or precision health care”. Systematic and integrative analysis of omics data in conjugation with healthcare analytics can help design better treatment strategies towards precision and personalized medicine (Fig.
3
). The genomics-driven experiments e.g., genotyping, gene expression, and NGS-based studies are the major source of big data in biomedical healthcare along with EMRs, pharmacy prescription information, and insurance records. Healthcare requires a strong integration of such biomedical data from various sources to provide better treatments and patient care. These prospects are so exciting that even though genomic data from patients would have many variables to be accounted, yet commercial organizations are already using human genome data to help the providers in making personalized medical decisions. This might turn out to be a game-changer in future medicine and health.
Fig. 3
A framework for integrating omics data and health care analytics to promote personalized treatment
Full size image
Internet of Things (IOT)
Healthcare industry has not been quick enough to adapt to the big data movement compared to other industries. Therefore, big data usage in the healthcare sector is still in its infancy. For example, healthcare and biomedical big data have not yet converged to enhance healthcare data with molecular pathology. Such convergence can help unravel various mechanisms of action or other aspects of predictive biology. Therefore, to assess an individual’s health status, biomolecular and clinical datasets need to be married. One such source of clinical data in healthcare is ‘internet of things’ (IoT).
In fact, IoT is another big player implemented in a number of other industries including healthcare. Until recently, the objects of common use such as cars, watches, refrigerators and health-monitoring devices, did not usually produce or handle data and lacked internet connectivity. However, furnishing such objects with computer chips and sensors that enable data collection and transmission over internet has opened new avenues. The device technologies such as Radio Frequency IDentification (RFID) tags and readers, and Near Field Communication (NFC) devices, that can not only gather information but interact physically, are being increasingly used as the information and communication systems [
3
]. This enables objects with RFID or NFC to communicate and function as a web of smart things. The analysis of data collected from these chips or sensors may reveal critical information that might be beneficial in improving lifestyle, establishing measures for energy conservation, improving transportation, and healthcare. In fact, IoT has become a rising movement in the field of healthcare. IoT devices create a continuous stream of data while monitoring the health of people (or patients) which makes these devices a major contributor to big data in healthcare. Such resources can interconnect various devices to provide a reliable, effective and smart healthcare service to the elderly and patients with a chronic illness [
12
].
Advantages of IoT in healthcare
Using the web of IoT devices, a doctor can measure and monitor various parameters from his/her clients in their respective locations for example, home or office. Therefore, through early intervention and treatment, a patient might not need hospitalization or even visit the doctor resulting in significant cost reduction in healthcare expenses. Some examples of IoT devices used in healthcare include fitness or health-tracking wearable devices, biosensors, clinical devices for monitoring vital signs, and others types of devices or clinical instruments. Such IoT devices generate a large amount of health related data. If we can integrate this data with other existing healthcare data like EMRs or PHRs, we can predict a patients’ health status and its progression from subclinical to pathological state [
9
]. In fact, big data generated from IoT has been quiet advantageous in several areas in offering better investigation and predictions. On a larger scale, the data from such devices can help in personnel health monitoring, modelling the spread of a disease and finding ways to contain a particular disease outbreak.
The analysis of data from IoT would require an updated operating software because of its specific nature along with advanced hardware and software applications. We would need to manage data inflow from IoT instruments in real-time and analyze it by the minute. Associates in the healthcare system are trying to trim down the cost and ameliorate the quality of care by applying advanced analytics to both internally and externally generated data.
Mobile computing and mobile health (mHealth)
In today’s digital world, every individual seems to be obsessed to track their fitness and health statistics using the in-built pedometer of their portable and wearable devices such as, smartphones, smartwatches, fitness dashboards or tablets. With an increasingly mobile society in almost all aspects of life, the healthcare infrastructure needs remodeling to accommodate mobile devices [
13
]. The practice of medicine and public health using mobile devices, known as mHealth or mobile health, pervades different degrees of health care especially for chronic diseases, such as diabetes and cancer [
14
]. Healthcare organizations are increasingly using mobile health and wellness services for implementing novel and innovative ways to provide care and coordinate health as well as wellness. Mobile platforms can improve healthcare by accelerating interactive communication between patients and healthcare providers. In fact, Apple and Google have developed devoted platforms like Apple’s ResearchKit and Google Fit for developing research applications for fitness and health statistics [
15
]. These applications support seamless interaction with various consumer devices and embedded sensors for data integration. These apps help the doctors to have direct access to your overall health data. Both the user and their doctors get to know the real-time status of your body. These apps and smart devices also help by improving our wellness planning and encouraging healthy lifestyles. The users or patients can become advocates for their own health.
Nature of the big data in healthcare
EHRs can enable advanced analytics and help clinical decision-making by providing enormous data. However, a large proportion of this data is currently unstructured in nature. An unstructured data is the information that does not adhere to a pre-defined model or organizational framework. The reason for this choice may simply be that we can record it in a myriad of formats. Another reason for opting unstructured format is that often the structured input options (drop-down menus, radio buttons, and check boxes) can fall short for capturing data of complex nature. For example, we cannot record the non-standard data regarding a patient’s clinical suspicions, socioeconomic data, patient preferences, key lifestyle factors, and other related information in any other way but an unstructured format. It is difficult to group such varied, yet critical, sources of information into an intuitive or unified data format for further analysis using algorithms to understand and leverage the patients care. Nonetheless, the healthcare industry is required to utilize the full potential of these rich streams of information to enhance the patient experience. In the healthcare sector, it could materialize in terms of better management, care and low-cost treatments. We are miles away from realizing the benefits of big data in a meaningful way and harnessing the insights that come from it. In order to achieve these goals, we need to manage and analyze the big data in a systematic manner.
Management and analysis of big data
Big data is the huge amounts of a variety of data generated at a rapid rate. The data gathered from various sources is mostly required for optimizing consumer services rather than consumer consumption. This is also true for big data from the biomedical research and healthcare. The major challenge with big data is how to handle this large volume of information. To make it available for scientific community, the data is required to be stored in a file format that is easily accessible and readable for an efficient analysis. In the context of healthcare data, another major challenge is the implementation of high-end computing tools, protocols and high-end hardware in the clinical setting. Experts from diverse backgrounds including biology, information technology, statistics, and mathematics are required to work together to achieve this goal. The data collected using the sensors can be made available on a storage cloud with pre-installed software tools developed by analytic tool developers. These tools would have data mining and ML functions developed by AI experts to convert the information stored as data into knowledge. Upon implementation, it would enhance the efficiency of acquiring, storing, analyzing, and visualization of big data from healthcare. The main task is to annotate, integrate, and present this complex data in an appropriate manner for a better understanding. In absence of such relevant information, the (healthcare) data remains quite cloudy and may not lead the biomedical researchers any further. Finally, visualization tools developed by computer graphics designers can efficiently display this newly gained knowledge.
Heterogeneity of data is another challenge in big data analysis. The huge size and highly heterogeneous nature of big data in healthcare renders it relatively less informative using the conventional technologies. The most common platforms for operating the software framework that assists big data analysis are high power computing clusters accessed via grid computing infrastructures. Cloud computing is such a system that has virtualized storage technologies and provides reliable services. It offers high reliability, scalability and autonomy along with ubiquitous access, dynamic resource discovery and composability. Such platforms can act as a receiver of data from the ubiquitous sensors, as a computer to analyze and interpret the data, as well as providing the user with easy to understand web-based visualization. In IoT, the big data processing and analytics can be performed closer to data source using the services of mobile edge computing cloudlets and fog computing. Advanced algorithms are required to implement ML and AI approaches for big data analysis on computing clusters. A programming language suitable for working on big data (e.g. Python, R or other languages) could be used to write such algorithms or software. Therefore, a good knowledge of biology and IT is required to handle the big data from biomedical research. Such a combination of both the trades usually fits for bioinformaticians. The most common among various platforms used for working with big data include Hadoop and Apache Spark. We briefly introduce these platforms below.
Hadoop
Loading large amounts of (big) data into the memory of even the most powerful of computing clusters is not an efficient way to work with big data. Therefore, the best logical approach for analyzing huge volumes of complex big data is to distribute and process it in parallel on multiple nodes. However, the size of data is usually so large that thousands of computing machines are required to distribute and finish processing in a reasonable amount of time. When working with hundreds or thousands of nodes, one has to handle issues like how to parallelize the computation, distribute the data, and handle failures. One of most popular open-source distributed application for this purpose is Hadoop [
16
]. Hadoop implements MapReduce algorithm for processing and generating large datasets. MapReduce uses
map
and
reduce
primitives to
map
each logical record’ in the input into a set of intermediate key/value pairs, and
reduce
operation combines all the values that shared the same key [
17
]. It efficiently parallelizes the computation, handles failures, and schedules inter-machine communication across large-scale clusters of machines. Hadoop Distributed File System (HDFS) is the file system component that provides a scalable, efficient, and replica based storage of data at various nodes that form a part of a cluster [
16
]. Hadoop has other tools that enhance the storage and processing components therefore many large companies like Yahoo, Facebook, and others have rapidly adopted  it. Hadoop has enabled researchers to use data sets otherwise impossible to handle. Many large projects, like the determination of a correlation between the air quality data and asthma admissions, drug development using genomic and proteomic data, and other such aspects of healthcare are implementing Hadoop. Therefore, with the implementation of Hadoop system, the healthcare analytics will not be held back.
Apache Spark
Apache Spark is another open source alternative to Hadoop. It is a unified engine for distributed data processing that includes higher-level libraries for supporting SQL queries (
Spark SQL
), streaming data (
Spark Streaming
), machine learning (
MLlib
) and graph processing (
GraphX
) [
18
]. These libraries help in increasing developer productivity because the programming interface requires lesser coding efforts and can be seamlessly combined to create more types of complex computations. By implementing Resilient distributed Datasets (RDDs), in-memory processing of data is supported that can make Spark about 100× faster than Hadoop in multi-pass analytics (on smaller datasets) [
19
,
20
]. This is more true when the data size is smaller than the available memory [
21
]. This indicates that processing of really big data with Apache Spark would require a large amount of memory. Since, the cost of memory is higher than the hard drive, MapReduce is expected to be more cost effective for large datasets compared to Apache Spark. Similarly, Apache Storm was developed to provide a real-time framework for data stream processing. This platform supports most of the programming languages. Additionally, it offers good horizontal scalability and built-in-fault-tolerance capability for big data analysis.
Machine learning for information extraction, data analysis and predictions
In healthcare, patient data contains recorded signals for instance, electrocardiogram (ECG), images, and videos. Healthcare providers have barely managed to convert such healthcare data into EHRs. Efforts are underway to digitize patient-histories from pre-EHR era notes and supplement the standardization process by turning static images into machine-readable text. For example, optical character recognition (OCR) software is one such approach that can recognize handwriting as well as computer fonts and push digitization. Such unstructured and structured healthcare datasets have untapped wealth of information that can be harnessed using advanced AI programs to draw critical actionable insights in the context of patient care. In fact, AI has emerged as the method of choice for big data applications in medicine. This smart system has quickly found its niche in decision making process for the diagnosis of diseases. Healthcare professionals analyze such data for targeted abnormalities using appropriate ML approaches. ML can filter out structured information from such raw data.
Extracting information from EHR datasets
Emerging ML or AI based strategies are helping to refine healthcare industry’s information processing capabilities. For example, natural language processing (NLP) is a rapidly developing area of machine learning that can identify key syntactic structures in free text, help in speech recognition and extract the meaning behind a narrative. NLP tools can help generate new documents, like a clinical visit summary, or to dictate clinical notes. The unique content and complexity of clinical documentation can be challenging for many NLP developers. Nonetheless, we should be able to extract relevant information from healthcare data using such approaches as NLP.
AI has also been used to provide predictive capabilities to healthcare big data. For example, ML algorithms can convert the diagnostic system of medical images into automated decision-making. Though it is apparent that healthcare professionals may not be replaced by machines in the near future, yet AI can definitely assist physicians to make better clinical decisions or even replace human judgment in certain functional areas of healthcare.
Image analytics
Some of the most widely used imaging techniques in healthcare include computed tomography (CT), magnetic resonance imaging (MRI), X-ray, molecular imaging, ultrasound, photo-acoustic imaging, functional MRI (fMRI), positron emission tomography (PET), electroencephalography (EEG), and mammograms. These techniques capture high definition medical images (patient data) of large sizes. Healthcare professionals like radiologists, doctors and others do an excellent job in analyzing medical data in the form of these files for targeted abnormalities. However, it is also important to acknowledge the lack of specialized professionals for many diseases. In order to compensate for this dearth of professionals, efficient systems like Picture Archiving and Communication System (PACS) have been developed for storing and convenient access to medical image and reports data [
22
]. PACSs are popular for delivering images to local workstations, accomplished by protocols such as digital image communication in medicine (DICOM). However, data exchange with a PACS relies on using structured data to retrieve medical images. This by nature misses out on the unstructured information contained in some of the biomedical images. Moreover, it is possible to miss an additional information about a patient’s health status that is present in these images or similar data. A professional focused on diagnosing an unrelated condition might not observe it, especially when the condition is still emerging. To help in such situations, image analytics is making an impact on healthcare by actively extracting disease biomarkers from biomedical images. This approach uses ML and pattern recognition techniques to draw insights from massive volumes of clinical image data to transform the diagnosis, treatment and monitoring of patients. It focuses on enhancing the diagnostic capability of medical imaging for clinical decision-making.
A number of software tools have been developed based on functionalities such as generic, registration, segmentation, visualization, reconstruction, simulation and diffusion to perform medical image analysis in order to dig out the hidden information. For example, Visualization Toolkit is a freely available software which allows powerful processing and analysis of 3D images from medical tests [
23
], while SPM can process and analyze 5 different types of brain images (e.g. MRI, fMRI, PET, CT-Scan and EEG) [
24
]. Other software like GIMIAS, Elastix, and MITK support all types of images. Various other widely used tools and their features in this domain are listed in Table
1
. Such bioinformatics-based big data analysis may extract greater insights and value from imaging data to boost and support precision medicine projects, clinical decision support tools, and other modes of healthcare. For example, we can also use it to monitor new targeted-treatments for cancer.
Table 1 Bioinformatics tools for medical image processing and analysis
Full size table
Big data from omics
The big data from “omics” studies is a new kind of challenge for the bioinformaticians. Robust algorithms are required to analyze such complex data from biological systems. The ultimate goal is to convert this huge data into an informative knowledge base. The application of bioinformatics approaches to transform the biomedical and genomics data into predictive and preventive health is known as translational bioinformatics. It is at the forefront of data-driven healthcare. Various kinds of quantitative data in healthcare, for example from laboratory measurements, medication data and genomic profiles, can be combined and used to identify new meta-data that can help precision therapies [
25
]. This is why emerging new technologies are required to help in analyzing this digital wealth. In fact, highly ambitious multimillion-dollar projects like “
Big Data Research and Development Initiative
” have been launched that aim to enhance the quality of big data tools and techniques for a better organization, efficient access and smart analysis of big data. There are many advantages anticipated from the processing of ‘
omics’
data from large-scale Human Genome Project and other population sequencing projects. In the population sequencing projects like 1000 genomes, the researchers will have access to a marvelous amount of raw data. Similarly, Human Genome Project based
Encyclopedia of DNA Elements
(ENCODE) project aimed to determine all functional elements in the human genome using bioinformatics approaches. Here, we list some of the widely used bioinformatics-based tools for big data analytics on omics data.
1.
SparkSeq
is an efficient and cloud-ready platform based on Apache Spark framework and Hadoop library that is used for analyses of genomic data for interactive genomic data analysis with nucleotide precision
2.
SAMQA
identifies errors and ensures the quality of large-scale genomic data. This tool was originally built for the National Institutes of Health Cancer Genome Atlas project to identify and report errors including sequence alignment/map [SAM] format error and empty reads.
3.
ART
can simulate profiles of read errors and read lengths for data obtained using high throughput sequencing platforms including SOLiD and Illumina platforms.
4.
DistMap
is another toolkit used for distributed short-read mapping based on Hadoop cluster that aims to cover a wider range of sequencing applications. For instance, one of its applications namely the BWA mapper can perform 500 million read pairs in about 6 h, approximately 13 times faster than a conventional single-node mapper.
5.
SeqWare
is a query engine based on Apache HBase database system that enables access for large-scale whole-genome datasets by integrating genome browsers and tools.
6.
CloudBurst
is a parallel computing model utilized in genome mapping experiments to improve the scalability of reading large sequencing data.
7.
Hydra
uses the Hadoop-distributed computing framework for processing large peptide and spectra databases for proteomics datasets. This specific tool is capable of performing 27 billion peptide scorings in less than 60 min on a Hadoop cluster.
8.
BlueSNP
is an R package based on Hadoop platform used for genome-wide association studies (GWAS) analysis, primarily aiming on the statistical readouts to obtain significant associations between genotype–phenotype datasets. The efficiency of this tool is estimated to analyze 1000 phenotypes on 10
6
SNPs in 10
4
individuals in a duration of half-an-hour.
9.
Myrna
the cloud-based pipeline, provides information on the expression level differences of genes, including read alignments, data normalization, and statistical modeling.
The past few years have witnessed a tremendous increase in disease specific datasets from omics platforms. For example, the
ArrayExpress Archive of Functional Genomics
data repository contains information from approximately 30,000 experiments and more than one million functional assays. The growing amount of data demands for better and efficient bioinformatics driven packages to analyze and interpret the information obtained. This has also led to the birth of specific tools to analyze such massive amounts of data. Below, we mention some of the most popular commercial platforms for big data analytics.
Commercial platforms for healthcare data analytics
In order to tackle big data challenges and perform smoother analytics, various companies have implemented AI to analyze published results, textual data, and image data to obtain meaningful outcomes. IBM Corporation is one of the biggest and experienced players in this sector to provide healthcare analytics services commercially. IBM’s Watson Health is an AI platform to share and analyze health data among hospitals, providers and researchers. Similarly, Flatiron Health provides technology-oriented services in healthcare analytics specially focused in cancer research. Other big companies such as Oracle Corporation and Google Inc. are also focusing to develop cloud-based storage and distributed computing power platforms. Interestingly, in the recent few years, several companies and start-ups have also emerged to provide health care-based analytics and solutions. Some of the vendors in healthcare sector are provided in Table
2
. Below we discuss a few of these commercial solutions.
Table 2 List of some of big companies which provide services on big data analysis in healthcare sector
Full size table
AYASDI
Ayasdi is one such big vendor which focuses on ML based methodologies to primarily provide machine intelligence platform along with an application framework with tried & tested enterprise scalability. It provides various applications for healthcare analytics, for example, to understand and manage clinical variation, and to transform clinical care costs. It is also capable of analyzing and managing how hospitals are organized, conversation between doctors, risk-oriented decisions by doctors for treatment, and the care they deliver to patients. It also provides an application for the assessment and management of population health, a proactive strategy that goes beyond traditional risk analysis methodologies. It uses ML intelligence for predicting future risk trajectories, identifying risk drivers, and providing solutions for best outcomes. A strategic illustration of the company’s methodology for analytics is provided in Fig.
4
.
Fig. 4
Illustration of application of “Intelligent Application Suite” provided by AYASDI for various analyses such as clinical variation, population health, and risk management in healthcare sector
Full size image
Linguamatics
It is an NLP based algorithm that relies on an interactive text mining algorithm (I2E). I2E can extract and analyze a wide array of information. Results obtained using this technique are tenfold faster than other tools and does not require expert knowledge for data interpretation. This approach can provide information on genetic relationships and facts from unstructured data. Classical, ML requires well-curated data as input to generate clean and filtered results. However, NLP when integrated in EHR or clinical records per se facilitates the extraction of clean and structured information that often remains hidden in unstructured input data (Fig.
5
).
Fig. 5
Schematic representation for the working principle of NLP-based AI system used in massive data retention and analysis in Linguamatics
Full size image
IBM Watson
This is one of the unique ideas of the tech-giant IBM that targets big data analytics in almost every professional sector. This platform utilizes ML and AI based algorithms extensively to extract the maximum information from minimal input. IBM Watson enforces the regimen of integrating a wide array of healthcare domains to provide meaningful and structured data (Fig.
6
). In an attempt to uncover novel drug targets specifically in cancer disease model, IBM Watson and Pfizer have formed a productive collaboration to accelerate the discovery of novel immune-oncology combinations. Combining Watson’s deep learning modules integrated with AI technologies allows the researchers to interpret complex genomic data sets. IBM Watson has been used to predict specific types of cancer based on the gene expression profiles obtained from various large data sets providing signs of multiple druggable targets. IBM Watson is also used in drug discovery programs by integrating curated literature and forming network maps to provide a detailed overview of the molecular landscape in a specific disease model.
Fig. 6
IBM Watson in healthcare data analytics. Schematic representation of the various functional modules in IBM Watson’s big-data healthcare package. For instance, the drug discovery domain involves network of highly coordinated data acquisition and analysis within the spectrum of curating database to building meaningful pathways towards elucidating novel druggable targets
Full size image
In order to analyze the diversified medical data, healthcare domain, describes analytics in four categories: descriptive, diagnostic, predictive, and prescriptive analytics. Descriptive analytics refers for describing the current medical situations and commenting on that whereas diagnostic analysis explains reasons and factors behind occurrence of certain events, for example, choosing treatment option for a patient based on clustering and decision trees. Predictive analytics focuses on predictive ability of the future outcomes by determining trends and probabilities. These methods are mainly built up of machine leaning techniques and are helpful in the context of understanding complications that a patient can develop. Prescriptive analytics is to perform analysis to propose an action towards optimal decision making. For example, decision of avoiding a given treatment to the patient based on observed side effects and predicted complications. In order to improve performance of the current medical systems integration of big data into healthcare analytics can be a major factor; however, sophisticated strategies  need to be developed. An architecture of best practices of different analytics in healthcare domain is required for integrating big data technologies to improve the outcomes. However, there are many challenges associated with the implementation of such strategies.
Challenges associated with healthcare big data
Methods for big data management and analysis are being continuously developed especially for real-time data streaming, capture, aggregation, analytics (using ML and predictive), and visualization solutions that can help integrate a better utilization of EMRs with the healthcare. For example, the EHR adoption rate of federally tested and certified EHR programs in the healthcare sector in the U.S.A. is nearly complete [
7
]. However, the availability of hundreds of EHR products certified by the government, each with different clinical terminologies, technical specifications, and functional capabilities has led to difficulties in the interoperability and sharing of data. Nonetheless, we can safely say that the healthcare industry has entered into a ‘post-EMR’ deployment phase. Now, the main objective is to gain actionable insights from these vast amounts of data collected as EMRs. Here, we discuss some of these challenges in brief.
Storage
Storing large volume of data is one of the primary challenges, but many organizations are comfortable with data storage on their own premises. It has several advantages like control over security, access, and up-time. However, an on-site server network can be expensive to scale and difficult to maintain. It appears that with decreasing costs and increasing reliability, the cloud-based storage using IT infrastructure is a better option which most of the healthcare organizations have opted for. Organizations must choose cloud-partners that understand the importance of healthcare-specific compliance and security issues. Additionally, cloud storage offers lower up-front costs, nimble disaster recovery, and easier expansion. Organizations can also have a hybrid approach to their data storage programs, which may be the most flexible and workable approach for providers with varying data access and storage needs.
Cleaning
The data needs to cleansed or scrubbed to ensure the accuracy, correctness, consistency, relevancy, and purity after acquisition. This cleaning process can be manual or automatized using logic rules to ensure high levels of accuracy and integrity. More sophisticated and precise tools use machine-learning techniques to reduce time and expenses and to stop foul data from derailing big data projects.
Unified format
Patients produce a huge volume of data that is not easy to capture with traditional EHR format, as it is knotty and not easily manageable. It is too difficult to handle big data especially when it comes without a perfect data organization to the healthcare providers. A need to codify all the clinically relevant information surfaced for the purpose of claims, billing purposes, and clinical analytics. Therefore, medical coding systems like Current Procedural Terminology (CPT) and International Classification of Diseases (ICD) code sets were developed to represent the core clinical concepts. However, these code sets have their own limitations.
Accuracy
Some studies have observed that the reporting of patient data into EMRs or EHRs is not entirely accurate yet [
26
,
27
,
28
,
29
], probably because of poor EHR utility, complex workflows, and a broken understanding of why big data is all-important to capture well. All these factors can contribute to the quality issues for big data all along its lifecycle. The EHRs intend to improve the quality and communication of data in clinical workflows though reports indicate discrepancies in these contexts. The documentation quality might improve by using self-report questionnaires from patients for their symptoms.
Image pre-processing
Studies have observed various physical factors that can lead to altered data quality and misinterpretations from existing medical records [
30
]. Medical images often suffer technical barriers that involve multiple types of noise and artifacts. Improper handling of medical images can also cause tampering of images for instance might lead to delineation of anatomical structures such as veins which is non-correlative with real case scenario. Reduction of noise, clearing artifacts, adjusting contrast of acquired images and image quality adjustment post mishandling are some of the measures that can be implemented to benefit the purpose.
Security
There have been many security breaches, hackings, phishing attacks, and ransomware episodes that data security is a priority for healthcare organizations. After noticing an array of vulnerabilities, a list of technical safeguards was developed for the protected health information (PHI). These rules, termed as HIPAA Security Rules, help guide organizations with storing, transmission, authentication protocols, and controls over access, integrity, and auditing. Common security measures like using up-to-date anti-virus software, firewalls, encrypting sensitive data, and multi-factor authentication can save a lot of trouble.
Meta-data
To have a successful data governance plan, it would be mandatory to have complete, accurate, and up-to-date metadata regarding all the stored data. The metadata would be composed of information like time of creation, purpose and person responsible for the data, previous usage (by who, why, how, and when) for researchers and data analysts. This would allow analysts to replicate previous queries and help later scientific studies and accurate benchmarking. This increases the usefulness of data and prevents creation of “data dumpsters” of low or no use.
Querying
Metadata would make it easier for organizations to query their data and get some answers. However, in absence of proper interoperability between datasets the query tools may not access an entire repository of data. Also, different components of a dataset should be well interconnected or linked and easily accessible otherwise a complete portrait of an individual patient’s health may not be generated. Medical coding systems like ICD-10, SNOMED-CT, or LOINC must be implemented to reduce free-form concepts into a shared ontology. If the accuracy, completeness, and standardization of the data are not in question, then Structured Query Language (SQL) can be used to query large datasets and relational databases.
Visualization
A clean and engaging visualization of data with charts, heat maps, and histograms to illustrate contrasting figures and correct labeling of information to reduce potential confusion, can make it much easier for us to absorb information and use it appropriately. Other examples include bar charts, pie charts, and scatterplots with their own specific ways to convey the data.
Data sharing
Patients may or may not receive their care at multiple locations. In the former case, sharing data with other healthcare organizations would be essential. During such sharing, if the data is not interoperable then data movement between disparate organizations could be severely curtailed. This could be due to technical and organizational barriers. This may leave clinicians without key information for making decisions regarding follow-ups and treatment strategies for patients. Solutions like Fast Healthcare Interoperability Resource (FHIR) and public APIs, CommonWell (a not-for-profit trade association) and Carequality (a consensus-built, common interoperability framework) are making data interoperability and sharing easy and secure. The biggest roadblock for data sharing is the treatment of data as a commodity that can provide a competitive advantage. Therefore, sometimes both providers and vendors intentionally interfere with the flow of information to block the information flow between different EHR systems [
31
].
The healthcare providers will need to overcome every challenge on this list and more to develop a big data exchange ecosystem that provides trustworthy, timely, and meaningful information by connecting all members of the care continuum. Time, commitment, funding, and communication would be required before these challenges are overcome.
Big data analytics for cutting costs
To develop a healthcare system based on big data that can exchange big data and provides us with trustworthy, timely, and meaningful information, we need to overcome every challenge mentioned above. Overcoming these challenges would require investment in terms of time, funding, and commitment. However, like other technological advances, the success of these ambitious steps would apparently ease the present burdens on healthcare especially in terms of costs. It is believed that the implementation of big data analytics by healthcare organizations might lead to a saving of over 25% in annual costs in the coming years. Better diagnosis and disease predictions by big data analytics can enable cost reduction by decreasing the hospital readmission rate. The healthcare firms do not understand the variables responsible for readmissions well enough. It would be easier for healthcare organizations to improve their protocols for dealing with patients and prevent readmission by determining these relationships well. Big data analytics can also help in optimizing staffing, forecasting operating room demands, streamlining patient care, and improving the pharmaceutical supply chain. All of these factors will lead to an ultimate reduction in the healthcare costs by the organizations.
Quantum mechanics and big data analysis
Big data sets can be staggering in size. Therefore, its analysis remains daunting even with the most powerful modern computers. For most of the analysis, the bottleneck lies in the computer’s ability to access its memory and not in the processor [
32
,
33
]. The capacity, bandwidth or latency requirements of memory hierarchy outweigh the computational requirements so much that supercomputers are increasingly used for big data analysis [
34
,
35
]. An additional solution is the application of quantum approach for big data analysis.
Quantum computing and its advantages
The common digital computing uses binary digits to code for the data whereas quantum computation uses quantum bits or
qubits
[
36
]. A
qubit
is a quantum version of the classical binary bits that can represent a zero, a one, or any linear combination of states (called
superpositions
) of those two qubit states [
37
]. Therefore, qubits allow computer bits to operate in three states compared to two states in the classical computation. This allows quantum computers to work thousands of times faster than regular computers. For example, a conventional analysis of a dataset with
n
points would require 2
n
processing units whereas it would require just
n
quantum bits using a quantum computer. Quantum computers use quantum mechanical phenomena like superposition and quantum entanglement to perform computations [
38
,
39
].
Quantum algorithms can speed-up the big data analysis exponentially [
40
]. Some complex problems, believed to be unsolvable using conventional computing, can be solved by quantum approaches. For example, the current encryption techniques such as RSA, public-key (PK) and Data Encryption Standard (DES) which are thought to be impassable now would be irrelevant in future because quantum computers will quickly get through them [
41
]. Quantum approaches can dramatically reduce the information required for big data analysis. For example, quantum theory can maximize the distinguishability between a multilayer network using a minimum number of layers [
42
]. In addition, quantum approaches require a relatively small dataset to obtain a maximally sensitive data analysis compared to the conventional (machine-learning) techniques. Therefore, quantum approaches can drastically reduce the amount of computational power required to analyze big data. Even though, quantum computing is still in its infancy and presents many open challenges, it is being implemented for healthcare data.
Applications in big data analysis
Quantum computing is picking up and seems to be a potential solution for big data analysis. For example, identification of rare events, such as the production of Higgs bosons at the Large Hadron Collider (LHC) can now be performed using quantum approaches [
43
]. At LHC, huge amounts of collision data (1PB/s) is generated that needs to be filtered and analyzed. One such approach, the quantum annealing for ML (QAML) that implements a combination of ML and quantum computing with a programmable quantum annealer, helps reduce human intervention and increase the accuracy of assessing particle-collision data. In another example, the quantum support vector machine was implemented for both training and classification stages to classify new data [
44
]. Such quantum approaches could find applications in many areas of science [
43
]. Indeed, recurrent quantum neural network (RQNN) was implemented to increase signal separability in electroencephalogram (EEG) signals [
45
]. Similarly, quantum annealing was applied to intensity modulated radiotherapy (IMRT) beamlet intensity optimization [
46
]. Similarly, there exist more applications of quantum approaches regarding healthcare e.g. quantum sensors and quantum microscopes [
47
].
Conclusions and future prospects
Nowadays, various biomedical and healthcare tools such as genomics, mobile biometric sensors, and smartphone apps generate a big amount of data. Therefore, it is mandatory for us to know about and assess that can be achieved using this data. For example, the analysis of such data can provide further insights in terms of procedural, technical, medical and other types of improvements in healthcare. After a review of these healthcare procedures, it appears that the full potential of patient-specific medical specialty or personalized medicine is under way. The collective big data analysis of EHRs, EMRs and other medical data is continuously helping build a better prognostic framework. The companies providing service for healthcare analytics and clinical transformation are indeed contributing towards better and effective outcome. Common goals of these companies include reducing cost of analytics, developing effective Clinical Decision Support (CDS) systems, providing platforms for better treatment strategies, and identifying and preventing fraud associated with big data. Though, almost all of them face challenges on federal issues like how private data is handled, shared and kept safe. The combined pool of data from healthcare organizations and biomedical researchers have resulted in a better outlook, determination, and treatment of various diseases. This has also helped in building a better and healthier personalized healthcare framework. Modern healthcare fraternity has realized the potential of big data and therefore, have implemented big data analytics in healthcare and clinical practices. Supercomputers to quantum computers are helping in extracting meaningful information from big data in dramatically reduced time periods. With high hopes of extracting new and actionable knowledge that can improve the present status of healthcare services, researchers are plunging into biomedical big data despite the infrastructure challenges. Clinical trials, analysis of pharmacy and insurance claims together, discovery of biomarkers is a part of a novel and creative way to analyze healthcare big data.
Big data analytics leverage the gap within structured and unstructured data sources. The shift to an integrated data environment is a well-known hurdle to overcome. Interesting enough, the principle of big data heavily relies on the idea of the more the information, the more insights one can gain from this information and can make predictions for future events. It is rightfully projected by various reliable consulting firms and health care companies that the big data healthcare market is poised to grow at an exponential rate. However, in a short span we have witnessed a spectrum of analytics currently in use that have shown significant impacts on the decision making and performance of healthcare industry. The exponential growth of medical data from various domains has forced computational experts to design innovative strategies to analyze and interpret such enormous amount of data within a given timeframe. The integration of computational systems for signal processing from both research and practicing medical professionals has witnessed growth. Thus, developing a detailed model of a human body by combining physiological data and “-omics” techniques can be the next big target. This unique idea can enhance our knowledge of disease conditions and possibly help in the development of novel diagnostic tools. The continuous rise in available genomic data including inherent hidden errors from experiment and analytical practices need further attention. However, there are opportunities in each step of this extensive process to introduce systemic improvements within the healthcare research.
High volume of medical data collected across heterogeneous platforms has put a challenge to data scientists for careful integration and implementation. It is therefore suggested that revolution in healthcare is further needed to group together bioinformatics, health informatics and analytics to promote personalized and more effective treatments. Furthermore, new strategies and technologies should be developed to understand the nature (structured, semi-structured, unstructured), complexity (dimensions and attributes) and volume of the data to derive meaningful information. The greatest asset of big data lies in its limitless possibilities. The birth and integration of big data within the past few years has brought substantial advancements in the health care sector ranging from medical data management to drug discovery programs for complex human diseases including cancer and neurodegenerative disorders. To quote a simple example supporting the stated idea, since the late 2000′s the healthcare market has witnessed advancements in the EHR system in the context of data collection, management and usability. We believe that big data will add-on and bolster the existing pipeline of healthcare advances instead of replacing skilled manpower, subject knowledge experts and intellectuals, a notion argued by many. One can clearly see the transitions of health care market from a wider volume base to personalized or individual specific domain. Therefore, it is essential for technologists and professionals to understand this evolving situation. In the coming year it can be projected that big data analytics will march towards a predictive system. This would mean prediction of futuristic outcomes in an individual’s health state based on current or existing data (such as EHR-based and Omics-based). Similarly, it can also be presumed that structured information obtained from a certain geography might lead to generation of population health information. Taken together, big data will facilitate healthcare by introducing prediction of epidemics (in relation to population health), providing early warnings of disease conditions, and helping in the discovery of novel biomarkers and intelligent therapeutic intervention strategies for an improved quality of life.
Availability of data and materials
Not applicable.
References
Laney D. 3D data management: controlling data volume, velocity, and variety, Application delivery strategies. Stamford: META Group Inc; 2001.
Google Scholar
Mauro AD, Greco M, Grimaldi M. A formal definition of big data based on its essential features. Libr Rev. 2016;65(3):122–35.
Article
Google Scholar
Gubbi J, et al. Internet of Things (IoT): a vision, architectural elements, and future directions. Future Gener Comput Syst. 2013;29(7):1645–60.
Article
Google Scholar
Doyle-Lindrud S. The evolution of the electronic health record. Clin J Oncol Nurs. 2015;19(2):153–4.
Article
Google Scholar
Gillum RF. From papyrus to the electronic tablet: a brief history of the clinical medical record with lessons for the digital Age. Am J Med. 2013;126(10):853–7.
Article
Google Scholar
Reiser SJ. The clinical record in medicine part 1: learning from cases*. Ann Intern Med. 1991;114(10):902–7.
Article
Google Scholar
Reisman M. EHRs: the challenge of making electronic data usable and interoperable. Pharm Ther. 2017;42(9):572–5.
Google Scholar
Murphy G, Hanken MA, Waters K. Electronic health records: changing the vision. Philadelphia: Saunders W B Co; 1999. p. 627.
Google Scholar
Shameer K, et al. Translational bioinformatics in the era of real-time biomedical, health care and wellness data streams. Brief Bioinform. 2017;18(1):105–24.
Article
Google Scholar
Service, R.F. The race for the $1000 genome. Science. 2006;311(5767):1544–6.
Article
Google Scholar
Stephens ZD, et al. Big data: astronomical or genomical? PLoS Biol. 2015;13(7):e1002195.
Article
Google Scholar
Yin Y, et al. The internet of things in healthcare: an overview. J Ind Inf Integr. 2016;1:3–13.
Google Scholar
Moore SK. Unhooking medicine [wireless networking]. IEEE Spectr 2001; 38(1): 107–8, 110.
MathSciNet
Google Scholar
Nasi G, Cucciniello M, Guerrazzi C. The role of mobile technologies in health care processes: the case of cancer supportive care. J Med Internet Res. 2015;17(2):e26.
Article
Google Scholar
Apple, ResearchKit/ResearchKit: ResearchKit 1.5.3. 2017.
Shvachko K, et al. The hadoop distributed file system. In: Proceedings of the 2010 IEEE 26th symposium on mass storage systems and technologies (MSST). New York: IEEE Computer Society; 2010. p. 1–10.
Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters. Commun ACM. 2008;51(1):107–13.
Article
Google Scholar
Zaharia M, et al. Apache Spark: a unified engine for big data processing. Commun ACM. 2016;59(11):56–65.
Article
Google Scholar
Gopalani S, Arora R. Comparing Apache Spark and Map Reduce with performance analysis using K-means; 2015.
Article
Google Scholar
Ahmed H, et al. Performance comparison of spark clusters configured conventionally and a cloud servicE. Procedia Comput Sci. 2016;82:99–106.
Article
Google Scholar
Saouabi M, Ezzati A. A comparative between hadoop mapreduce and apache Spark on HDFS. In: Proceedings of the 1st international conference on internet of things and machine learning. Liverpool: ACM; 2017. p. 1–4.
Strickland NH. PACS (picture archiving and communication systems): filmless radiology. Arch Dis Child. 2000;83(1):82–6.
Article
MathSciNet
Google Scholar
Schroeder W, Martin K, Lorensen B. The visualization toolkit. 4th ed. Clifton Park: Kitware; 2006.
Google Scholar
Friston K, et al. Statistical parametric mapping. London: Academic Press; 2007. p. vii.
Google Scholar
Li L, et al. Identification of type 2 diabetes subgroups through topological analysis of patient similarity. Sci Transl Med. 2015;7(311):311ra174.
Article
Google Scholar
Valikodath NG, et al. Agreement of ocular symptom reporting between patient-reported outcomes and medical records. JAMA Ophthalmol. 2017;135(3):225–31.
Article
Google Scholar
Fromme EK, et al. How accurate is clinician reporting of chemotherapy adverse effects? A comparison with patient-reported symptoms from the Quality-of-Life Questionnaire C30. J Clin Oncol. 2004;22(17):3485–90.
Article
Google Scholar
Beckles GL, et al. Agreement between self-reports and medical records was only fair in a cross-sectional study of performance of annual eye examinations among adults with diabetes in managed care. Med Care. 2007;45(9):876–83.
Article
Google Scholar
Echaiz JF, et al. Low correlation between self-report and medical record documentation of urinary tract infection symptoms. Am J Infect Control. 2015;43(9):983–6.
Article
Google Scholar
Belle A, et al. Big data analytics in healthcare. Biomed Res Int. 2015;2015:370194.
Article
Google Scholar
Adler-Milstein J, Pfeifer E. Information blocking: is it occurring and what policy strategies can address it? Milbank Q. 2017;95(1):117–35.
Article
Google Scholar
Or-Bach, Z. A 1,000x improvement in computer systems by bridging the processor-memory gap. In: 2017 IEEE SOI-3D-subthreshold microelectronics technology unified conference (S3S). 2017.
Mahapatra NR, Venkatrao B. The processor-memory bottleneck: problems and solutions. XRDS. 1999;5(3es):2.
Article
Google Scholar
Voronin AA, Panchenko VY, Zheltikov AM. Supercomputations and big-data analysis in strong-field ultrafast optical physics: filamentation of high-peak-power ultrashort laser pulses. Laser Phys Lett. 2016;13(6):065403.
Article
Google Scholar
Dollas, A. Big data processing with FPGA supercomputers: opportunities and challenges. In: 2014 IEEE computer society annual symposium on VLSI; 2014.
Saffman M. Quantum computing with atomic qubits and Rydberg interactions: progress and challenges. J Phys B: At Mol Opt Phys. 2016;49(20):202001.
Article
Google Scholar
Nielsen MA, Chuang IL. Quantum computation and quantum information. 10th anniversary ed. Cambridge: Cambridge University Press; 2011. p. 708.
Google Scholar
Raychev N. Quantum computing models for algebraic applications. Int J Scientific Eng Res. 2015;6(8):1281–8.
Article
Google Scholar
Harrow A. Why now is the right time to study quantum computing. XRDS. 2012;18(3):32–7.
Article
Google Scholar
Lloyd S, Garnerone S, Zanardi P. Quantum algorithms for topological and geometric analysis of data. Nat Commun. 2016;7:10138.
Article
Google Scholar
Buchanan W, Woodward A. Will quantum computers be the end of public key encryption? J Cyber Secur Technol. 2017;1(1):1–22.
Article
Google Scholar
De Domenico M, et al. Structural reducibility of multilayer networks. Nat Commun. 2015;6:6864.
Article
Google Scholar
Mott A, et al. Solving a Higgs optimization problem with quantum annealing for machine learning. Nature. 2017;550:375.
Article
Google Scholar
Rebentrost P, Mohseni M, Lloyd S. Quantum support vector machine for big data classification. Phys Rev Lett. 2014;113(13):130503.
Article
Google Scholar
Gandhi V, et al. Quantum neural network-based EEG filtering for a brain-computer interface. IEEE Trans Neural Netw Learn Syst. 2014;25(2):278–88.
Article
Google Scholar
Nazareth DP, Spaans JD. First application of quantum annealing to IMRT beamlet intensity optimization. Phys Med Biol. 2015;60(10):4137–48.
Article
Google Scholar
Reardon S. Quantum microscope offers MRI for molecules. Nature. 2017;543(7644):162.
Article
Google Scholar
Download references
Acknowledgements
Not applicable.
Funding
None.
Author information
Author notes
Sabyasachi Dash and Sushil Kumar Shakyawar contributed equally to this work
Authors and Affiliations
Department of Pathology and Laboratory Medicine, Weill Cornell Medicine, New York, 10065, NY, USA
Sabyasachi Dash
Center of Biological Engineering, University of Minho, Campus de Gualtar, 4710-057, Braga, Portugal
Sushil Kumar Shakyawar
SilicoLife Lda, Rua do Canastreiro 15, 4715-387, Braga, Portugal
Sushil Kumar Shakyawar
Postgraduate School for Molecular Medicine, Warszawskiego Uniwersytetu Medycznego, Warsaw, Poland
Mohit Sharma
Małopolska Centre for Biotechnology, Jagiellonian University, Kraków, Poland
Mohit Sharma
3B’s Research Group, Headquarters of the European Institute of Excellence on Tissue Engineering and Regenerative Medicine, AvePark - Parque de Ciência e Tecnologia, Zona Industrial da Gandra, Barco, 4805-017, Guimarães, Portugal
Sandeep Kaushik
Authors
Sabyasachi Dash
View author publications
You can also search for this author in
PubMed
Google Scholar
Sushil Kumar Shakyawar
View author publications
You can also search for this author in
PubMed
Google Scholar
Mohit Sharma
View author publications
You can also search for this author in
PubMed
Google Scholar
Sandeep Kaushik
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
MS wrote the manuscript. SD and SKS further added significant discussion that highly improved the quality of manuscript. SK designed the content sequence, guided SD, SS and MS in writing and revising the manuscript and checked the manuscript. All authors read and approved the final manuscript.
Corresponding author
Correspondence to
Sandeep Kaushik
.
Ethics declarations
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (
http://creativecommons.org/licenses/by/4.0/
), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.
Reprints and permissions
About this article
Cite this article
Dash, S., Shakyawar, S.K., Sharma, M.
et al.
Big data in healthcare: management, analysis and future prospects.
J Big Data
6
, 54 (2019). https://doi.org/10.1186/s40537-019-0217-0
Download citation
Received
:
17 January 2019
Accepted
:
06 June 2019
Published
:
19 June 2019
DOI
:
https://doi.org/10.1186/s40537-019-0217-0
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Healthcare
Biomedical research
Big data analytics
Internet of things
Personalized medicine
Quantum computing
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8):
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Download PDF
Download ePub
Download PDF
Download ePub
Survey Paper
Open access
Published:
31 March 2021
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
Laith Alzubaidi
ORCID:
orcid.org/0000-0002-7296-5413
1
,
5
,
Jinglan Zhang
1
,
Amjad J. Humaidi
2
,
Ayad Al-Dujaili
3
,
Ye Duan
4
,
Omran Al-Shamma
5
,
J. Santamaría
6
,
Mohammed A. Fadhel
7
,
Muthana Al-Amidie
4
&
…
Laith Farhan
8
Show authors
Journal of Big Data
volume
8
, Article number:
53
(
2021
)
Cite this article
461k
Accesses
2870
Citations
37
Altmetric
Metrics
details
Abstract
In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.
Introduction
Recently, machine learning (ML) has become very widespread in research and has been incorporated in a variety of applications, including text mining, spam detection, video recommendation, image classification, and multimedia concept retrieval [
1
,
2
,
3
,
4
,
5
,
6
]. Among the different ML algorithms, deep learning (DL) is very commonly employed in these applications [
7
,
8
,
9
]. Another name for DL is representation learning (RL). The continuing appearance of novel studies in the fields of deep and distributed learning is due to both the unpredictable growth in the ability to obtain data and the amazing progress made in the hardware technologies, e.g. High Performance Computing (HPC) [
10
].
DL is derived from the conventional neural network but considerably outperforms its predecessors. Moreover, DL employs transformations and graph technologies simultaneously in order to build up multi-layer learning models. The most recently developed DL techniques have obtained good outstanding performance across a variety of applications, including audio and speech processing, visual data processing, natural language processing (NLP), among others [
11
,
12
,
13
,
14
].
Usually, the effectiveness of an ML algorithm is highly dependent on the integrity of the input-data representation. It has been shown that a suitable data representation provides an improved performance when compared to a poor data representation. Thus, a significant research trend in ML for many years has been feature engineering, which has informed numerous research studies. This approach aims at constructing features from raw data. In addition, it is extremely field-specific and frequently requires sizable human effort. For instance, several types of features were introduced and compared in the computer vision context, such as, histogram of oriented gradients (HOG) [
15
], scale-invariant feature transform (SIFT) [
16
], and bag of words (BoW) [
17
]. As soon as a novel feature is introduced and is found to perform well, it becomes a new research direction that is pursued over multiple decades.
Relatively speaking, feature extraction is achieved in an automatic way throughout the DL algorithms. This encourages researchers to extract discriminative features using the smallest possible amount of human effort and field knowledge [
18
]. These algorithms have a multi-layer data representation architecture, in which the first layers extract the low-level features while the last layers extract the high-level features. Note that artificial intelligence (AI) originally inspired this type of architecture, which simulates the process that occurs in core sensorial regions within the human brain. Using different scenes, the human brain can automatically extract data representation. More specifically, the output of this process is the classified objects, while the received scene information represents the input. This process simulates the working methodology of the human brain. Thus, it emphasizes the main benefit of DL.
In the field of ML, DL, due to its considerable success, is currently one of the most prominent research trends. In this paper, an overview of DL is presented that adopts various perspectives such as the main concepts, architectures, challenges, applications, computational tools and evolution matrix. Convolutional neural network (CNN) is one of the most popular and used of DL networks [
19
,
20
]. Because of CNN, DL is very popular nowadays. The main advantage of CNN compared to its predecessors is that it automatically detects the significant features without any human supervision which made it the most used. Therefore, we have dug in deep with CNN by presenting the main components of it. Furthermore, we have elaborated in detail the most common CNN architectures, starting with the AlexNet network and ending with the High-Resolution network (HR.Net).
Several published DL review papers have been presented in the last few years. However, all of them have only been addressed one side focusing on one application or topic such as the review of CNN architectures [
21
], DL for classification of plant diseases [
22
], DL for object detection [
23
], DL applications in medical image analysis [
24
], and etc. Although these reviews present good topics, they do not provide a full understanding of DL topics such as concepts, detailed research gaps, computational tools, and DL applications. First, It is required to understand DL aspects including concepts, challenges, and applications then going deep in the applications. To achieve that, it requires extensive time and a large number of research papers to learn about DL including research gaps and applications. Therefore, we propose a deep review of DL to provide a more suitable starting point from which to develop a full understanding of DL from one review paper. The motivation behinds our review was to cover the most important aspect of DL including open challenges, applications, and computational tools perspective. Furthermore, our review can be the first step towards other DL topics.
The main aim of this review is to present the most important aspects of DL to make it easy for researchers and students to have a clear image of DL from single review paper. This review will further advance DL research by helping people discover more about recent developments in the field. Researchers would be allowed to decide the more suitable direction of work to be taken in order to provide more accurate alternatives to the field. Our contributions are outlined as follows:
This is the first review that almost provides a deep survey of the most important aspects of deep learning. This review helps researchers and students to have a good understanding from one paper.
We explain CNN in deep which the most popular deep learning algorithm by describing the concepts, theory, and state-of-the-art architectures.
We review current challenges (limitations) of Deep Learning including lack of training data, Imbalanced Data, Interpretability of data, Uncertainty scaling, Catastrophic forgetting, Model compression, Overfitting, Vanishing gradient problem, Exploding Gradient Problem, and Underspecification. We additionally discuss the proposed solutions tackling these issues.
We provide an exhaustive list of medical imaging applications with deep learning by categorizing them based on the tasks by starting with classification and ending with registration.
We discuss the computational approaches (CPU, GPU, FPGA) by comparing the influence of each tool on deep learning algorithms.
The rest of the paper is organized as follows: “
Survey methodology
” section describes The survey methodology. “
Background
” section presents the background. “
Classification of DL approaches
” section defines the classification of DL approaches. “
Types of DL networks
” section displays types of DL networks. “
CNN architectures
” section shows CNN Architectures. “
Challenges (limitations) of deep learning and alternate solutions
” section details the challenges of DL and alternate solutions. “
Applications of deep learning
” section outlines the applications of DL. “
Computational approaches
” section explains the influence of computational approaches (CPU, GPU, FPGA) on DL. “
Evaluation metrics
” section presents the evaluation metrics. “
Frameworks and datasets
” section lists frameworks and datasets. “
Summary and conclusion
” section presents the summary and conclusion.
Survey methodology
We have reviewed the significant research papers in the field published during 2010–2020, mainly from the years of 2020 and 2019 with some papers from 2021. The main focus was papers from the most reputed publishers such as IEEE, Elsevier, MDPI, Nature, ACM, and Springer. Some papers have been selected from ArXiv. We have reviewed more than 300 papers on various DL topics. There are 108 papers from the year 2020, 76 papers from the year 2019, and 48 papers from the year 2018. This indicates that this review focused on the latest publications in the field of DL. The selected papers were analyzed and reviewed to (1) list and define the DL approaches and network types, (2) list and explain CNN architectures, (3) present the challenges of DL and suggest the alternate solutions, (4) assess the applications of DL, (5) assess computational approaches. The most keywords used for search criteria for this review paper are (“Deep Learning”), (“Machine Learning”), (“Convolution Neural Network”), (“Deep Learning” AND “Architectures”), ((“Deep Learning”) AND (“Image”) AND (“detection” OR “classification” OR “segmentation” OR “Localization”)), (“Deep Learning” AND “detection” OR “classification” OR “segmentation” OR “Localization”), (“Deep Learning” AND “CPU” OR “GPU” OR “FPGA”), (“Deep Learning” AND “Transfer Learning”), (“Deep Learning” AND “Imbalanced Data”), (“Deep Learning” AND “Interpretability of data”), (“Deep Learning” AND “Overfitting”), (“Deep Learning” AND “Underspecification”). Figure
1
shows our search structure of the survey paper. Table
1
presents the details of some of the journals that have been cited in this review paper.
Fig. 1
Search framework
Full size image
Table 1 Some of the journals have been cited in this review paper
Full size table
Background
This section will present a background of DL. We begin with a quick introduction to DL, followed by the difference between DL and ML. We then show the situations that require DL. Finally, we present the reasons for applying DL.
DL, a subset of ML (Fig.
2
), is inspired by the information processing patterns found in the human brain. DL does not require any human-designed rules to operate; rather, it uses a large amount of data to map the given input to specific labels. DL is designed using numerous layers of algorithms (artificial neural networks, or ANNs), each of which provides a different interpretation of the data that has been fed to them [
18
,
25
].
Fig. 2
Deep learning family
Full size image
Achieving the classification task using conventional ML techniques requires several sequential steps, specifically pre-processing, feature extraction, wise feature selection, learning, and classification. Furthermore, feature selection has a great impact on the performance of ML techniques. Biased feature selection may lead to incorrect discrimination between classes. Conversely, DL has the ability to automate the learning of feature sets for several tasks, unlike conventional ML methods [
18
,
26
]. DL enables learning and classification to be achieved in a single shot (Fig.
3
). DL has become an incredibly popular type of ML algorithm in recent years due to the huge growth and evolution of the field of big data [
27
,
28
]. It is still in continuous development regarding novel performance for several ML tasks [
22
,
29
,
30
,
31
] and has simplified the improvement of many learning fields [
32
,
33
], such as image super-resolution [
34
], object detection [
35
,
36
], and image recognition [
30
,
37
]. Recently, DL performance has come to exceed human performance on tasks such as image classification (Fig.
4
).
Fig. 3
The difference between deep learning and traditional machine learning
Full size image
Fig. 4
Deep learning performance compared to human
Full size image
Nearly all scientific fields have felt the impact of this technology. Most industries and businesses have already been disrupted and transformed through the use of DL. The leading technology and economy-focused companies around the world are in a race to improve DL. Even now, human-level performance and capability cannot exceed that the performance of DL in many areas, such as predicting the time taken to make car deliveries, decisions to certify loan requests, and predicting movie ratings [
38
]. The winners of the 2019 “Nobel Prize” in computing, also known as the Turing Award, were three pioneers in the field of DL (Yann LeCun, Geoffrey Hinton, and Yoshua Bengio) [
39
]. Although a large number of goals have been achieved, there is further progress to be made in the DL context. In fact, DL has the ability to enhance human lives by providing additional accuracy in diagnosis, including estimating natural disasters [
40
], the discovery of new drugs [
41
], and cancer diagnosis [
42
,
43
,
44
]. Esteva et al. [
45
] found that a DL network has the same ability to diagnose the disease as twenty-one board-certified dermatologists using 129,450 images of 2032 diseases. Furthermore, in grading prostate cancer, US board-certified general pathologists achieved an average accuracy of 61%, while the Google AI [
44
] outperformed these specialists by achieving an average accuracy of 70%. In 2020, DL is playing an increasingly vital role in early diagnosis of the novel coronavirus (COVID-19) [
29
,
46
,
47
,
48
]. DL has become the main tool in many hospitals around the world for automatic COVID-19 classification and detection using chest X-ray images or other types of images. We end this section by the saying of AI pioneer Geoffrey Hinton “Deep learning is going to be able to do everything”.
When to apply deep learning
Machine intelligence is useful in many situations which is equal or better than human experts in some cases [
49
,
50
,
51
,
52
], meaning that DL can be a solution to the following problems:
Cases where human experts are not available.
Cases where humans are unable to explain decisions made using their expertise (language understanding, medical decisions, and speech recognition).
Cases where the problem solution updates over time (price prediction, stock preference, weather prediction, and tracking).
Cases where solutions require adaptation based on specific cases (personalization, biometrics).
Cases where size of the problem is extremely large and exceeds our inadequate reasoning abilities (sentiment analysis, matching ads to Facebook, calculation webpage ranks).
Why deep learning?
Several performance features may answer this question, e.g
1.
Universal Learning Approach: Because DL has the ability to perform in approximately all application domains, it is sometimes referred to as universal learning.
2.
Robustness: In general, precisely designed features are not required in DL techniques. Instead, the optimized features are learned in an automated fashion related to the task under consideration. Thus, robustness to the usual changes of the input data is attained.
3.
Generalization: Different data types or different applications can use the same DL technique, an approach frequently referred to as transfer learning (TL) which explained in the latter section. Furthermore, it is a useful approach in problems where data is insufficient.
4.
Scalability: DL is highly scalable. ResNet [
37
], which was invented by Microsoft, comprises 1202 layers and is frequently applied at a supercomputing scale. Lawrence Livermore National Laboratory (LLNL), a large enterprise working on evolving frameworks for networks, adopted a similar approach, where thousands of nodes can be implemented [
53
].
Classification of DL approaches
DL techniques are classified into three major categories: unsupervised, partially supervised (semi-supervised) and supervised. Furthermore, deep reinforcement learning (DRL), also known as RL, is another type of learning technique, which is mostly considered to fall into the category of partially supervised (and occasionally unsupervised) learning techniques.
Deep supervised learning
This technique deals with labeled data. When considering such a technique, the environs have a collection of inputs and resultant outputs
\((x_t,y_t)\sim \rho \)
. For instance, the smart agent guesses
if the input is xt and will obtain
as a loss value. Next, the network parameters are repeatedly updated by the agent to obtain an improved estimate for the preferred outputs. Following a positive training outcome, the agent acquires the ability to obtain the right solutions to the queries from the environs. For DL, there are several supervised learning techniques, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and deep neural networks (DNNs). In addition, the RNN category includes gated recurrent units (GRUs) and long short-term memory (LSTM) approaches. The main advantage of this technique is the ability to collect data or generate a data output from the prior knowledge. However, the disadvantage of this technique is that decision boundary might be overstrained when training set doesn’t own samples that should be in a class. Overall, this technique is simpler than other techniques in the way of learning with high performance.
Deep semi-supervised learning
In this technique, the learning process is based on semi-labeled datasets. Occasionally, generative adversarial networks (GANs) and DRL are employed in the same way as this technique. In addition, RNNs, which include GRUs and LSTMs, are also employed for partially supervised learning. One of the advantages of this technique is to minimize the amount of labeled data needed. On other the hand, One of the disadvantages of this technique is irrelevant input feature present training data could furnish incorrect decisions. Text document classifier is one of the most popular example of an application of semi-supervised learning. Due to difficulty of obtaining a large amount of labeled text documents, semi-supervised learning is ideal for text document classification task.
Deep unsupervised learning
This technique makes it possible to implement the learning process in the absence of available labeled data (i.e. no labels are required). Here, the agent learns the significant features or interior representation required to discover the unidentified structure or relationships in the input data. Techniques of generative networks, dimensionality reduction and clustering are frequently counted within the category of unsupervised learning. Several members of the DL family have performed well on non-linear dimensionality reduction and clustering tasks; these include restricted Boltzmann machines, auto-encoders and GANs as the most recently developed techniques. Moreover, RNNs, which include GRUs and LSTM approaches, have also been employed for unsupervised learning in a wide range of applications. The main disadvantages of unsupervised learning are unable to provide accurate information concerning data sorting and computationally complex. One of the most popular unsupervised learning approaches is clustering [
54
].
Deep reinforcement learning
Reinforcement Learning operates on interacting with the environment, while supervised learning operates on provided sample data. This technique was developed in 2013 with Google Deep Mind [
55
]. Subsequently, many enhanced techniques dependent on reinforcement learning were constructed. For example, if the input environment samples:
\(x_t\sim \rho \)
, agent predict:
and the received cost of the agent is
, P here is the unknown probability distribution, then the environment asks a question to the agent. The answer it gives is a noisy score. This method is sometimes referred to as semi-supervised learning. Based on this concept, several supervised and unsupervised techniques were developed. In comparison with traditional supervised techniques, performing this learning is much more difficult, as no straightforward loss function is available in the reinforcement learning technique. In addition, there are two essential differences between supervised learning and reinforcement learning: first, there is no complete access to the function, which requires optimization, meaning that it should be queried via interaction; second, the state being interacted with is founded on an environment, where the input
\(x_t\)
is based on the preceding actions [
9
,
56
].
For solving a task, the selection of the type of reinforcement learning that needs to be performed is based on the space or the scope of the problem. For example, DRL is the best way for problems involving many parameters to be optimized. By contrast, derivative-free reinforcement learning is a technique that performs well for problems with limited parameters. Some of the applications of reinforcement learning are business strategy planning and robotics for industrial automation. The main drawback of Reinforcement Learning is that parameters may influence the speed of learning. Here are the main motivations for utilizing Reinforcement Learning:
It assists you to identify which action produces the highest reward over a longer period.
It assists you to discover which situation requires action.
It also enables it to figure out the best approach for reaching large rewards.
Reinforcement Learning also gives the learning agent a reward function.
Reinforcement Learning can’t utilize in all the situation such as:
In case there is sufficient data to resolve the issue with supervised learning techniques.
Reinforcement Learning is computing-heavy and time-consuming. Specially when the workspace is large.
Types of DL networks
The most famous types of deep learning networks are discussed in this section: these include recursive neural networks (RvNNs), RNNs, and CNNs. RvNNs and RNNs were briefly explained in this section while CNNs were explained in deep due to the importance of this type. Furthermore, it is the most used in several applications among other networks.
Recursive neural networks
RvNN can achieve predictions in a hierarchical structure also classify the outputs utilizing compositional vectors [
57
]. Recursive auto-associative memory (RAAM) [
58
] is the primary inspiration for the RvNN development. The RvNN architecture is generated for processing objects, which have randomly shaped structures like graphs or trees. This approach generates a fixed-width distributed representation from a variable-size recursive-data structure. The network is trained using an introduced back-propagation through structure (BTS) learning system [
58
]. The BTS system tracks the same technique as the general-back propagation algorithm and has the ability to support a treelike structure. Auto-association trains the network to regenerate the input-layer pattern at the output layer. RvNN is highly effective in the NLP context. Socher et al.  [
59
] introduced RvNN architecture designed to process inputs from a variety of modalities. These authors demonstrate two applications for classifying natural language sentences: cases where each sentence is split into words and nature images, and cases where each image is separated into various segments of interest. RvNN computes a likely pair of scores for merging and constructs a syntactic tree. Furthermore, RvNN calculates a score related to the merge plausibility for every pair of units. Next, the pair with the largest score is merged within a composition vector. Following every merge, RvNN generates (a) a larger area of numerous units, (b) a compositional vector of the area, and (c) a label for the class (for instance, a noun phrase will become the class label for the new area if two units are noun words). The compositional vector for the entire area is the root of the RvNN tree structure. An example RvNN tree is shown in Fig.
5
. RvNN has been employed in several applications [
60
,
61
,
62
].
Fig. 5
An example of RvNN tree
Full size image
Recurrent neural networks
RNNs are a commonly employed and familiar algorithm in the discipline of DL [
63
,
64
,
65
]. RNN is mainly applied in the area of speech processing and NLP contexts [
66
,
67
]. Unlike conventional networks, RNN uses sequential data in the network. Since the embedded structure in the sequence of the data delivers valuable information, this feature is fundamental to a range of different applications. For instance, it is important to understand the context of the sentence in order to determine the meaning of a specific word in it. Thus, it is possible to consider the RNN as a unit of short-term memory, where x represents the input layer, y is the output layer, and s represents the state (hidden) layer. For a given input sequence, a typical unfolded RNN diagram is illustrated in Fig.
6
. Pascanu et al. [
68
] introduced three different types of deep RNN techniques, namely “Hidden-to-Hidden”, “Hidden-to-Output”, and “Input-to-Hidden”. A deep RNN is introduced that lessens the learning difficulty in the deep network and brings the benefits of a deeper RNN based on these three techniques.
Fig. 6
Typical unfolded RNN diagram
Full size image
However, RNN’s sensitivity to the exploding gradient and vanishing problems represent one of the main issues with this approach [
69
]. More specifically, during the training process, the reduplications of several large or small derivatives may cause the gradients to exponentially explode or decay. With the entrance of new inputs, the network stops thinking about the initial ones; therefore, this sensitivity decays over time. Furthermore, this issue can be handled using LSTM [
70
]. This approach offers recurrent connections to memory blocks in the network. Every memory block contains a number of memory cells, which have the ability to store the temporal states of the network. In addition, it contains gated units for controlling the flow of information. In very deep networks [
37
], residual connections also have the ability to considerably reduce the impact of the vanishing gradient issue which explained in later sections. CNN is considered to be more powerful than RNN. RNN includes less feature compatibility when compared to CNN.
Convolutional neural networks
In the field of DL, the CNN is the most famous and commonly employed algorithm [
30
,
71
,
72
,
73
,
74
,
75
]. The main benefit of CNN compared to its predecessors is that it automatically identifies the relevant features without any human supervision [
76
]. CNNs have been extensively applied in a range of different fields, including computer vision [
77
], speech processing [
78
], Face Recognition [
79
], etc. The structure of CNNs was inspired by neurons in human and animal brains, similar to a conventional neural network. More specifically, in a cat’s brain, a complex sequence of cells forms the visual cortex; this sequence is simulated by the CNN [
80
]. Goodfellow et al. [
28
] identified three key benefits of the CNN: equivalent representations, sparse interactions, and parameter sharing. Unlike conventional fully connected (FC) networks, shared weights and local connections in the CNN are employed to make full use of 2D input-data structures like image signals. This operation utilizes an extremely small number of parameters, which both simplifies the training process and speeds up the network. This is the same as in the visual cortex cells. Notably, only small regions of a scene are sensed by these cells rather than the whole scene (i.e., these cells spatially extract the local correlation available in the input, like local filters over the input).
A commonly used type of CNN, which is similar to the multi-layer perceptron (MLP), consists of numerous convolution layers preceding sub-sampling (pooling) layers, while the ending layers are FC layers. An example of CNN architecture for image classification is illustrated in Fig.
7
.
Fig. 7
An example of CNN architecture for image classification
Full size image
The input
x
of each layer in a CNN model is organized in three dimensions: height, width, and depth, or
\(m \times m \times r\)
, where the height (m) is equal to the width. The depth is also referred to as the channel number. For example, in an RGB image, the depth (r) is equal to three. Several kernels (filters) available in each convolutional layer are denoted by
k
and also have three dimensions (
\(n \times n \times q\)
), similar to the input image; here, however,
n
must be smaller than
m
, while
q
is either equal to or smaller than
r
. In addition, the kernels are the basis of the local connections, which share similar parameters (bias
\(b^{k}\)
and weight
\(W^{k}\)
) for generating
k
feature maps
\(h^{k}\)
with a size of (
\(m-n-1\)
) each and are convolved with input, as mentioned above. The convolution layer calculates a dot product between its input and the weights as in Eq.
1
, similar to NLP, but the inputs are undersized areas of the initial image size. Next, by applying the nonlinearity or an activation function to the convolution-layer output, we obtain the following:
$$ h^{k}= f(W^{k}*x+ b^{k} )$$
(1)
The next step is down-sampling every feature map in the sub-sampling layers. This leads to a reduction in the network parameters, which accelerates the training process and in turn enables handling of the overfitting issue. For all feature maps, the pooling function (e.g. max or average) is applied to an adjacent area of size
\(p \times p\)
, where
p
is the kernel size. Finally, the FC layers receive the mid- and low-level features and create the high-level abstraction, which represents the last-stage layers as in a typical neural network. The classification scores are generated using the ending layer [e.g. support vector machines (SVMs) or softmax]. For a given instance, every score represents the probability of a specific class.
Benefits of employing CNNs
The benefits of using CNNs over other traditional neural networks in the computer vision environment are listed as follows:
1.
The main reason to consider CNN is the weight sharing feature, which reduces the number of trainable network parameters and in turn helps the network to enhance generalization and to avoid overfitting.
2.
Concurrently learning the feature extraction layers and the classification layer causes the model output to be both highly organized and highly reliant on the extracted features.
3.
Large-scale network implementation is much easier with CNN than with other neural networks.
CNN layers
The CNN architecture consists of a number of layers (or so-called multi-building blocks). Each layer in the CNN architecture, including its function, is described in detail below.
1.
Convolutional Layer: In CNN architecture, the most significant component is the convolutional layer. It consists of a collection of convolutional filters (so-called kernels). The input image, expressed as N-dimensional metrics, is convolved with these filters to generate the output feature map.
Kernel definition: A grid of discrete numbers or values describes the kernel. Each value is called the kernel weight. Random numbers are assigned to act as the weights of the kernel at the beginning of the CNN training process. In addition, there are several different methods used to initialize the weights. Next, these weights are adjusted at each training era; thus, the kernel learns to extract significant features.
Convolutional Operation: Initially, the CNN input format is described. The vector format is the input of the traditional neural network, while the multi-channeled image is the input of the CNN. For instance, single-channel is the format of the gray-scale image, while the RGB image format is three-channeled. To understand the convolutional operation, let us take an example of a
\(4 \times 4\)
gray-scale image with a
\(2 \times 2\)
random weight-initialized kernel. First, the kernel slides over the whole image horizontally and vertically. In addition, the dot product between the input image and the kernel is determined, where their corresponding values are multiplied and then summed up to create a single scalar value, calculated concurrently. The whole process is then repeated until no further sliding is possible. Note that the calculated dot product values represent the feature map of the output. Figure
8
graphically illustrates the primary calculations executed at each step. In this figure, the light green color represents the
\(2 \times 2\)
kernel, while the light blue color represents the similar size area of the input image. Both are multiplied; the end result after summing up the resulting product values (marked in a light orange color) represents an entry value to the output feature map.
Fig. 8
The primary calculations executed at each step of convolutional layer
Full size image
However, padding to the input image is not applied in the previous example, while a stride of one (denoted for the selected step-size over all vertical or horizontal locations) is applied to the kernel. Note that it is also possible to use another stride value. In addition, a feature map of lower dimensions is obtained as a result of increasing the stride value.
On the other hand, padding is highly significant to determining border size information related to the input image. By contrast, the border side-features moves carried away very fast. By applying padding, the size of the input image will increase, and in turn, the size of the output feature map will also increase. Core Benefits of Convolutional Layers.
Sparse Connectivity: Each neuron of a layer in FC neural networks links with all neurons in the following layer. By contrast, in CNNs, only a few weights are available between two adjacent layers. Thus, the number of required weights or connections is small, while the memory required to store these weights is also small; hence, this approach is memory-effective. In addition, matrix operation is computationally much more costly than the dot (.) operation in CNN.
Weight Sharing: There are no allocated weights between any two neurons of neighboring layers in CNN, as the whole weights operate with one and all pixels of the input matrix. Learning a single group of weights for the whole input will significantly decrease the required training time and various costs, as it is not necessary to learn additional weights for each neuron.
2.
Pooling Layer: The main task of the pooling layer is the sub-sampling of the feature maps. These maps are generated by following the convolutional operations. In other words, this approach shrinks large-size feature maps to create smaller feature maps. Concurrently, it maintains the majority of the dominant information (or features) in every step of the pooling stage. In a similar manner to the convolutional operation, both the stride and the kernel are initially size-assigned before the pooling operation is executed. Several types of pooling methods are available for utilization in various pooling layers. These methods include tree pooling, gated pooling, average pooling, min pooling, max pooling, global average pooling (GAP), and global max pooling. The most familiar and frequently utilized pooling methods are the max, min, and GAP pooling. Figure
9
illustrates these three pooling operations.
Fig. 9
Three types of pooling operations
Full size image
Sometimes, the overall CNN performance is decreased as a result; this represents the main shortfall of the pooling layer, as this layer helps the CNN to determine whether or not a certain feature is available in the particular input image, but focuses exclusively on ascertaining the correct location of that feature. Thus, the CNN model misses the relevant information.
3.
Activation Function (non-linearity) Mapping the input to the output is the core function of all types of activation function in all types of neural network. The input value is determined by computing the weighted summation of the neuron input along with its bias (if present). This means that the activation function makes the decision as to whether or not to fire a neuron with reference to a particular input by creating the corresponding output.
Non-linear activation layers are employed after all layers with weights (so-called learnable layers, such as FC layers and convolutional layers) in CNN architecture. This non-linear performance of the activation layers means that the mapping of input to output will be non-linear; moreover, these layers give the CNN the ability to learn extra-complicated things. The activation function must also have the ability to differentiate, which is an extremely significant feature, as it allows error back-propagation to be used to train the network. The following types of activation functions are most commonly used in CNN and other deep neural networks.
Sigmoid: The input of this activation function is real numbers, while the output is restricted to between zero and one. The sigmoid function curve is S-shaped and can be represented mathematically by Eq.
2
.
$$ f(x)_{sigm}=\frac{1}{1+e^{-x}} $$
(2)
Tanh: It is similar to the sigmoid function, as its input is real numbers, but the output is restricted to between − 1 and 1. Its mathematical representation is in Eq.
3
.
$$ f(x)_{tanh}=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$
(3)
ReLU: The mostly commonly used function in the CNN context. It converts the whole values of the input to positive numbers. Lower computational load is the main benefit of ReLU over the others. Its mathematical representation is in Eq.
4
.
$$ f(x)_{ReLU}= max(0,x) $$
(4)
Occasionally, a few significant issues may occur during the use of ReLU. For instance, consider an error back-propagation algorithm with a larger gradient flowing through it. Passing this gradient within the ReLU function will update the weights in a way that makes the neuron certainly not activated once more. This issue is referred to as “Dying ReLU”. Some ReLU alternatives exist to solve such issues. The following discusses some of them.
Leaky ReLU: Instead of ReLU down-scaling the negative inputs, this activation function ensures these inputs are never ignored. It is employed to solve the Dying ReLU problem. Leaky ReLU can be represented mathematically as in Eq.
5
.
$$\begin{aligned} f(x)_{Leaky ReLU}= \left \{ \begin{array}{ll} x,& if \quad  x > 0\\ mx,& x \le 0 \end{array} \right \} \end{aligned}$$
(5)
Note that the leak factor is denoted by m. It is commonly set to a very small value, such as 0.001.
Noisy ReLU: This function employs a Gaussian distribution to make ReLU noisy. It can be represented mathematically as in Eq.
6
.
$$ f(x)_{Noisy ReLU}= max(x+Y),with\, Y \sim N (0,\sigma (x)) $$
(6)
Parametric Linear Units: This is mostly the same as Leaky ReLU. The main difference is that the leak factor in this function is updated through the model training process. The parametric linear unit can be represented mathematically as in Eq.
7
.
$$\begin{aligned} f(x)_{ Parametric Linear}=\begin{Bmatrix} x,& if\; x >0\\ ax,& x \le 0 \end{Bmatrix} \end{aligned}$$
(7)
Note that the learnable weight is denoted as a.
4.
Fully Connected Layer: Commonly, this layer is located at the end of each CNN architecture. Inside this layer, each neuron is connected to all neurons of the previous layer, the so-called Fully Connected (FC) approach. It is utilized as the CNN classifier. It follows the basic method of the conventional multiple-layer perceptron neural network, as it is a type of feed-forward ANN. The input of the FC layer comes from the last pooling or convolutional layer. This input is in the form of a vector, which is created from the feature maps after flattening. The output of the FC layer represents the final CNN output, as illustrated in Fig.
10
.
Fig. 10
Fully connected layer
Full size image
5.
Loss Functions: The previous section has presented various layer-types of CNN architecture. In addition, the final classification is achieved from the output layer, which represents the last layer of the CNN architecture. Some loss functions are utilized in the output layer to calculate the predicted error created across the training samples in the CNN model. This error reveals the difference between the actual output and the predicted one. Next, it will be optimized through the CNN learning process.
However, two parameters are used by the loss function to calculate the error. The CNN estimated output (referred to as the prediction) is the first parameter. The actual output (referred to as the label) is the second parameter. Several types of loss function are employed in various problem types. The following concisely explains some of the loss function types.
(a)
Cross-Entropy or Softmax Loss Function: This function is commonly employed for measuring the CNN model performance. It is also referred to as the log loss function. Its output is the probability
\(p \in \left\{ 0\left. , 1 \right\} \right. \)
. In addition, it is usually employed as a substitution of the square error loss function in multi-class classification problems. In the output layer, it employs the softmax activations to generate the output within a probability distribution. The mathematical representation of the output class probability is Eq.
8
.
$$ p_{i}= \frac{e^{a_{i}}}{\sum _{k=1}^{N} e^{a}_{_{k}}}$$
(8)
Here,
\(e^{a_{i}}\)
represents the non-normalized output from the preceding layer, while
N
represents the number of neurons in the output layer. Finally, the mathematical representation of cross-entropy loss function is Eq.
9
.
$$ H(p,y)=-\sum _{i}^{} y_{i}\log (p_{i}) \quad where \quad i \in [1,N] $$
(9)
(b)
Euclidean Loss Function: This function is widely used in regression problems. In addition, it is also the so-called mean square error. The mathematical expression of the estimated Euclidean loss is Eq.
10
.
$$ H(p,y)=\frac{1}{2N}\sum _{i=1}^{N} (p_{i}-y_{i})^{2} $$
(10)
(c)
Hinge Loss Function: This function is commonly employed in problems related to binary classification. This problem relates to maximum-margin-based classification; this is mostly important for SVMs, which use the hinge loss function, wherein the optimizer attempts to maximize the margin around dual objective classes. Its mathematical formula is Eq.
11
.
$$ H(p,y)=\sum _{i=1}^{N} max (0,m-(2y_{i}-1)p_{_{i}}) $$
(11)
The margin
m
is commonly set to 1. Moreover, the predicted output is denoted as
\(p_{_{i}}\)
, while the desired output is denoted as
\(y_{_{i}}\)
.
Regularization to CNN
For CNN models, over-fitting represents the central issue associated with obtaining well-behaved generalization. The model is entitled over-fitted in cases where the model executes especially well on training data and does not succeed on test data (unseen data) which is more explained in the latter section. An under-fitted model is the opposite; this case occurs when the model does not learn a sufficient amount from the training data. The model is referred to as “just-fitted” if it executes well on both training and testing data. These three types are illustrated in Fig.
11
. Various intuitive concepts are used to help the regularization to avoid over-fitting; more details about over-fitting and under-fitting are discussed in latter sections.
1.
Dropout: This is a widely utilized technique for generalization. During each training epoch, neurons are randomly dropped. In doing this, the feature selection power is distributed equally across the whole group of neurons, as well as forcing the model to learn different independent features. During the training process, the dropped neuron will not be a part of back-propagation or forward-propagation. By contrast, the full-scale network is utilized to perform prediction during the testing process.
2.
Drop-Weights: This method is highly similar to dropout. In each training epoch, the connections between neurons (weights) are dropped rather than dropping the neurons; this represents the only difference between drop-weights and dropout.
3.
Data Augmentation: Training the model on a sizeable amount of data is the easiest way to avoid over-fitting. To achieve this, data augmentation is used. Several techniques are utilized to artificially expand the size of the training dataset. More details can be found in the latter section, which describes the data augmentation techniques.
4.
Batch Normalization: This method ensures the performance of the output activations [
81
]. This performance follows a unit Gaussian distribution. Subtracting the mean and dividing by the standard deviation will normalize the output at each layer. While it is possible to consider this as a pre-processing task at each layer in the network, it is also possible to differentiate and to integrate it with other networks. In addition, it is employed to reduce the “internal covariance shift” of the activation layers. In each layer, the variation in the activation distribution defines the internal covariance shift. This shift becomes very high due to the continuous weight updating through training, which may occur if the samples of the training data are gathered from numerous dissimilar sources (for example, day and night images). Thus, the model will consume extra time for convergence, and in turn, the time required for training will also increase. To resolve this issue, a layer representing the operation of batch normalization is applied in the CNN architecture.
The advantages of utilizing batch normalization are as follows:
It prevents the problem of vanishing gradient from arising.
It can effectively control the poor weight initialization.
It significantly reduces the time required for network convergence (for large-scale datasets, this will be extremely useful).
It struggles to decrease training dependency across hyper-parameters.
Chances of over-fitting are reduced, since it has a minor influence on regularization.
Fig. 11
Over-fitting and under-fitting issues
Full size image
Optimizer selection
This section discusses the CNN learning process. Two major issues are included in the learning process: the first issue is the learning algorithm selection (optimizer), while the second issue is the use of many enhancements (such as AdaDelta, Adagrad, and momentum) along with the learning algorithm to enhance the output.
Loss functions, which are founded on numerous learnable parameters (e.g. biases, weights, etc.) or minimizing the error (variation between actual and predicted output), are the core purpose of all supervised learning algorithms. The techniques of gradient-based learning for a CNN network appear as the usual selection. The network parameters should always update though all training epochs, while the network should also look for the locally optimized answer in all training epochs in order to minimize the error.
The learning rate is defined as the step size of the parameter updating. The training epoch represents a complete repetition of the parameter update that involves the complete training dataset at one time. Note that it needs to select the learning rate wisely so that it does not influence the learning process imperfectly, although it is a hyper-parameter.
Gradient Descent or Gradient-based learning algorithm: To minimize the training error, this algorithm repetitively updates the network parameters through every training epoch. More specifically, to update the parameters correctly, it needs to compute the objective function gradient (slope) by applying a first-order derivative with respect to the network parameters. Next, the parameter is updated in the reverse direction of the gradient to reduce the error. The parameter updating process is performed though network back-propagation, in which the gradient at every neuron is back-propagated to all neurons in the preceding layer. The mathematical representation of this operation is as Eq.
12
.
$$ w_{i j^{t}}=w_{i j^{t-1}}-\Delta w_{i j^{t}},\quad \Delta w_{i j^{t}}=\eta *\frac{\partial E}{\partial w_{i j}} $$
(12)
The final weight in the current training epoch is denoted by
\(w_{i j^{t}}\)
, while the weight in the preceding
\((t-1)\)
training epoch is denoted
\(w_{i j^{t-1}}\)
. The learning rate is
\(\eta \)
and the prediction error is
E
. Different alternatives of the gradient-based learning algorithm are available and commonly employed; these include the following:
1.
Batch Gradient Descent: During the execution of this technique [
82
], the network parameters are updated merely one time behind considering all training datasets via the network. In more depth, it calculates the gradient of the whole training set and subsequently uses this gradient to update the parameters. For a small-sized dataset, the CNN model converges faster and creates an extra-stable gradient using BGD. Since the parameters are changed only once for every training epoch, it requires a substantial amount of resources. By contrast, for a large training dataset, additional time is required for converging, and it could converge to a local optimum (for non-convex instances).
2.
Stochastic Gradient Descent: The parameters are updated at each training sample in this technique [
83
]. It is preferred to arbitrarily sample the training samples in every epoch in advance of training. For a large-sized training dataset, this technique is both more memory-effective and much faster than BGD. However, because it is frequently updated, it takes extremely noisy steps in the direction of the answer, which in turn causes the convergence behavior to become highly unstable.
3.
Mini-batch Gradient Descent: In this approach, the training samples are partitioned into several mini-batches, in which every mini-batch can be considered an under-sized collection of samples with no overlap between them [
84
]. Next, parameter updating is performed following gradient computation on every mini-batch. The advantage of this method comes from combining the advantages of both BGD and SGD techniques. Thus, it has a steady convergence, more computational efficiency and extra memory effectiveness. The following describes several enhancement techniques in gradient-based learning algorithms (usually in SGD), which further powerfully enhance the CNN training process.
4.
Momentum: For neural networks, this technique is employed in the objective function. It enhances both the accuracy and the training speed by summing the computed gradient at the preceding training step, which is weighted via a factor
\(\lambda \)
(known as the momentum factor). However, it therefore simply becomes stuck in a local minimum rather than a global minimum. This represents the main disadvantage of gradient-based learning algorithms. Issues of this kind frequently occur if the issue has no convex surface (or solution space).
Together with the learning algorithm, momentum is used to solve this issue, which can be expressed mathematically as in Eq.
13
.
$$ \Delta w_{i j^{t}}= \left( \eta *\frac{\partial E}{\partial w_{i j}}\right) +(\lambda *\Delta w_{i j^{t-1}}) $$
(13)
The weight increment in the current
\(t^{\prime} \text{th}\)
training epoch is denoted as
\( \Delta w_{i j^{t}}\)
, while
\(\eta \)
is the learning rate, and the weight increment in the preceding
\((t-1)^{\prime} \text{th}\)
training epoch. The momentum factor value is maintained within the range 0 to 1; in turn, the step size of the weight updating increases in the direction of the bare minimum to minimize the error. As the value of the momentum factor becomes very low, the model loses its ability to avoid the local bare minimum. By contrast, as the momentum factor value becomes high, the model develops the ability to converge much more rapidly. If a high value of momentum factor is used together with LR, then the model could miss the global bare minimum by crossing over it.
However, when the gradient varies its direction continually throughout the training process, then the suitable value of the momentum factor (which is a hyper-parameter) causes a smoothening of the weight updating variations.
5.
Adaptive Moment Estimation (Adam): It is another optimization technique or learning algorithm that is widely used. Adam [
85
] represents the latest trends in deep learning optimization. This is represented by the Hessian matrix, which employs a second-order derivative. Adam is a learning strategy that has been designed specifically for training deep neural networks. More memory efficient and less computational power are two advantages of Adam. The mechanism of Adam is to calculate adaptive LR for each parameter in the model. It integrates the pros of both Momentum and RMSprop. It utilizes the squared gradients to scale the learning rate as RMSprop and it is similar to the momentum by using the moving average of the gradient. The equation of Adam is represented in Eq.
14
.
$$ w_{i j^{t}}=w_{i j^{t-1}}-\frac{\eta }{\sqrt{\widehat{E[\delta ^{2}]^{t}} }+\in } *\widehat{E[\delta ^{2}]^{t}} $$
(14)
Design of algorithms (backpropagation)
Let’s start with a notation that refers to weights in the network unambiguously. We denote
\({\varvec{w}}_{i j}^{h}\)
to be the weight for the connection from
\(\text {ith}\)
input or (neuron at
\(\left. (\text {h}-1){\text{th}}\right) \)
to the
\(j{\text{t }}\)
neuron in the
\(\text {hth}\)
layer. So, Fig.
12
shows the weight on a connection from the neuron in the first layer to another neuron in the next layer in the network.
Fig. 12
MLP structure
Full size image
Where
\(w_{11}^{2}\)
has represented the weight from the first neuron in the first layer to the first neuron in the second layer, based on that the second weight for the same neuron will be
\(w_{21}^{2}\)
which means is the weight comes from the second neuron in the previous layer to the first layer in the next layer which is the second in this net. Regarding the bias, since the bias is not the connection between the neurons for the layers, so it is easily handled each neuron must have its own bias, some network each layer has a certain bias. It can be seen from the above net that each layer has its own bias. Each network has the parameters such as the no of the layer in the net, the number of the neurons in each layer, no of the weight (connection) between the layers, the no of connection can be easily determined based on the no of neurons in each layer, for example, if there are ten input fully connect with two neurons in the next layer then the number of connection between them is
\((10 * 2=20\)
connection, weights), how the error is defined, and the weight is updated, we will imagine there is there are two layers in our neural network,
$$ \text{ error } =1 / 2\left( {\mathbf {d}}_{i}-{\mathbf {y}}_{i}\right) ^{2} $$
(15)
where
\(\text {d}\)
is the label of induvial input
\(\text {ith}\)
and
\(\text {y}\)
is the output of the same individual input. Backpropagation is about understanding how to change the weights and biases in a network based on the changes of the cost function (Error). Ultimately, this means computing the partial derivatives
\(\partial \text {E} / \partial \text {w}_{\text {ij}}^{h}\)
and
\(\partial \text {E} / \partial \text {b}_{\text {j}}^{h}.\)
But to compute those, a local variable is introduced,
\(\delta _{j}^{1}\)
which is called the local error in the
\(j{\text{th} }\)
neuron in the
\(h{\text{th} }\)
layer. Based on that local error Backpropagation will give the procedure to compute
\(\partial \text {E} / \partial \text {w}_{\text {ij}}^{h}\)
and
\(\partial \text {E} / \partial \text {b}_{\text {j}}^{h}\)
how the error is defined, and the weight is updated, we will imagine there is there are two layers in our neural network that is shown in Fig.
13
.
Fig. 13
Neuron activation functions
Full size image
Output error for
\(\delta _{\text {j}}^{1}\)
each
\(1=1: \text {L}\)
where
\(\text {L}\)
is no. of neuron in output
$$ \delta _{\text {j}}^{1}({\mathbf {k}})=(-1) \text {e}(\text {k}) \varvec{\vartheta }^{\prime }\left( v_{j}({\varvec{k}})\right) $$
(16)
where
\(\text {e}(\text {k})\)
is the error of the epoch
\(\text {k}\)
as shown in Eq. (
2
) and
\(\varvec{\vartheta }^{\prime }\left( {\varvec{v}}_{j}({\varvec{k}})\right) \)
is the derivate of the activation function for
\(v_{j}\)
at the output.
Backpropagate the error at all the rest layer except the output
$$ \delta _{\text {j}}^{\text {h}}({\mathbf {k}}) =\varvec{\vartheta }^{\prime }\left( v_{j}({\varvec{k}})\right) \sum _{l=1}^{L} \delta _{\text {j}}^{1} \quad {\varvec{w}}_{j l}^{h+1}({\varvec{k}})$$
(17)
where
\(\delta _{j}^{1}({\mathbf {k}})\)
is the output error and
\(w_{j l}^{h+1}(k)\)
is represented the weight after the layer where the error need to obtain.
After finding the error at each neuron in each layer, now we can update the weight in each layer based on Eqs. (
16
) and (
17
).
Improving performance of CNN
Based on our experiments in different DL applications [
86
,
87
,
88
]. We can conclude the most active solutions that may improve the performance of CNN are:
Expand the dataset with data augmentation or use transfer learning (explained in latter sections).
Increase the training time.
Increase the depth (or width) of the model.
Add regularization.
Increase hyperparameters tuning.
CNN architectures
Over the last 10 years, several CNN architectures have been presented [
21
,
26
]. Model architecture is a critical factor in improving the performance of different applications. Various modifications have been achieved in CNN architecture from 1989 until today. Such modifications include structural reformulation, regularization, parameter optimizations, etc. Conversely, it should be noted that the key upgrade in CNN performance occurred largely due to the processing-unit reorganization, as well as the development of novel blocks. In particular, the most novel developments in CNN architectures were performed on the use of network depth. In this section, we review the most popular CNN architectures, beginning from the AlexNet model in 2012 and ending at the High-Resolution (HR) model in 2020. Studying these architectures features (such as input size, depth, and robustness) is the key to help researchers to choose the suitable architecture for the their target task. Table
2
presents the brief overview of CNN architectures.
Table 2 Brief overview of CNN architectures
Full size table
AlexNet
The history of deep CNNs began with the appearance of LeNet [
89
] (Fig.
14
). At that time, the CNNs were restricted to handwritten digit recognition tasks, which cannot be scaled to all image classes. In deep CNN architecture, AlexNet is highly respected [
30
], as it achieved innovative results in the fields of image recognition and classification. Krizhevesky et al. [
30
] first proposed AlexNet and consequently improved the CNN learning ability by increasing its depth and implementing several parameter optimization strategies. Figure
15
illustrates the basic design of the AlexNet architecture.
Fig. 14
The architecture of LeNet
Full size image
Fig. 15
The architecture of AlexNet
Full size image
The learning ability of the deep CNN was limited at this time due to hardware restrictions. To overcome these hardware limitations, two GPUs (NVIDIA GTX 580) were used in parallel to train AlexNet. Moreover, in order to enhance the applicability of the CNN to different image categories, the number of feature extraction stages was increased from five in LeNet to seven in AlexNet. Regardless of the fact that depth enhances generalization for several image resolutions, it was in fact overfitting that represented the main drawback related to the depth. Krizhevesky et al. used Hinton’s idea to address this problem [
90
,
91
]. To ensure that the features learned by the algorithm were extra robust, Krizhevesky et al.’s algorithm randomly passes over several transformational units throughout the training stage. Moreover, by reducing the vanishing gradient problem, ReLU [
92
] could be utilized as a non-saturating activation function to enhance the rate of convergence [
93
]. Local response normalization and overlapping subsampling were also performed to enhance the generalization by decreasing the overfitting. To improve on the performance of previous networks, other modifications were made by using large-size filters
\((5\times 5 \; \text{and}\; 11 \times 11)\)
in the earlier layers. AlexNet has considerable significance in the recent CNN generations, as well as beginning an innovative research era in CNN applications.
Network-in-network
This network model, which has some slight differences from the preceding models, introduced two innovative concepts [
94
]. The first was employing multiple layers of perception convolution. These convolutions are executed using a 1×1 filter, which supports the addition of extra nonlinearity in the networks. Moreover, this supports enlarging the network depth, which may later be regularized using dropout. For DL models, this idea is frequently employed in the bottleneck layer. As a substitution for a FC layer, the GAP is also employed, which represents the second novel concept and enables a significant reduction in the number of model parameters. In addition, GAP considerably updates the network architecture. Generating a final low-dimensional feature vector with no reduction in the feature maps dimension is possible when GAP is used on a large feature map [
95
,
96
]. Figure
16
shows the structure of the network.
Fig. 16
The architecture of network-in-network
Full size image
ZefNet
Before 2013, the CNN learning mechanism was basically constructed on a trial-and-error basis, which precluded an understanding of the precise purpose following the enhancement. This issue restricted the deep CNN performance on convoluted images. In response, Zeiler and Fergus introduced DeconvNet (a multilayer de-convolutional neural network) in 2013 [
97
]. This method later became known as ZefNet, which was developed in order to quantitively visualize the network. Monitoring the CNN performance via understanding the neuron activation was the purpose of the network activity visualization. However, Erhan et al. utilized this exact concept to optimize deep belief network (DBN) performance by visualizing the features of the hidden layers [
98
]. Moreover, in addition to this issue, Le et al. assessed the deep unsupervised auto-encoder (AE) performance by visualizing the created classes of the image using the output neurons [
99
]. By reversing the operation order of the convolutional and pooling layers, DenconvNet operates like a forward-pass CNN. Reverse mapping of this kind launches the convolutional layer output backward to create visually observable image shapes that accordingly give the neural interpretation of the internal feature representation learned at each layer [
100
]. Monitoring the learning schematic through the training stage was the key concept underlying ZefNet. In addition, it utilized the outcomes to recognize an ability issue coupled with the model. This concept was experimentally proven on AlexNet by applying DeconvNet. This indicated that only certain neurons were working, while the others were out of action in the first two layers of the network. Furthermore, it indicated that the features extracted via the second layer contained aliasing objects. Thus, Zeiler and Fergus changed the CNN topology due to the existence of these outcomes. In addition, they executed parameter optimization, and also exploited the CNN learning by decreasing the stride and the filter sizes in order to retain all features of the initial two convolutional layers. An improvement in performance was accordingly achieved due to this rearrangement in CNN topology. This rearrangement proposed that the visualization of the features could be employed to identify design weaknesses and conduct appropriate parameter alteration. Figure
17
shows the structure of the network.
Fig. 17
The architecture of ZefNet
Full size image
Visual geometry group (VGG)
After CNN was determined to be effective in the field of image recognition, an easy and efficient design principle for CNN was proposed by Simonyan and Zisserman. This innovative design was called Visual Geometry Group (VGG). A multilayer model [
101
], it featured nineteen more layers than ZefNet [
97
] and AlexNet [
30
] to simulate the relations of the network representational capacity in depth. Conversely, in the 2013-ILSVRC competition, ZefNet was the frontier network, which proposed that filters with small sizes could enhance the CNN performance. With reference to these results, VGG inserted a layer of the heap of
\(3\times 3\)
filters rather than the
\(5\times 5\)
and 11 × 11 filters in ZefNet. This showed experimentally that the parallel assignment of these small-size filters could produce the same influence as the large-size filters. In other words, these small-size filters made the receptive field similarly efficient to the large-size filters
\((7 \times 7 \; \text{and}\; 5 \times 5)\)
. By decreasing the number of parameters, an extra advantage of reducing computational complication was achieved by using small-size filters. These outcomes established a novel research trend for working with small-size filters in CNN. In addition, by inserting
\(1\times 1\)
convolutions in the middle of the convolutional layers, VGG regulates the network complexity. It learns a linear grouping of the subsequent feature maps. With respect to network tuning, a max pooling layer [
102
] is inserted following the convolutional layer, while padding is implemented to maintain the spatial resolution. In general, VGG obtained significant results for localization problems and image classification. While it did not achieve first place in the 2014-ILSVRC competition, it acquired a reputation due to its enlarged depth, homogenous topology, and simplicity. However, VGG’s computational cost was excessive due to its utilization of around 140 million parameters, which represented its main shortcoming. Figure
18
shows the structure of the network.
Fig. 18
The architecture of VGG
Full size image
GoogLeNet
In the 2014-ILSVRC competition, GoogleNet (also called Inception-V1) emerged as the winner [
103
]. Achieving high-level accuracy with decreased computational cost is the core aim of the GoogleNet architecture. It proposed a novel inception block (module) concept in the CNN context, since it combines multiple-scale convolutional transformations by employing merge, transform, and split functions for feature extraction. Figure
19
illustrates the inception block architecture. This architecture incorporates filters of different sizes (
\(5\times 5, 3\times 3, \; \text{and} \; 1\times 1\)
) to capture channel information together with spatial information at diverse ranges of spatial resolution. The common convolutional layer of GoogLeNet is substituted by small blocks using the same concept of network-in-network (NIN) architecture [
94
], which replaced each layer with a micro-neural network. The GoogLeNet concepts of merge, transform, and split were utilized, supported by attending to an issue correlated with different learning types of variants existing in a similar class of several images. The motivation of GoogLeNet was to improve the efficiency of CNN parameters, as well as to enhance the learning capacity. In addition, it regulates the computation by inserting a
\(1\times 1\)
convolutional filter, as a bottleneck layer, ahead of using large-size kernels. GoogleNet employed sparse connections to overcome the redundant information problem. It decreased cost by neglecting the irrelevant channels. It should be noted here that only some of the input channels are connected to some of the output channels. By employing a GAP layer as the end layer, rather than utilizing a FC layer, the density of connections was decreased. The number of parameters was also significantly decreased from 40 to 5 million parameters due to these parameter tunings. The additional regularity factors used included the employment of RmsProp as optimizer and batch normalization [
104
]. Furthermore, GoogleNet proposed the idea of auxiliary learners to speed up the rate of convergence. Conversely, the main shortcoming of GoogleNet was its heterogeneous topology; this shortcoming requires adaptation from one module to another. Other shortcomings of GoogleNet include the representation jam, which substantially decreased the feature space in the following layer, and in turn occasionally leads to valuable information loss.
Fig. 19
The basic structure of Google Block
Full size image
Highway network
Increasing the network depth enhances its performance, mainly for complicated tasks. By contrast, the network training becomes difficult. The presence of several layers in deeper networks may result in small gradient values of the back-propagation of error at lower layers. In 2015, Srivastava et al. [
105
] suggested a novel CNN architecture, called Highway Network, to overcome this issue. This approach is based on the cross-connectivity concept. The unhindered information flow in Highway Network is empowered by instructing two gating units inside the layer. The gate mechanism concept was motivated by LSTM-based RNN [
106
,
107
]. The information aggregation was conducted by merging the information of the
\(\i{\text{th}}-k\)
layers with the next
\(\i{\text{th}}\)
layer to generate a regularization impact, which makes the gradient-based training of the deeper network very simple. This empowers the training of networks with more than 100 layers, such as a deeper network of 900 layers with the SGD algorithm. A Highway Network with a depth of fifty layers presented an improved rate of convergence, which is better than thin and deep architectures at the same time [
108
]. By contrast, [
69
] empirically demonstrated that plain Net performance declines when more than ten hidden layers are inserted. It should be noted that even a Highway Network 900 layers in depth converges much more rapidly than the plain network.
ResNet
He et al. [
37
] developed ResNet (Residual Network), which was the winner of ILSVRC 2015. Their objective was to design an ultra-deep network free of the vanishing gradient issue, as compared to the previous networks. Several types of ResNet were developed based on the number of layers (starting with 34 layers and going up to 1202 layers). The most common type was ResNet50, which comprised 49 convolutional layers plus a single FC layer. The overall number of network weights was 25.5 M, while the overall number of MACs was 3.9 M. The novel idea of ResNet is its use of the bypass pathway concept, as shown in Fig.
20
, which was employed in Highway Nets to address the problem of training a deeper network in 2015. This is illustrated in Fig.
20
, which contains the fundamental ResNet block diagram. This is a conventional feedforward network plus a residual connection. The residual layer output can be identified as the
\((l - 1){\text{th}}\)
outputs, which are delivered from the preceding layer
\((x_{l} - 1)\)
. After executing different operations [such as convolution using variable-size filters, or batch normalization, before applying an activation function like ReLU on
\((x_{l} - 1)\)
], the output is
\(F(x_{l} - 1)\)
. The ending residual output is
\(x_{l}\)
, which can be mathematically represented as in Eq.
18
.
$$ x_{l}= F(x_{l} - 1)+x_{l} - 1 $$
(18)
There are numerous basic residual blocks included in the residual network. Based on the type of the residual network architecture, operations in the residual block are also changed [
37
].
Fig. 20
The block diagram for ResNet
Full size image
In comparison to the highway network, ResNet presented shortcut connections inside layers to enable cross-layer connectivity, which are parameter-free and data-independent. Note that the layers characterize non-residual functions when a gated shortcut is closed in the highway network. By contrast, the individuality shortcuts are never closed, while the residual information is permanently passed in ResNet. Furthermore, ResNet has the potential to prevent the problems of gradient diminishing, as the shortcut connections (residual links) accelerate the deep network convergence. ResNet was the winner of the 2015-ILSVRC championship with 152 layers of depth; this represents 8 times the depth of VGG and 20 times the depth of AlexNet. In comparison with VGG, it has lower computational complexity, even with enlarged depth.
Inception: ResNet and Inception-V3/4
Szegedy et al. [
103
,
109
,
110
] proposed Inception-ResNet and Inception-V3/4 as upgraded types of Inception-V1/2. The concept behind Inception-V3 was to minimize the computational cost with no effect on the deeper network generalization. Thus, Szegedy et al. used asymmetric small-size filters (
\(1\times 5\)
and
\(1\times 7\)
) rather than large-size filters (
\( 7\times 7\)
and
\(5\times 5\)
); moreover, they utilized a bottleneck of
\(1\times 1\)
convolution prior to the large-size filters [
110
]. These changes make the operation of the traditional convolution very similar to cross-channel correlation. Previously, Lin et al. utilized the 1 × 1 filter potential in NIN architecture [
94
]. Subsequently, [
110
] utilized the same idea in an intelligent manner. By using
\(1\times 1\)
convolutional operation in Inception-V3, the input data are mapped into three or four isolated spaces, which are smaller than the initial input spaces. Next, all of these correlations are mapped in these smaller spaces through common
\(5\times 5\)
or
\(3\times 3\)
convolutions. By contrast, in Inception-ResNet, Szegedy et al. bring together the inception block and the residual learning power by replacing the filter concatenation with the residual connection [
111
]. Szegedy et al. empirically demonstrated that Inception-ResNet (Inception-4 with residual connections) can achieve a similar generalization power to Inception-V4 with enlarged width and depth and without residual connections. Thus, it is clearly illustrated that using residual connections in training will significantly accelerate the Inception network training. Figure
21
shows The basic block diagram for Inception Residual unit.
Fig. 21
The basic block diagram for Inception Residual unit
Full size image
DenseNet
To solve the problem of the vanishing gradient, DenseNet was presented, following the same direction as ResNet and the Highway network [
105
,
111
,
112
]. One of the drawbacks of ResNet is that it clearly conserves information by means of preservative individuality transformations, as several layers contribute extremely little or no information. In addition, ResNet has a large number of weights, since each layer has an isolated group of weights. DenseNet employed cross-layer connectivity in an improved approach to address this problem [
112
,
113
,
114
]. It connected each layer to all layers in the network using a feed-forward approach. Therefore, the feature maps of each previous layer were employed to input into all of the following layers. In traditional CNNs, there are
l
connections between the previous layer and the current layer, while in DenseNet, there are
\(\frac{l(l+1)}{2}\)
direct connections. DenseNet demonstrates the influence of cross-layer depth wise-convolutions. Thus, the network gains the ability to discriminate clearly between the added and the preserved information, since DenseNet concatenates the features of the preceding layers rather than adding them. However, due to its narrow layer structure, DenseNet becomes parametrically high-priced in addition to the increased number of feature maps. The direct admission of all layers to the gradients via the loss function enhances the information flow all across the network. In addition, this includes a regularizing impact, which minimizes overfitting on tasks alongside minor training sets. Figure
22
shows the architecture of DenseNet Network.
Fig. 22
(adopted from [
112
])
The architecture of DenseNet Network
Full size image
ResNext
ResNext is an enhanced version of the Inception Network [
115
]. It is also known as the Aggregated Residual Transform Network. Cardinality, which is a new term presented by [
115
], utilized the split, transform, and merge topology in an easy and effective way. It denotes the size of the transformation set as an extra dimension [
116
,
117
,
118
]. However, the Inception network manages network resources more efficiently, as well as enhancing the learning ability of the conventional CNN. In the transformation branch, different spatial embeddings (employing e.g.
\(5\times 5\)
,
\(3\times 3\)
, and
\(1\times 1\)
) are used. Thus, customizing each layer is required separately. By contrast, ResNext derives its characteristic features from ResNet, VGG, and Inception. It employed the VGG deep homogenous topology with the basic architecture of GoogleNet by setting
\(3\times 3\)
filters as spatial resolution inside the blocks of split, transform, and merge. Figure
23
shows the ResNext building blocks. ResNext utilized multi-transformations inside the blocks of split, transform, and merge, as well as outlining such transformations in cardinality terms. The performance is significantly improved by increasing the cardinality, as Xie et al. showed. The complexity of ResNext was regulated by employing
\(1\times 1\)
filters (low embeddings) ahead of a
\(3\times 3\)
convolution. By contrast, skipping connections are used for optimized training [
115
].
Fig. 23
The basic block diagram for the ResNext building blocks
Full size image
WideResNet
The feature reuse problem is the core shortcoming related to deep residual networks, since certain feature blocks or transformations contribute a very small amount to learning. Zagoruyko and Komodakis [
119
] accordingly proposed WideResNet to address this problem. These authors advised that the depth has a supplemental influence, while the residual units convey the core learning ability of deep residual networks. WideResNet utilized the residual block power via making the ResNet wider instead of deeper [
37
]. It enlarged the width by presenting an extra factor, k, which handles the network width. In other words, it indicated that layer widening is a highly successful method of performance enhancement compared to deepening the residual network. While enhanced representational capacity is achieved by deep residual networks, these networks also have certain drawbacks, such as the exploding and vanishing gradient problems, feature reuse problem (inactivation of several feature maps), and the time-intensive nature of the training. He et al. [
37
] tackled the feature reuse problem by including a dropout in each residual block to regularize the network in an efficient manner. In a similar manner, utilizing dropouts, Huang et al. [
120
] presented the stochastic depth concept to solve the slow learning and gradient vanishing problems. Earlier research was focused on increasing the depth; thus, any small enhancement in performance required the addition of several new layers. When comparing the number of parameters, WideResNet has twice that of ResNet, as an experimental study showed. By contrast, WideResNet presents an improved method for training relative to deep networks [
119
]. Note that most architectures prior to residual networks (including the highly effective VGG and Inception) were wider than ResNet. Thus, wider residual networks were established once this was determined. However, inserting a dropout between the convolutional layers (as opposed to within the residual block) made the learning more effective in WideResNet [
121
,
122
].
Pyramidal Net
The depth of the feature map increases in the succeeding layer due to the deep stacking of multi-convolutional layers, as shown in previous deep CNN architectures such as ResNet, VGG, and AlexNet. By contrast, the spatial dimension reduces, since a sub-sampling follows each convolutional layer. Thus, augmented feature representation is recompensed by decreasing the size of the feature map. The extreme expansion in the depth of the feature map, alongside the spatial information loss, interferes with the learning ability in the deep CNNs. ResNet obtained notable outcomes for the issue of image classification. Conversely, deleting a convolutional block—in which both the number of channel and spatial dimensions vary (channel depth enlarges, while spatial dimension reduces)—commonly results in decreased classifier performance. Accordingly, the stochastic ResNet enhanced the performance by decreasing the information loss accompanying the residual unit drop. Han et al. [
123
] proposed Pyramidal Net to address the ResNet learning interference problem. To address the depth enlargement and extreme reduction in spatial width via ResNet, Pyramidal Net slowly enlarges the residual unit width to cover the most feasible places rather than saving the same spatial dimension inside all residual blocks up to the appearance of the down-sampling. It was referred to as Pyramidal Net due to the slow enlargement in the feature map depth based on the up-down method. Factor l, which was determined by Eq.
19
, regulates the depth of the feature map.
$$\begin{aligned} d_{l}=\begin{Bmatrix} 16&if \,\, l =1\\ \left\lfloor d_{l-1}+\frac{\lambda }{n} \right\rfloor&if \,\,2 \le l \le n+1 \end{Bmatrix} \end{aligned}$$
(19)
Here, the dimension of the
l
th residual unit is indicated by
\(d_{l}\)
; moreover,
n
indicates the overall number of residual units, the step factor is indicated by
\(\lambda \)
, and the depth increase is regulated by the factor
\(\frac{\lambda }{n}\)
, which uniformly distributes the weight increase across the dimension of the feature map. Zero-padded identity mapping is used to insert the residual connections among the layers. In comparison to the projection-based shortcut connections, zero-padded identity mapping requires fewer parameters, which in turn leads to enhanced generalization [
124
]. Multiplication- and addition-based widening are two different approaches used in Pyramidal Nets for network widening. More specifically, the first approach (multiplication) enlarges geometrically, while the second one (addition) enlarges linearly [
92
]. The main problem associated with the width enlargement is the growth in time and space required related to the quadratic time.
Xception
Extreme inception architecture is the main characteristic of Xception. The main idea behind Xception is its depthwise separable convolution [
125
]. The Xception model adjusted the original inception block by making it wider and exchanging a single dimension (
\(3 \times 3\)
) followed by a
\(1 \times 1\)
convolution to reduce computational complexity. Figure
24
shows the Xception block architecture. The Xception network becomes extra computationally effective through the use of the decoupling channel and spatial correspondence. Moreover, it first performs mapping of the convolved output to the embedding short dimension by applying
\(1 \times 1\)
convolutions. It then performs
k
spatial transformations. Note that
k
here represents the width-defining cardinality, which is obtained via the transformations number in Xception. However, the computations were made simpler in Xception by distinctly convolving each channel around the spatial axes. These axes are subsequently used as the
\(1 \times 1\)
convolutions (pointwise convolution) for performing cross-channel correspondence. The
\(1 \times 1\)
convolution is utilized in Xception to regularize the depth of the channel. The traditional convolutional operation in Xception utilizes a number of transformation segments equivalent to the number of channels; Inception, moreover, utilizes three transformation segments, while traditional CNN architecture utilizes only a single transformation segment. Conversely, the suggested Xception transformation approach achieves extra learning efficiency and better performance but does not minimize the number of parameters [
126
,
127
].
Fig. 24
The basic block diagram for the Xception block architecture
Full size image
Residual attention neural network
To improve the network feature representation, Wang et al. [
128
] proposed the Residual Attention Network (RAN). Enabling the network to learn aware features of the object is the main purpose of incorporating attention into the CNN. The RAN consists of stacked residual blocks in addition to the attention module; hence, it is a feed-forward CNN. However, the attention module is divided into two branches, namely the mask branch and trunk branch. These branches adopt a top-down and bottom-up learning strategy respectively. Encapsulating two different strategies in the attention model supports top-down attention feedback and fast feed-forward processing in only one particular feed-forward process. More specifically, the top-down architecture generates dense features to make inferences about every aspect. Moreover, the bottom-up feedforward architecture generates low-resolution feature maps in addition to robust semantic information. Restricted Boltzmann machines employed a top-down bottom-up strategy as in previously proposed studies [
129
]. During the training reconstruction phase, Goh et al. [
130
] used the mechanism of top-down attention in deep Boltzmann machines (DBMs) as a regularizing factor. Note that the network can be globally optimized using a top-down learning strategy in a similar manner, where the maps progressively output to the input throughout the learning process [
129
,
130
,
131
,
132
].
Incorporating the attention concept with convolutional blocks in an easy way was used by the transformation network, as obtained in a previous study [
133
]. Unfortunately, these are inflexible, which represents the main problem, along with their inability to be used for varying surroundings. By contrast, stacking multi-attention modules has made RAN very effective at recognizing noisy, complex, and cluttered images. RAN’s hierarchical organization gives it the capability to adaptively allocate a weight for every feature map depending on its importance within the layers. Furthermore, incorporating three distinct levels of attention (spatial, channel, and mixed) enables the model to use this ability to capture the object-aware features at these distinct levels.
Convolutional block attention module
The importance of the feature map utilization and the attention mechanism is certified via SE-Network and RAN [
128
,
134
,
135
]. The convolutional block attention (CBAM) module, which is a novel attention-based CNN, was first developed by Woo et al. [
136
]. This module is similar to SE-Network and simple in design. SE-Network disregards the object’s spatial locality in the image and considers only the channels’ contribution during the image classification. Regarding object detection, object spatial location plays a significant role. The convolutional block attention module sequentially infers the attention maps. More specifically, it applies channel attention preceding the spatial attention to obtain the refined feature maps. Spatial attention is performed using 1 × 1 convolution and pooling functions, as in the literature. Generating an effective feature descriptor can be achieved by using a spatial axis along with the pooling of features. In addition, generating a robust spatial attention map is possible, as CBAM concatenates the max pooling and average pooling operations. In a similar manner, a collection of GAP and max pooling operations is used to model the feature map statistics. Woo et al. [
136
] demonstrated that utilizing GAP will return a sub-optimized inference of channel attention, whereas max pooling provides an indication of the distinguishing object features. Thus, the utilization of max pooling and average pooling enhances the network’s representational power. The feature maps improve the representational power, as well as facilitating a focus on the significant portion of the chosen features. The expression of 3D attention maps through a serial learning procedure assists in decreasing the computational cost and the number of parameters, as Woo et al. [
136
] experimentally proved. Note that any CNN architecture can be simply integrated with CBAM.
Concurrent spatial and channel excitation mechanism
To make the work valid for segmentation tasks, Roy et al. [
137
,
138
] expanded Hu et al. [
134
] effort by adding the influence of spatial information to the channel information. Roy et al. [
137
,
138
] presented three types of modules: (1) channel squeeze and excitation with concurrent channels (scSE); (2) exciting spatially and squeezing channel-wise (sSE); (3) exciting channel-wise and squeezing spatially (cSE). For segmentation purposes, they employed auto-encoder-based CNNs. In addition, they suggested inserting modules following the encoder and decoder layers. To specifically highlight the object-specific feature maps, they further allocated attention to every channel by expressing a scaling factor from the channel and spatial information in the first module (scSE). In the second module (sSE), the feature map information has lower importance than the spatial locality, as the spatial information plays a significant role during the segmentation process. Therefore, several channel collections are spatially divided and developed so that they can be employed in segmentation. In the final module (cSE), a similar SE-block concept is used. Furthermore, the scaling factor is derived founded on the contribution of the feature maps within the object detection [
137
,
138
].
CapsuleNet
CNN is an efficient technique for detecting object features and achieving well-behaved recognition performance in comparison with innovative handcrafted feature detectors. A number of restrictions related to CNN are present, meaning that the CNN does not consider certain relations, orientation, size, and perspectives of features. For instance, when considering a face image, the CNN does not count the various face components (such as mouth, eyes, nose, etc.) positions, and will incorrectly activate the CNN neurons and recognize the face without taking specific relations (such as size, orientation etc.) into account. At this point, consider a neuron that has probability in addition to feature properties such as size, orientation, perspective, etc. A specific neuron/capsule of this type has the ability to effectively detect the face along with different types of information. Thus, many layers of capsule nodes are used to construct the capsule network. An encoding unit, which contains three layers of capsule nodes, forms the CapsuleNet or CapsNet (the initial version of the capsule networks).
For example, the MNIST architecture comprises
\(28\times 28\)
images, applying 256 filters of size
\(9\times 9\)
and with stride 1. The
\(28-9+1=20\)
is the output plus 256 feature maps. Next, these outputs are input to the first capsule layer, while producing an 8D vector rather than a scalar; in fact, this is a modified convolution layer. Note that a stride 2 with
\(9\times 9\)
filters is employed in the first convolution layer. Thus, the dimension of the output is
\((20-9)/2+1=6\)
. The initial capsules employ
\(8\times 32\)
filters, which generate 32 × 8 × 6 × 6 (32 for groups, 8 for neurons, while 6 × 6 is the neuron size).
Figure
25
represents the complete CapsNet encoding and decoding processes. In the CNN context, a max-pooling layer is frequently employed to handle the translation change. It can detect the feature moves in the event that the feature is still within the max-pooling window. This approach has the ability to detect the overlapped features; this is highly significant in detection and segmentation operations, since the capsule involves the weighted features sum from the preceding layer.
Fig. 25
The complete CapsNet encoding and decoding processes
Full size image
In conventional CNNs, a particular cost function is employed to evaluate the global error that grows toward the back throughout the training process. Conversely, in such cases, the activation of a neuron will not grow further once the weight between two neurons turns out to be zero. Instead of a single size being provided with the complete cost function in repetitive dynamic routing alongside the agreement, the signal is directed based on the feature parameters. Sabour et al. [
139
] provides more details about this architecture. When using MNIST to recognize handwritten digits, this innovative CNN architecture gives superior accuracy. From the application perspective, this architecture has extra suitability for segmentation and detection approaches when compared with classification approaches [
140
,
141
,
142
].
High-resolution network (HRNet)
High-resolution representations are necessary for position-sensitive vision tasks, such as semantic segmentation, object detection, and human pose estimation. In the present up-to-date frameworks, the input image is encoded as a low-resolution representation using a subnetwork that is constructed as a connected series of high-to-low resolution convolutions such as VGGNet and ResNet. The low-resolution representation is then recovered to become a high-resolution one. Alternatively, high-resolution representations are maintained during the entire process using a novel network, referred to as a High-Resolution Network (HRNet) [
143
,
144
]. This network has two principal features. First, the convolution series of high-to-low resolutions are connected in parallel. Second, the information across the resolutions are repeatedly exchanged. The advantage achieved includes getting a representation that is more accurate in the spatial domain and extra-rich in the semantic domain. Moreover, HRNet has several applications in the fields of object detection, semantic segmentation, and human pose prediction. For computer vision problems, the HRNet represents a more robust backbone. Figure
26
illustrates the general architecture of HRNet.
Fig. 26
The general architecture of HRNet
Full size image
Challenges (limitations) of deep learning and alternate solutions
When employing DL, several difficulties are often taken into consideration. Those more challenging are listed next and several possible alternatives are accordingly provided.
Training data
DL is extremely data-hungry considering it also involves representation learning [
145
,
146
]. DL demands an extensively large amount of data to achieve a well-behaved performance model, i.e. as the data increases, an extra well-behaved performance model can be achieved (Fig.
27
). In most cases, the available data are sufficient to obtain a good performance model. However, sometimes there is a shortage of data for using DL directly [
87
]. To properly address this issue, three suggested methods are available. The first involves the employment of the transfer-learning concept after data is collected from similar tasks. Note that while the transferred data will not directly augment the actual data, it will help in terms of both enhancing the original input representation of data and its mapping function [
147
]. In this way, the model performance is boosted. Another technique involves employing a well-trained model from a similar task and fine-tuning the ending of two layers or even one layer based on the limited original data. Refer to [
148
,
149
] for a review of different transfer-learning techniques applied in the DL approach. In the second method, data augmentation is performed [
150
]. This task is very helpful for use in augmenting the image data, since the image translation, mirroring, and rotation commonly do not change the image label. Conversely, it is important to take care when applying this technique in some cases such as with bioinformatics data. For instance, when mirroring an enzyme sequence, the output data may not represent the actual enzyme sequence. In the third method, the simulated data can be considered for increasing the volume of the training set. It is occasionally possible to create simulators based on the physical process if the issue is well understood. Therefore, the result will involve the simulation of as much data as needed. Processing the data requirement for DL-based simulation is obtained as an example in Ref. [
151
].
Fig. 27
The performance of DL regarding the amount of data
Full size image
Transfer learning
Recent research has revealed a widespread use of deep CNNs, which offer ground-breaking support for answering many classification problems. Generally speaking, deep CNN models require a sizable volume of data to obtain good performance. The common challenge associated with using such models concerns the lack of training data. Indeed, gathering a large volume of data is an exhausting job, and no successful solution is available at this time. The undersized dataset problem is therefore currently solved using the TL technique [
148
,
149
], which is highly efficient in addressing the lack of training data issue. The mechanism of TL involves training the CNN model with large volumes of data. In the next step, the model is fine-tuned for training on a small request dataset.
The student-teacher relationship is a suitable approach to clarifying TL. Gathering detailed knowledge of the subject is the first step [
152
]. Next, the teacher provides a “course” by conveying the information within a “lecture series” over time. Put simply, the teacher transfers the information to the student. In more detail, the expert (teacher) transfers the knowledge (information) to the learner (student). Similarly, the DL network is trained using a vast volume of data, and also learns the bias and the weights during the training process. These weights are then transferred to different networks for retraining or testing a similar novel model. Thus, the novel model is enabled to pre-train weights rather than requiring training from scratch. Figure
28
illustrates the conceptual diagram of the TL technique.
1.
Pre-trained models: Many CNN models, e.g. AlexNet [
30
], GoogleNet [
103
], and ResNet [
37
], have been trained on large datasets such as ImageNet for image recognition purposes. These models can then be employed to recognize a different task without the need to train from scratch. Furthermore, the weights remain the same apart from a few learned features. In cases where data samples are lacking, these models are very useful. There are many reasons for employing a pre-trained model. First, training large models on sizeable datasets requires high-priced computational power. Second, training large models can be time-consuming, taking up to multiple weeks. Finally, a pre-trained model can assist with network generalization and speed up the convergence.
2.
A research problem using pre-trained models: Training a DL approach requires a massive number of images. Thus, obtaining good performance is a challenge under these circumstances. Achieving excellent outcomes in image classification or recognition applications, with performance occasionally superior to that of a human, becomes possible through the use of deep convolutional neural networks (DCNNs) including several layers if a huge amount of data is available [
37
,
148
,
153
]. However, avoiding overfitting problems in such applications requires sizable datasets and properly generalizing DCNN models. When training a DCNN model, the dataset size has no lower limit. However, the accuracy of the model becomes insufficient in the case of the utilized model has fewer layers, or if a small dataset is used for training due to over- or under-fitting problems. Due to they have no ability to utilize the hierarchical features of sizable datasets, models with fewer layers have poor accuracy. It is difficult to acquire sufficient training data for DL models. For example, in medical imaging and environmental science, gathering labelled datasets is very costly [
148
]. Moreover, the majority of the crowdsourcing workers are unable to make accurate notes on medical or biological images due to their lack of medical or biological knowledge. Thus, ML researchers often rely on field experts to label such images; however, this process is costly and time consuming. Therefore, producing the large volume of labels required to develop flourishing deep networks turns out to be unfeasible. Recently, TL has been widely employed to address the later issue. Nevertheless, although TL enhances the accuracy of several tasks in the fields of pattern recognition and computer vision [
154
,
155
], there is an essential issue related to the source data type used by the TL as compared to the target dataset. For instance, enhancing the medical image classification performance of CNN models is achieved by training the models using the ImageNet dataset, which contains natural images [
153
]. However, such natural images are completely dissimilar from the raw medical images, meaning that the model performance is not enhanced. It has further been proven that TL from different domains does not significantly affect performance on medical imaging tasks, as lightweight models trained from scratch perform nearly as well as standard ImageNet-transferred models [
156
]. Therefore, there exists scenarios in which using pre-trained models do not become an affordable solution. In 2020, some researchers have utilized same-domain TL and achieved excellent results [
86
,
87
,
88
,
157
]. Same-domain TL is an approach of using images that look similar to the target dataset for training. For example, using X-ray images of different chest diseases to train the model, then fine-tuning and training it on chest X-ray images for COVID-19 diagnosis. More details about same-domain TL and how to implement the fine-tuning process can be found in [
87
].
Fig. 28
The conceptual diagram of the TL technique
Full size image
Data augmentation techniques
If the goal is to increase the amount of available data and avoid the overfitting issue, data augmentation techniques are one possible solution [
150
,
158
,
159
]. These techniques are data-space solutions for any limited-data problem. Data augmentation incorporates a collection of methods that improve the attributes and size of training datasets. Thus, DL networks can perform better when these techniques are employed. Next, we list some data augmentation alternate solutions.
1.
Flipping: Flipping the vertical axis is a less common practice than flipping the horizontal one. Flipping has been verified as valuable on datasets like ImageNet and CIFAR-10. Moreover, it is highly simple to implement. In addition, it is not a label-conserving transformation on datasets that involve text recognition (such as SVHN and MNIST).
2.
Color space: Encoding digital image data is commonly used as a dimension tensor (
\(height \times width \times color channels\)
). Accomplishing augmentations in the color space of the channels is an alternative technique, which is extremely workable for implementation. A very easy color augmentation involves separating a channel of a particular color, such as Red, Green, or Blue. A simple way to rapidly convert an image using a single-color channel is achieved by separating that matrix and inserting additional double zeros from the remaining two color channels. Furthermore, increasing or decreasing the image brightness is achieved by using straightforward matrix operations to easily manipulate the RGB values. By deriving a color histogram that describes the image, additional improved color augmentations can be obtained. Lighting alterations are also made possible by adjusting the intensity values in histograms similar to those employed in photo-editing applications.
3.
Cropping: Cropping a dominant patch of every single image is a technique employed with combined dimensions of height and width as a specific processing step for image data. Furthermore, random cropping may be employed to produce an impact similar to translations. The difference between translations and random cropping is that translations conserve the spatial dimensions of this image, while random cropping reduces the input size [for example from (256, 256) to (224, 224)]. According to the selected reduction threshold for cropping, the label-preserving transformation may not be addressed.
4.
Rotation: When rotating an image left or right from within 0 to 360 degrees around the axis, rotation augmentations are obtained. The rotation degree parameter greatly determines the suitability of the rotation augmentations. In digit recognition tasks, small rotations (from 0 to 20 degrees) are very helpful. By contrast, the data label cannot be preserved post-transformation when the rotation degree increases.
5.
Translation: To avoid positional bias within the image data, a very useful transformation is to shift the image up, down, left, or right. For instance, it is common that the whole dataset images are centered; moreover, the tested dataset should be entirely made up of centered images to test the model. Note that when translating the initial images in a particular direction, the residual space should be filled with Gaussian or random noise, or a constant value such as 255 s or 0 s. The spatial dimensions of the image post-augmentation are preserved using this padding.
6.
Noise injection This approach involves injecting a matrix of arbitrary values. Such a matrix is commonly obtained from a Gaussian distribution. Moreno-Barea et al. [
160
] employed nine datasets to test the noise injection. These datasets were taken from the UCI repository [
161
]. Injecting noise within images enables the CNN to learn additional robust features.
However, highly well-behaved solutions for positional biases available within the training data are achieved by means of geometric transformations. To separate the distribution of the testing data from the training data, several prospective sources of bias exist. For instance, when all faces should be completely centered within the frames (as in facial recognition datasets), the problem of positional biases emerges. Thus, geometric translations are the best solution. Geometric translations are helpful due to their simplicity of implementation, as well as their effective capability to disable the positional biases. Several libraries of image processing are available, which enables beginning with simple operations such as rotation or horizontal flipping. Additional training time, higher computational costs, and additional memory are some shortcomings of geometric transformations. Furthermore, a number of geometric transformations (such as arbitrary cropping or translation) should be manually observed to ensure that they do not change the image label. Finally, the biases that separate the test data from the training data are more complicated than transitional and positional changes. Hence, it is not trivial answering to when and where geometric transformations are suitable to be applied.
Imbalanced data
Commonly, biological data tend to be imbalanced, as negative samples are much more numerous than positive ones [
162
,
163
,
164
]. For example, compared to COVID-19-positive X-ray images, the volume of normal X-ray images is very large. It should be noted that undesirable results may be produced when training a DL model using imbalanced data. The following techniques are used to solve this issue. First, it is necessary to employ the correct criteria for evaluating the loss, as well as the prediction result. In considering the imbalanced data, the model should perform well on small classes as well as larger ones. Thus, the model should employ area under curve (AUC) as the resultant loss as well as the criteria [
165
]. Second, it should employ the weighted cross-entropy loss, which ensures the model will perform well with small classes if it still prefers to employ the cross-entropy loss. Simultaneously, during model training, it is possible either to down-sample the large classes or up-sample the small classes. Finally, to make the data balanced as in Ref. [
166
], it is possible to construct models for every hierarchical level, as a biological system frequently has hierarchical label space. However, the effect of the imbalanced data on the performance of the DL model has been comprehensively investigated. In addition, to lessen the problem, the most frequently used techniques were also compared. Nevertheless, note that these techniques are not specified for biological problems.
Interpretability of data
Occasionally, DL techniques are analyzed to act as a black box. In fact, they are interpretable. The need for a method of interpreting DL, which is used to obtain the valuable motifs and patterns recognized by the network, is common in many fields, such as bioinformatics [
167
]. In the task of disease diagnosis, it is not only required to know the disease diagnosis or prediction results of a trained DL model, but also how to enhance the surety of the prediction outcomes, as the model makes its decisions based on these verifications [
168
]. To achieve this, it is possible to give a score of importance for every portion of the particular example. Within this solution, back-propagation-based techniques or perturbation-based approaches are used [
169
]. In the perturbation-based approaches, a portion of the input is changed and the effect of this change on the model output is observed [
170
,
171
,
172
,
173
]. This concept has high computational complexity, but it is simple to understand. On the other hand, to check the score of the importance of various input portions, the signal from the output propagates back to the input layer in the back-propagation-based techniques. These techniques have been proven valuable in [
174
]. In different scenarios, various meanings can represent the model interpretability.
Uncertainty scaling
Commonly, the final prediction label is not the only label required when employing DL techniques to achieve the prediction; the score of confidence for every inquiry from the model is also desired. The score of confidence is defined as how confident the model is in its prediction [
175
]. Since the score of confidence prevents belief in unreliable and misleading predictions, it is a significant attribute, regardless of the application scenario. In biology, the confidence score reduces the resources and time expended in proving the outcomes of the misleading prediction. Generally speaking, in healthcare or similar applications, the uncertainty scaling is frequently very significant; it helps in evaluating automated clinical decisions and the reliability of machine learning-based disease-diagnosis [
176
,
177
]. Because overconfident prediction can be the output of different DL models, the score of probability (achieved from the softmax output of the direct-DL) is often not in the correct scale [
178
]. Note that the softmax output requires post-scaling to achieve a reliable probability score. For outputting the probability score in the correct scale, several techniques have been introduced, including Bayesian Binning into Quantiles (BBQ) [
179
], isotonic regression [
180
], histogram binning [
181
], and the legendary Platt scaling [
182
]. More specifically, for DL techniques, temperature scaling was recently introduced, which achieves superior performance compared to the other techniques.
Catastrophic forgetting
This is defined as incorporating new information into a plain DL model, made possible by interfering with the learned information. For instance, consider a case where there are 1000 types of flowers and a model is trained to classify these flowers, after which a new type of flower is introduced; if the model is fine-tuned only with this new class, its performance will become unsuccessful with the older classes [
183
,
184
]. The logical data are continually collected and renewed, which is in fact a highly typical scenario in many fields, e.g. Biology. To address this issue, there is a direct solution that involves employing old and new data to train an entirely new model from scratch. This solution is time-consuming and computationally intensive; furthermore, it leads to an unstable state for the learned representation of the initial data. At this time, three different types of ML techniques, which have not catastrophic forgetting, are made available to solve the human brain problem founded on the neurophysiological theories [
185
,
186
]. Techniques of the first type are founded on regularizations such as EWC [
183
] Techniques of the second type employ rehearsal training techniques and dynamic neural network architecture like iCaRL [
187
,
188
]. Finally, techniques of the third type are founded on dual-memory learning systems [
189
]. Refer to [
190
,
191
,
192
] in order to gain more details.
Model compression
To obtain well-trained models that can still be employed productively, DL models have intensive memory and computational requirements due to their huge complexity and large numbers of parameters [
193
,
194
]. One of the fields that is characterized as data-intensive is the field of healthcare and environmental science. These needs reduce the deployment of DL in limited computational-power machines, mainly in the healthcare field. The numerous methods of assessing human health and the data heterogeneity have become far more complicated and vastly larger in size [
195
]; thus, the issue requires additional computation [
196
]. Furthermore, novel hardware-based parallel processing solutions such as FPGAs and GPUs [
197
,
198
,
199
] have been developed to solve the computation issues associated with DL. Recently, numerous techniques for compressing the DL models, designed to decrease the computational issues of the models from the starting point, have also been introduced. These techniques can be classified into four classes. In the first class, the redundant parameters (which have no significant impact on model performance) are reduced. This class, which includes the famous deep compression method, is called parameter pruning [
200
]. In the second class, the larger model uses its distilled knowledge to train a more compact model; thus, it is called knowledge distillation [
201
,
202
]. In the third class, compact convolution filters are used to reduce the number of parameters [
203
]. In the final class, the information parameters are estimated for preservation using low-rank factorization [
204
]. For model compression, these classes represent the most representative techniques. In [
193
], it has been provided a more comprehensive discussion about the topic.
Overfitting
DL models have excessively high possibilities of resulting in data overfitting at the training stage due to the vast number of parameters involved, which are correlated in a complex manner. Such situations reduce the model’s ability to achieve good performance on the tested data [
90
,
205
]. This problem is not only limited to a specific field, but involves different tasks. Therefore, when proposing DL techniques, this problem should be fully considered and accurately handled. In DL, the implied bias of the training process enables the model to overcome crucial overfitting problems, as recent studies suggest [
205
,
206
,
207
,
208
]. Even so, it is still necessary to develop techniques that handle the overfitting problem. An investigation of the available DL algorithms that ease the overfitting problem can categorize them into three classes. The first class acts on both the model architecture and model parameters and includes the most familiar approaches, such as weight decay [
209
], batch normalization [
210
], and dropout [
90
]. In DL, the default technique is weight decay [
209
], which is used extensively in almost all ML algorithms as a universal regularizer. The second class works on model inputs such as data corruption and data augmentation [
150
,
211
]. One reason for the overfitting problem is the lack of training data, which makes the learned distribution not mirror the real distribution. Data augmentation enlarges the training data. By contrast, marginalized data corruption improves the solution exclusive to augmenting the data. The final class works on the model output. A recently proposed technique penalizes the over-confident outputs for regularizing the model [
178
]. This technique has demonstrated the ability to regularize RNNs and CNNs.
Vanishing gradient problem
In general, when using backpropagation and gradient-based learning techniques along with ANNs, largely in the training stage, a problem called the vanishing gradient problem arises [
212
,
213
,
214
]. More specifically, in each training iteration, every weight of the neural network is updated based on the current weight and is proportionally relative to the partial derivative of the error function. However, this weight updating may not occur in some cases due to a vanishingly small gradient, which in the worst case means that no extra training is possible and the neural network will stop completely. Conversely, similarly to other activation functions, the sigmoid function shrinks a large input space to a tiny input space. Thus, the derivative of the sigmoid function will be small due to large variation at the input that produces a small variation at the output. In a shallow network, only some layers use these activations, which is not a significant issue. While using more layers will lead the gradient to become very small in the training stage, in this case, the network works efficiently. The back-propagation technique is used to determine the gradients of the neural networks. Initially, this technique determines the network derivatives of each layer in the reverse direction, starting from the last layer and progressing back to the first layer. The next step involves multiplying the derivatives of each layer down the network in a similar manner to the first step. For instance, multiplying N small derivatives together when there are N hidden layers employs an activation function such as the sigmoid function. Hence, the gradient declines exponentially while propagating back to the first layer. More specifically, the biases and weights of the first layers cannot be updated efficiently during the training stage because the gradient is small. Moreover, this condition decreases the overall network accuracy, as these first layers are frequently critical to recognizing the essential elements of the input data. However, such a problem can be avoided through employing activation functions. These functions lack the squishing property, i.e., the ability to squish the input space to within a small space. By mapping X to max, the ReLU [
91
] is the most popular selection, as it does not yield a small derivative that is employed in the field. Another solution involves employing the batch normalization layer [
81
]. As mentioned earlier, the problem occurs once a large input space is squashed into a small space, leading to vanishing the derivative. Employing batch normalization degrades this issue by simply normalizing the input, i.e., the expression |
x
| does not accomplish the exterior boundaries of the sigmoid function. The normalization process makes the largest part of it come down in the green area, which ensures that the derivative is large enough for further actions. Furthermore, faster hardware can tackle the previous issue, e.g. that provided by GPUs. This makes standard back-propagation possible for many deeper layers of the network compared to the time required to recognize the vanishing gradient problem [
215
].
Exploding gradient problem
Opposite to the vanishing problem is the one related to gradient. Specifically, large error gradients are accumulated during back-propagation [
216
,
217
,
218
]. The latter will lead to extremely significant updates to the weights of the network, meaning that the system becomes unsteady. Thus, the model will lose its ability to learn effectively. Grosso modo, moving backward in the network during back-propagation, the gradient grows exponentially by repetitively multiplying gradients. The weight values could thus become incredibly large and may overflow to become a not-a-number (NaN) value. Some potential solutions include:
1.
Using different weight regularization techniques.
2.
Redesigning the architecture of the network model.
Underspecification
In 2020, a team of computer scientists at Google has identified a new challenge called underspecification [
219
]. ML models including DL models often show surprisingly poor behavior when they are tested in real-world applications such as computer vision, medical imaging, natural language processing, and medical genomics. The reason behind the weak performance is due to underspecification. It has been shown that small modifications can force a model towards a completely different solution as well as lead to different predictions in deployment domains. There are different techniques of addressing underspecification issue. One of them is to design “stress tests” to examine how good a model works on real-world data and to find out the possible issues. Nevertheless, this demands a reliable understanding of the process the model can work inaccurately. The team stated that “Designing stress tests that are well-matched to applied requirements, and that provide good “coverage” of potential failure modes is a major challenge”. Underspecification puts major constraints on the credibility of ML predictions and may require some reconsidering over certain applications. Since ML is linked to human by serving several applications such as medical imaging and self-driving cars, it will require proper attention to this issue.
Applications of deep learning
Presently, various DL applications are widespread around the world. These applications include healthcare, social network analysis, audio and speech processing (like recognition and enhancement), visual data processing methods (such as multimedia data analysis and computer vision), and NLP (translation and sentence classification), among others (Fig.
29
) [
220
,
221
,
222
,
223
,
224
]. These applications have been classified into five categories: classification, localization, detection, segmentation, and registration. Although each of these tasks has its own target, there is fundamental overlap in the pipeline implementation of these applications as shown in Fig.
30
. Classification is a concept that categorizes a set of data into classes. Detection is used to locate interesting objects in an image with consideration given to the background. In detection, multiple objects, which could be from dissimilar classes, are surrounded by bounding boxes. Localization is the concept used to locate the object, which is surrounded by a single bounding box. In segmentation (semantic segmentation), the target object edges are surrounded by outlines, which also label them; moreover, fitting a single image (which could be 2D or 3D) onto another refers to registration. One of the most important and wide-ranging DL applications are in healthcare [
225
,
226
,
227
,
228
,
229
,
230
]. This area of research is critical due to its relation to human lives. Moreover, DL has shown tremendous performance in healthcare. Therefore, we take DL applications in the medical image analysis field as an example to describe the DL applications.
Fig. 29
Examples of DL applications
Full size image
Fig. 30
Workflow of deep learning tasks
Full size image
Classification
Computer-Aided Diagnosis (CADx) is another title sometimes used for classification. Bharati et al. [
231
] used a chest X-ray dataset for detecting lung diseases based on a CNN. Another study attempted to read X-ray images by employing CNN [
232
]. In this modality, the comparative accessibility of these images has likely enhanced the progress of DL. [
233
] used an improved pre-trained GoogLeNet CNN containing more than 150,000 images for training and testing processes. This dataset was augmented from 1850 chest X-rays. The creators reorganized the image orientation into lateral and frontal views and achieved approximately 100% accuracy. This work of orientation classification has clinically limited use. As a part of an ultimately fully automated diagnosis workflow, it obtained the data augmentation and pre-trained efficiency in learning the metadata of relevant images. Chest infection, commonly referred to as pneumonia, is extremely treatable, as it is a commonly occurring health problem worldwide. Conversely, Rajpurkar et al. [
234
] utilized CheXNet, which is an improved version of DenseNet [
112
] with 121 convolution layers, for classifying fourteen types of disease. These authors used the CheXNet14 dataset [
235
], which comprises 112,000 images. This network achieved an excellent performance in recognizing fourteen different diseases. In particular, pneumonia classification accomplished a 0.7632 AUC score using receiver operating characteristics (ROC) analysis. In addition, the network obtained better than or equal to the performance of both a three-radiologist panel and four individual radiologists. Zuo et al. [
236
] have adopted CNN for candidate classification in lung nodule. Shen et al. [
237
] employed both Random Forest (RF) and SVM classifiers with CNNs to classify lung nodules. They employed two convolutional layers with each of the three parallel CNNs. The LIDC-IDRI (Lung Image Database Consortium) dataset, which contained 1010-labeled CT lung scans, was used to classify the two types of lung nodules (malignant and benign). Different scales of the image patches were used by every CNN to extract features, while the output feature vector was constructed using the learned features. Next, these vectors were classified into malignant or benign using either the RF classifier or SVM with radial basis function (RBF) filter. The model was robust to various noisy input levels and achieved an accuracy of 86% in nodule classification. Conversely, the model of [
238
] interpolates the image data missing between PET and MRI images using 3D CNNs. The Alzheimer Disease Neuroimaging Initiative (ADNI) database, containing 830 PET and MRI patient scans, was utilized in their work. The PET and MRI images are used to train the 3D CNNs, first as input and then as output. Furthermore, for patients who have no PET images, the 3D CNNs utilized the trained images to rebuild the PET images. These rebuilt images approximately fitted the actual disease recognition outcomes. However, this approach did not address the overfitting issues, which in turn restricted their technique in terms of its possible capacity for generalization. Diagnosing normal versus Alzheimer’s disease patients has been achieved by several CNN models [
239
,
240
]. Hosseini-Asl et al. [
241
] attained 99% accuracy for up-to-date outcomes in diagnosing normal versus Alzheimer’s disease patients. These authors applied an auto-encoder architecture using 3D CNNs. The generic brain features were pre-trained on the CADDementia dataset. Subsequently, the outcomes of these learned features became inputs to higher layers to differentiate between patient scans of Alzheimer’s disease, mild cognitive impairment, or normal brains based on the ADNI dataset and using fine-tuned deep supervision techniques. The architectures of VGGNet and RNNs, in that order, were the basis of both VOXCNN and ResNet models developed by Korolev et al. [
242
]. They also discriminated between Alzheimer’s disease and normal patients using the ADNI database. Accuracy was 79% for Voxnet and 80% for ResNet. Compared to Hosseini-Asl’s work, both models achieved lower accuracies. Conversely, the implementation of the algorithms was simpler and did not require feature hand-crafting, as Korolev declared. In 2020, Mehmood et al. [
240
] trained a developed CNN-based network called “SCNN” with MRI images for the tasks of classification of Alzheimer’s disease. They achieved state-of-the-art results by obtaining an accuracy of 99.05%.
Recently, CNN has taken some medical imaging classification tasks to different level from traditional diagnosis to automated diagnosis with tremendous performance. Examples of these tasks are diabetic foot ulcer (DFU) (as normal and abnormal (DFU) classes) [
87
,
243
,
244
,
245
,
246
], sickle cells anemia (SCA) (as normal, abnormal (SCA), and other blood components) [
86
,
247
], breast cancer by classify hematoxylin–eosin-stained breast biopsy images into four classes: invasive carcinoma, in-situ carcinoma, benign tumor and normal tissue [
42
,
88
,
248
,
249
,
250
,
251
,
252
], and multi-class skin cancer classification [
253
,
254
,
255
].
In 2020, CNNs are playing a vital role in early diagnosis of the novel coronavirus (COVID-2019). CNN has become the primary tool for automatic COVID-19 diagnosis in many hospitals around the world using chest X-ray images [
256
,
257
,
258
,
259
,
260
]. More details about the classification of medical imaging applications can be found in [
226
,
261
,
262
,
263
,
264
,
265
].
Localization
Although applications in anatomy education could increase, the practicing clinician is more likely to be interested in the localization of normal anatomy. Radiological images are independently examined and described outside of human intervention, while localization could be applied in completely automatic end-to-end applications [
266
,
267
,
268
]. Zhao et al. [
269
] introduced a new deep learning-based approach to localize pancreatic tumor in projection X-ray images for image-guided radiation therapy without the need for fiducials. Roth et al. [
270
] constructed and trained a CNN using five convolutional layers to classify around 4000 transverse-axial CT images. These authors used five categories for classification: legs, pelvis, liver, lung, and neck. After data augmentation techniques were applied, they achieved an AUC score of 0.998 and the classification error rate of the model was 5.9%. For detecting the positions of the spleen, kidney, heart, and liver, Shin et al. [
271
] employed stacked auto-encoders on 78 contrast-improved MRI scans of the stomach area containing the kidneys or liver. Temporal and spatial domains were used to learn the hierarchal features. Based on the organs, these approaches achieved detection accuracies of 62–79%. Sirazitdinov et al. [
268
] presented an aggregate of two convolutional neural networks, namely RetinaNet and Mask R-CNN for pneumonia detection and localization.
Detection
Computer-Aided Detection (CADe) is another method used for detection. For both the clinician and the patient, overlooking a lesion on a scan may have dire consequences. Thus, detection is a field of study requiring both accuracy and sensitivity [
272
,
273
,
274
]. Chouhan et al. [
275
] introduced an innovative deep learning framework for the detection of pneumonia by adopting the idea of transfer learning. Their approach obtained an accuracy of 96.4% with a recall of 99.62% on unseen data. In the area of COVID-19 and pulmonary disease, several convolutional neural network approaches have been proposed for automatic detection from X-ray images which showed an excellent performance [
46
,
276
,
277
,
278
,
279
].
In the area of skin cancer, there several applications were introduced for the detection task [
280
,
281
,
282
]. Thurnhofer-Hemsi et al. [
283
] introduced a deep learning approach for skin cancer detection by fine-tuning five state-of-art convolutional neural network models. They addressed the issue of a lack of training data by adopting the ideas of transfer learning and data augmentation techniques. DenseNet201 network has shown superior results compared to other models.
Another interesting area is that of histopathological images, which are progressively digitized. Several papers have been published in this field [
284
,
285
,
286
,
287
,
288
,
289
,
290
]. Human pathologists read these images laboriously; they search for malignancy markers, such as a high index of cell proliferation, using molecular markers (e.g. Ki-67), cellular necrosis signs, abnormal cellular architecture, enlarged numbers of mitotic figures denoting augmented cell replication, and enlarged nucleus-to-cytoplasm ratios. Note that the histopathological slide may contain a huge number of cells (up to the thousands). Thus, the risk of disregarding abnormal neoplastic regions is high when wading through these cells at excessive levels of magnification. Ciresan et al. [
291
] employed CNNs of 11–13 layers for identifying mitotic figures. Fifty breast histology images from the MITOS dataset were used. Their technique attained recall and precision scores of 0.7 and 0.88 respectively. Sirinukunwattana et al. [
292
] utilized 100 histology images of colorectal adenocarcinoma to detect cell nuclei using CNNs. Roughly 30,000 nuclei were hand-labeled for training purposes. The novelty of this approach was in the use of Spatially Constrained CNN. This CNN detects the center of nuclei using the surrounding spatial context and spatial regression. Instead of this CNN, Xu et al. [
293
] employed a stacked sparse auto-encoder (SSAE) to identify nuclei in histological slides of breast cancer, achieving 0.83 and 0.89 recall and precision scores respectively. In this field, they showed that unsupervised learning techniques are also effectively utilized. In medical images, Albarquoni et al. [
294
] investigated the problem of insufficient labeling. They crowd-sourced the actual mitoses labeling in the histology images of breast cancer (from amateurs online). Solving the recurrent issue of inadequate labeling during the analysis of medical images can be achieved by feeding the crowd-sourced input labels into the CNN. This method signifies a remarkable proof-of-concept effort. In 2020, Lei et al. [
285
] introduced the employment of deep convolutional neural networks for automatic identification of mitotic candidates from histological sections for mitosis screening. They obtained the state-of-the-art detection results on the dataset of the International Pattern Recognition Conference (ICPR) 2012 Mitosis Detection Competition.
Segmentation
Although MRI and CT image segmentation research includes different organs such as knee cartilage, prostate, and liver, most research work has concentrated on brain segmentation, particularly tumors [
295
,
296
,
297
,
298
,
299
,
300
]. This issue is highly significant in surgical preparation to obtain the precise tumor limits for the shortest surgical resection. During surgery, excessive sacrificing of key brain regions may lead to neurological shortfalls including cognitive damage, emotionlessness, and limb difficulty. Conventionally, medical anatomical segmentation was done by hand; more specifically, the clinician draws out lines within the complete stack of the CT or MRI volume slice by slice. Thus, it is perfect for implementing a solution that computerizes this painstaking work. Wadhwa et al. [
301
] presented a brief overview on brain tumor segmentation of MRI images. Akkus et al. [
302
] wrote a brilliant review of brain MRI segmentation that addressed the different metrics and CNN architectures employed. Moreover, they explain several competitions in detail, as well as their datasets, which included Ischemic Stroke Lesion Segmentation (ISLES), Mild Traumatic brain injury Outcome Prediction (MTOP), and Brain Tumor Segmentation (BRATS).
Chen et al. [
299
] proposed convolutional neural networks for precise brain tumor segmentation. The approach that they employed involves several approaches for better features learning including the DeepMedic model, a novel dual-force training scheme, a label distribution-based loss function, and Multi-Layer Perceptron-based post-processing. They conducted their method on the two most modern brain tumor segmentation datasets, i.e., BRATS 2017 and BRATS 2015 datasets. Hu et al. [
300
] introduced the brain tumor segmentation method by adopting a multi-cascaded convolutional neural network (MCCNN) and fully connected conditional random fields (CRFs). The achieved results were excellent compared with the state-of-the-art methods.
Moeskops et al. [
303
] employed three parallel-running CNNs, each of which had a 2D input patch of dissimilar size, for segmenting and classifying MRI brain images. These images, which include 35 adults and 22 pre-term infants, were classified into various tissue categories such as cerebrospinal fluid, grey matter, and white matter. Every patch concentrates on capturing various image aspects with the benefit of employing three dissimilar sizes of input patch; here, the bigger sizes incorporated the spatial features, while the lowest patch sizes concentrated on the local textures. In general, the algorithm has Dice coefficients in the range of 0.82–0.87 and achieved a satisfactory accuracy. Although 2D image slices are employed in the majority of segmentation research, Milletrate et al. [
304
] implemented 3D CNN for segmenting MRI prostate images. Furthermore, they used the PROMISE2012 challenge dataset, from which fifty MRI scans were used for training and thirty for testing. The U-Net architecture of Ronnerberger et al. [
305
] inspired their V-net. This model attained a 0.869 Dice coefficient score, the same as the winning teams in the competition. To reduce overfitting and create the model of a deeper 11-convolutional layer CNN, Pereira et al. [
306
] applied intentionally small-sized filters of 3x3. Their model used MRI scans of 274 gliomas (a type of brain tumor) for training. They achieved first place in the 2013 BRATS challenge, as well as second place in the BRATS challenge 2015. Havaei et al. [
307
] also considered gliomas using the 2013 BRATS dataset. They investigated different 2D CNN architectures. Compared to the winner of BRATS 2013, their algorithm worked better, as it required only 3 min to execute rather than 100 min. The concept of cascaded architecture formed the basis of their model. Thus, it is referred to as an InputCascadeCNN. Employing FC Conditional Random Fields (CRFs), atrous spatial pyramid pooling, and up-sampled filters were techniques introduced by Chen et al. [
308
]. These authors aimed to enhance the accuracy of localization and enlarge the field of view of every filter at a multi-scale. Their model, DeepLab, attained 79.7% mIOU (mean Intersection Over Union). In the PASCAL VOC-2012 image segmentation, their model obtained an excellent performance.
Recently, the Automatic segmentation of COVID-19 Lung Infection from CT Images helps to detect the development of COVID-19 infection by employing several deep learning techniques [
309
,
310
,
311
,
312
].
Registration
Usually, given two input images, the four main stages of the canonical procedure of the image registration task are [
313
,
314
]:
Target Selection: it illustrates the determined input image that the second counterpart input image needs to remain accurately superimposed to.
Feature Extraction: it computes the set of features extracted from each input image.
Feature Matching: it allows finding similarities between the previously obtained features.
Pose Optimization: it is aimed to minimize the distance between both input images.
Then, the result of the registration procedure is the suitable geometric transformation (e.g. translation, rotation, scaling, etc.) that provides both input images within the same coordinate system in a way the distance between them is minimal, i.e. their level of superimposition/overlapping is optimal. It is out of the scope of this work to provide an extensive review of this topic. Nevertheless, a short summary is accordingly introduced next.
Commonly, the input images for the DL-based registration approach could be in various forms, e.g. point clouds, voxel grids, and meshes. Additionally, some techniques allow as inputs the result of the Feature Extraction or Matching steps in the canonical scheme. Specifically, the outcome could be some data in a particular form as well as the result of the steps from the classical pipeline (feature vector, matching vector, and transformation). Nevertheless, with the newest DL-based methods, a novel conceptual type of ecosystem issues. It contains acquired characteristics about the target, materials, and their behavior that can be registered with the input data. Such a conceptual ecosystem is formed by a neural network and its training manner, and it could be counted as an input to the registration approach. Nevertheless, it is not an input that one might adopt in every registration situation since it corresponds to an interior data representation.
From a DL view-point, the interpretation of the conceptual design enables differentiating the input data of a registration approach into defined or non-defined models. In particular, the illustrated phases are models that depict particular spatial data (e.g. 2D or 3D) while a non-defined one is a generalization of a data set created by a learning system. Yumer et al. [
315
] developed a framework in which the model acquires characteristics of objects, meaning ready to identify what a more sporty car seems like or a more comfy chair is, also adjusting a 3D model to fit those characteristics while maintaining the main characteristics of the primary data. Likewise, a fundamental perspective of the unsupervised learning method introduced by Ding et al. [
316
] is that there is no target for the registration approach. In this instance, the network is able of placing each input point cloud in a global space, solving SLAM issues in which many point clouds have to be registered rigidly. On the other hand, Mahadevan [
317
] proposed the combination of two conceptual models utilizing the growth of Imagination Machines to give flexible artificial intelligence systems and relationships between the learned phases through training schemes that are not inspired on labels and classifications. Another practical application of DL, especially CNNs, to image registration is the 3D reconstruction of objects. Wang et al. [
318
] applied an adversarial way using CNNs to rebuild a 3D model of an object from its 2D image. The network learns many objects and orally accomplishes the registration between the image and the conceptual model. Similarly, Hermoza et al. [
319
] also utilize the GAN network for prognosticating the absent geometry of damaged archaeological objects, providing the reconstructed object based on a voxel grid format and a label selecting its class.
DL for medical image registration has numerous applications, which were listed by some review papers [
320
,
321
,
322
]. Yang et al. [
323
] implemented stacked convolutional layers as an encoder-decoder approach to predict the morphing of the input pixel into its last formation using MRI brain scans from the OASIS dataset. They employed a registration model known as Large Deformation Diffeomorphic Metric Mapping (LDDMM) and attained remarkable enhancements in computation time. Miao et al. [
324
] used synthetic X-ray images to train a five-layer CNN to register 3D models of a trans-esophageal probe, a hand implant, and a knee implant onto 2D X-ray images for pose estimation. They determined that their model achieved an execution time of 0.1 s, representing an important enhancement against the conventional registration techniques based on intensity; moreover, it achieved effective registrations 79–99% of the time. Li et al. [
325
] introduced a neural network-based approach for the non-rigid 2D–3D registration of the lateral cephalogram and the volumetric cone-beam CT (CBCT) images.
Computational approaches
For computationally exhaustive applications, complex ML and DL approaches have rapidly been identified as the most significant techniques and are widely used in different fields. The development and enhancement of algorithms aggregated with capabilities of well-behaved computational performance and large datasets make it possible to effectively execute several applications, as earlier applications were either not possible or difficult to take into consideration.
Currently, several standard DNN configurations are available. The interconnection patterns between layers and the total number of layers represent the main differences between these configurations. The Table
2
illustrates the growth rate of the overall number of layers over time, which seems to be far faster than the “Moore’s Law growth rate”. In normal DNN, the number of layers grew by around 2.3× each year in the period from 2012 to 2016. Recent investigations of future ResNet versions reveal that the number of layers can be extended up to 1000. However, an SGD technique is employed to fit the weights (or parameters), while different optimization techniques are employed to obtain parameter updating during the DNN training process. Repetitive updates are required to enhance network accuracy in addition to a minorly augmented rate of enhancement. For example, the training process using ImageNet as a large dataset, which contains more than 14 million images, along with ResNet as a network model, take around 30K to 40K repetitions to converge to a steady solution. In addition, the overall computational load, as an upper-level prediction, may exceed 1020 FLOPS when both the training set size and the DNN complexity increase.
Prior to 2008, boosting the training to a satisfactory extent was achieved by using GPUs. Usually, days or weeks are needed for a training session, even with GPU support. By contrast, several optimization strategies were developed to reduce the extensive learning time. The computational requirements are believed to increase as the DNNs continuously enlarge in both complexity and size.
In addition to the computational load cost, the memory bandwidth and capacity have a significant effect on the entire training performance, and to a lesser extent, deduction. More specifically, the parameters are distributed through every layer of the input data, there is a sizeable amount of reused data, and the computation of several network layers exhibits an excessive computation-to-bandwidth ratio. By contrast, there are no distributed parameters, the amount of reused data is extremely small, and the additional FC layers have an extremely small computation-to-bandwidth ratio. Table
3
presents a comparison between different aspects related to the devices. In addition, the table is established to facilitate familiarity with the tradeoffs by obtaining the optimal approach for configuring a system based on either FPGA, GPU, or CPU devices. It should be noted that each has corresponding weaknesses and strengths; accordingly, there are no clear one-size-fits-all solutions.
Table 3 A comparison between different aspects related to the devices
Full size table
Although GPU processing has enhanced the ability to address the computational challenges related to such networks, the maximum GPU (or CPU) performance is not achieved, and several techniques or models have turned out to be strongly linked to bandwidth. In the worst cases, the GPU efficiency is between 15 and 20% of the maximum theoretical performance. This issue is required to enlarge the memory bandwidth using high-bandwidth stacked memory. Next, different approaches based on FPGA, GPU, and CPU are accordingly detailed.
CPU-based approach
The well-behaved performance of the CPU nodes usually assists robust network connectivity, storage abilities, and large memory. Although CPU nodes are more common-purpose than those of FPGA or GPU, they lack the ability to match them in unprocessed computation facilities, since this requires increased network ability and a larger memory capacity.
GPU-based approach
GPUs are extremely effective for several basic DL primitives, which include greatly parallel-computing operations such as activation functions, matrix multiplication, and convolutions [
326
,
327
,
328
,
329
,
330
]. Incorporating HBM-stacked memory into the up-to-date GPU models significantly enhances the bandwidth. This enhancement allows numerous primitives to efficiently utilize all computational resources of the available GPUs. The improvement in GPU performance over CPU performance is usually 10-20:1 related to dense linear algebra operations.
Maximizing parallel processing is the base of the initial GPU programming model. For example, a GPU model may involve up to sixty-four computational units. There are four SIMD engines per each computational layer, and each SIMD has sixteen floating-point computation lanes. The peak performance is 25 TFLOPS (fp16) and 10 TFLOPS (fp32) as the percentage of the employment approaches 100%. Additional GPU performance may be achieved if the addition and multiply functions for vectors combine the inner production instructions for matching primitives related to matrix operations.
For DNN training, the GPU is usually considered to be an optimized design, while for inference operations, it may also offer considerable performance improvements.
FPGA-based approach
FPGA is wildly utilized in various tasks including deep learning [
199
,
247
,
331
,
332
,
333
,
334
]. Inference accelerators are commonly implemented utilizing FPGA. The FPGA can be effectively configured to reduce the unnecessary or overhead functions involved in GPU systems. Compared to GPU, the FPGA is restricted to both weak-behaved floating-point performance and integer inference. The main FPGA aspect is the capability to dynamically reconfigure the array characteristics (at run-time), as well as the capability to configure the array by means of effective design with little or no overhead.
As mentioned earlier, the FPGA offers both performance and latency for every watt it gains over GPU and CPU in DL inference operations. Implementation of custom high-performance hardware, pruned networks, and reduced arithmetic precision are three factors that enable the FPGA to implement DL algorithms and to achieve FPGA with this level of efficiency. In addition, FPGA may be employed to implement CNN overlay engines with over 80% efficiency, eight-bit accuracy, and over 15 TOPs peak performance; this is used for a few conventional CNNs, as Xillinx and partners demonstrated recently. By contrast, pruning techniques are mostly employed in the LSTM context. The sizes of the models can be efficiently minimized by up to 20×, which provides an important benefit during the implementation of the optimal solution, as MLP neural processing demonstrated. A recent study in the field of implementing fixed-point precision and custom floating-point has revealed that lowering the 8-bit is extremely promising; moreover, it aids in supplying additional advancements to implementing peak performance FPGA related to the DNN models.
Evaluation metrics
Evaluation metrics adopted within DL tasks play a crucial role in achieving the optimized classifier [
335
]. They are utilized within a usual data classification procedure through two main stages: training and testing. It is utilized to optimize the classification algorithm during the training stage. This means that the evaluation metric is utilized to discriminate and select the optimized solution, e.g., as a discriminator, which can generate an extra-accurate forecast of upcoming evaluations related to a specific classifier. For the time being, the evaluation metric is utilized to measure the efficiency of the created classifier, e.g. as an evaluator, within the model testing stage using hidden data. As given in Eq.
20
, TN and TP are defined as the number of negative and positive instances, respectively, which are successfully classified. In addition, FN and FP are defined as the number of misclassified positive and negative instances respectively. Next, some of the most well-known evaluation metrics are listed below.
1.
Accuracy: Calculates the ratio of correct predicted classes to the total number of samples evaluated (Eq.
20
).
$$ Accuracy = \frac{TP+TN }{TP+TN+FP+FN} $$
(20)
2.
Sensitivity or Recall: Utilized to calculate the fraction of positive patterns that are correctly classified (Eq.
21
).
$$ Sensitivity=\frac{TP}{TP+FN }$$
(21)
3.
Specificity: Utilized to calculate the fraction of negative patterns that are correctly classified (Eq.
22
).
$$ Specificity =\frac{TN }{FP+TN }$$
(22)
4.
Precision: Utilized to calculate the positive patterns that are correctly predicted by all predicted patterns in a positive class (Eq.
23
).
$$ Precision=\frac{TP }{TP+FP} $$
(23)
5.
F1-Score: Calculates the harmonic average between recall and precision rates (Eq.
24
).
$$ F1_{score} = 2\times \frac{Precision\times Recall}{Precision+Recall} $$
(24)
6.
J Score: This metric is also called Youdens J statistic. Eq.
25
represents the metric.
$$ J_{score} = Sensitivity + Specificity -1 $$
(25)
7.
False Positive Rate (FPR): This metric refers to the possibility of a false alarm ratio as calculated in Eq.
26
$$ FPR = 1- Specificity$$
(26)
8.
Area Under the ROC Curve: AUC is a common ranking type metric. It is utilized to conduct comparisons between learning algorithms [
336
,
337
,
338
], as well as to construct an optimal learning model [
339
,
340
]. In contrast to probability and threshold metrics, the AUC value exposes the entire classifier ranking performance. The following formula is used to calculate the AUC value for two-class problem [
341
] (Eq.
27
)
$$ AUC = \frac{S_{p}-n_{p} (n_{n}+1)/2}{n_{p}n_{n}} $$
(27)
Here,
\(S_{p}\)
represents the sum of all positive ranked samples. The number of negative and positive samples is denoted as
\(n_{n}\)
and
\(n_{p}\)
, respectively. Compared to the accuracy metrics, the AUC value was verified empirically and theoretically, making it very helpful for identifying an optimized solution and evaluating the classifier performance through classification training.
When considering the discrimination and evaluation processes, the AUC performance was brilliant. However, for multiclass issues, the AUC computation is primarily cost-effective when discriminating a large number of created solutions. In addition, the time complexity for computing the AUC is
\(O \left( |C|^{2} \; n\log n\right) \)
with respect to the Hand and Till AUC model [
341
] and
\(O \left( |C| \; n\log n\right) \)
according to Provost and Domingo’s AUC model [
336
].
Frameworks and datasets
Several DL frameworks and datasets have been developed in the last few years. various frameworks and libraries have also been used in order to expedite the work with good results. Through their use, the training process has become easier. Table
4
lists the most utilized frameworks and libraries.
Table 4 List of the most common frameworks and libraries
Full size table
Based on the star ratings on Github, as well as our own background in the field, TensorFlow is deemed the most effective and easy to use. It has the ability to work on several platforms. (Github is one of the biggest software hosting sites, while Github stars refer to how well-regarded a project is on the site). Moreover, there are several other benchmark datasets employed for different DL tasks. Some of these are listed in Table
5
.
Table 5 Benchmark datasets
Full size table
Summary and conclusion
Finally, it is mandatory the inclusion of a brief discussion by gathering all the relevant data provided along this extensive research. Next, an itemized analysis is presented in order to conclude our review and exhibit the future directions.
DL already experiences difficulties in simultaneously modeling multi-complex modalities of data. In recent DL developments, another common approach is that of multimodal DL.
DL requires sizeable datasets (labeled data preferred) to predict unseen data and to train the models. This challenge turns out to be particularly difficult when real-time data processing is required or when the provided datasets are limited (such as in the case of healthcare data). To alleviate this issue, TL and data augmentation have been researched over the last few years.
Although ML slowly transitions to semi-supervised and unsupervised learning to manage practical data without the need for manual human labeling, many of the current deep-learning models utilize supervised learning.
The CNN performance is greatly influenced by hyper-parameter selection. Any small change in the hyper-parameter values will affect the general CNN performance. Therefore, careful parameter selection is an extremely significant issue that should be considered during optimization scheme development.
Impressive and robust hardware resources like GPUs are required for effective CNN training. Moreover, they are also required for exploring the efficiency of using CNN in smart and embedded systems.
In the CNN context, ensemble learning [
342
,
343
] represents a prospective research area. The collection of different and multiple architectures will support the model in improving its generalizability across different image categories through extracting several levels of semantic image representation. Similarly, ideas such as new activation functions, dropout, and batch normalization also merit further investigation.
The exploitation of depth and different structural adaptations is significantly improved in the CNN learning capacity. Substituting the traditional layer configuration with blocks results in significant advances in CNN performance, as has been shown in the recent literature. Currently, developing novel and efficient block architectures is the main trend in new research models of CNN architectures. HRNet is only one example that shows there are always ways to improve the architecture.
It is expected that cloud-based platforms will play an essential role in the future development of computational DL applications. Utilizing cloud computing offers a solution to handling the enormous amount of data. It also helps to increase efficiency and reduce costs. Furthermore, it offers the flexibility to train DL architectures.
With the recent development in computational tools including a chip for neural networks and a mobile GPU, we will see more DL applications on mobile devices. It will be easier for users to use DL.
Regarding the issue of lack of training data, It is expected that various techniques of transfer learning will be considered such as training the DL model on large unlabeled image datasets and next transferring the knowledge to train the DL model on a small number of labeled images for the same task.
Last, this overview provides a starting point for the community of DL being interested in the field of DL. Furthermore, researchers would be allowed to decide the more suitable direction of work to be taken in order to provide more accurate alternatives to the field.
Availability of data and materials
Not applicable.
References
Rozenwald MB, Galitsyna AA, Sapunov GV, Khrameeva EE, Gelfand MS. A machine learning framework for the prediction of chromatin folding in
Drosophila
using epigenetic features. PeerJ Comput Sci. 2020;6:307.
Article
Google Scholar
Amrit C, Paauw T, Aly R, Lavric M. Identifying child abuse through text mining and machine learning. Expert Syst Appl. 2017;88:402–18.
Article
Google Scholar
Hossain E, Khan I, Un-Noor F, Sikander SS, Sunny MSH. Application of big data and machine learning in smart grid, and associated security concerns: a review. IEEE Access. 2019;7:13960–88.
Article
Google Scholar
Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine learning techniques. J Big Data. 2015;2(1):23.
Article
Google Scholar
Deldjoo Y, Elahi M, Cremonesi P, Garzotto F, Piazzolla P, Quadrana M. Content-based video recommendation system based on stylistic visual features. J Data Semant. 2016;5(2):99–113.
Article
Google Scholar
Al-Dulaimi K, Chandran V, Nguyen K, Banks J, Tomeo-Reyes I. Benchmarking hep-2 specimen cells classification using linear discriminant analysis on higher order spectra features of cell shape. Pattern Recogn Lett. 2019;125:534–41.
Article
Google Scholar
Liu W, Wang Z, Liu X, Zeng N, Liu Y, Alsaadi FE. A survey of deep neural network architectures and their applications. Neurocomputing. 2017;234:11–26.
Article
Google Scholar
Pouyanfar S, Sadiq S, Yan Y, Tian H, Tao Y, Reyes MP, Shyu ML, Chen SC, Iyengar S. A survey on deep learning: algorithms, techniques, and applications. ACM Comput Surv (CSUR). 2018;51(5):1–36.
Article
Google Scholar
Alom MZ, Taha TM, Yakopcic C, Westberg S, Sidike P, Nasrin MS, Hasan M, Van Essen BC, Awwal AA, Asari VK. A state-of-the-art survey on deep learning theory and architectures. Electronics. 2019;8(3):292.
Article
Google Scholar
Potok TE, Schuman C, Young S, Patton R, Spedalieri F, Liu J, Yao KT, Rose G, Chakma G. A study of complex deep learning networks on high-performance, neuromorphic, and quantum computers. ACM J Emerg Technol Comput Syst (JETC). 2018;14(2):1–21.
Article
Google Scholar
Adeel A, Gogate M, Hussain A. Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments. Inf Fusion. 2020;59:163–70.
Article
Google Scholar
Tian H, Chen SC, Shyu ML. Evolutionary programming based deep learning feature selection and network construction for visual data classification. Inf Syst Front. 2020;22(5):1053–66.
Article
Google Scholar
Young T, Hazarika D, Poria S, Cambria E. Recent trends in deep learning based natural language processing. IEEE Comput Intell Mag. 2018;13(3):55–75.
Article
Google Scholar
Koppe G, Meyer-Lindenberg A, Durstewitz D. Deep learning for small and big data in psychiatry. Neuropsychopharmacology. 2021;46(1):176–90.
Article
Google Scholar
Dalal N, Triggs B. Histograms of oriented gradients for human detection. In: 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05), vol. 1. IEEE; 2005. p. 886–93.
Lowe DG. Object recognition from local scale-invariant features. In: Proceedings of the seventh IEEE international conference on computer vision, vol. 2. IEEE; 1999. p. 1150–7.
Wu L, Hoi SC, Yu N. Semantics-preserving bag-of-words models and applications. IEEE Trans Image Process. 2010;19(7):1908–20.
Article
MathSciNet
MATH
Google Scholar
LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.
Article
Google Scholar
Yao G, Lei T, Zhong J. A review of convolutional-neural-network-based action recognition. Pattern Recogn Lett. 2019;118:14–22.
Article
Google Scholar
Dhillon A, Verma GK. Convolutional neural network: a review of models, methodologies and applications to object detection. Prog Artif Intell. 2020;9(2):85–112.
Article
Google Scholar
Khan A, Sohail A, Zahoora U, Qureshi AS. A survey of the recent architectures of deep convolutional neural networks. Artif Intell Rev. 2020;53(8):5455–516.
Article
Google Scholar
Hasan RI, Yusuf SM, Alzubaidi L. Review of the state of the art of deep learning for plant diseases: a broad analysis and discussion. Plants. 2020;9(10):1302.
Article
Google Scholar
Xiao Y, Tian Z, Yu J, Zhang Y, Liu S, Du S, Lan X. A review of object detection based on deep learning. Multimed Tools Appl. 2020;79(33):23729–91.
Article
Google Scholar
Ker J, Wang L, Rao J, Lim T. Deep learning applications in medical image analysis. IEEE Access. 2017;6:9375–89.
Article
Google Scholar
Zhang Z, Cui P, Zhu W. Deep learning on graphs: a survey. IEEE Trans Knowl Data Eng. 2020.
https://doi.org/10.1109/TKDE.2020.2981333
.
Article
Google Scholar
Shrestha A, Mahmood A. Review of deep learning algorithms and architectures. IEEE Access. 2019;7:53040–65.
Article
Google Scholar
Najafabadi MM, Villanustre F, Khoshgoftaar TM, Seliya N, Wald R, Muharemagic E. Deep learning applications and challenges in big data analytics. J Big Data. 2015;2(1):1.
Article
Google Scholar
Goodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning, vol. 1. Cambridge: MIT press; 2016.
MATH
Google Scholar
Shorten C, Khoshgoftaar TM, Furht B. Deep learning applications for COVID-19. J Big Data. 2021;8(1):1–54.
Article
Google Scholar
Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Commun ACM. 2017;60(6):84–90.
Article
Google Scholar
Bhowmick S, Nagarajaiah S, Veeraraghavan A. Vision and deep learning-based algorithms to detect and quantify cracks on concrete surfaces from uav videos. Sensors. 2020;20(21):6299.
Article
Google Scholar
Goh GB, Hodas NO, Vishnu A. Deep learning for computational chemistry. J Comput Chem. 2017;38(16):1291–307.
Article
Google Scholar
Li Y, Zhang T, Sun S, Gao X. Accelerating flash calculation through deep learning methods. J Comput Phys. 2019;394:153–65.
Article
MathSciNet
MATH
Google Scholar
Yang W, Zhang X, Tian Y, Wang W, Xue JH, Liao Q. Deep learning for single image super-resolution: a brief review. IEEE Trans Multimed. 2019;21(12):3106–21.
Article
Google Scholar
Tang J, Li S, Liu P. A review of lane detection methods based on deep learning. Pattern Recogn. 2020;111:107623.
Article
Google Scholar
Zhao ZQ, Zheng P, Xu ST, Wu X. Object detection with deep learning: a review. IEEE Trans Neural Netw Learn Syst. 2019;30(11):3212–32.
Article
Google Scholar
He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 770–8.
Ng A. Machine learning yearning: technical strategy for AI engineers in the era of deep learning. 2019.
https://www.mlyearning.org
.
Metz C. Turing award won by 3 pioneers in artificial intelligence. The New York Times. 2019;27.
Nevo S, Anisimov V, Elidan G, El-Yaniv R, Giencke P, Gigi Y, Hassidim A, Moshe Z, Schlesinger M, Shalev G, et al. Ml for flood forecasting at scale; 2019. arXiv preprint
arXiv:1901.09583
.
Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T. The rise of deep learning in drug discovery. Drug Discov Today. 2018;23(6):1241–50.
Article
Google Scholar
Benhammou Y, Achchab B, Herrera F, Tabik S. Breakhis based breast cancer automatic diagnosis using deep learning: taxonomy, survey and insights. Neurocomputing. 2020;375:9–24.
Article
Google Scholar
Wulczyn E, Steiner DF, Xu Z, Sadhwani A, Wang H, Flament-Auvigne I, Mermel CH, Chen PHC, Liu Y, Stumpe MC. Deep learning-based survival prediction for multiple cancer types using histopathology images. PLoS ONE. 2020;15(6):e0233678.
Article
Google Scholar
Nagpal K, Foote D, Liu Y, Chen PHC, Wulczyn E, Tan F, Olson N, Smith JL, Mohtashamian A, Wren JH, et al. Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. NPJ Digit Med. 2019;2(1):1–10.
Google Scholar
Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer with deep neural networks. Nature. 2017;542(7639):115–8.
Article
Google Scholar
Brunese L, Mercaldo F, Reginelli A, Santone A. Explainable deep learning for pulmonary disease and coronavirus COVID-19 detection from X-rays. Comput Methods Programs Biomed. 2020;196(105):608.
Google Scholar
Jamshidi M, Lalbakhsh A, Talla J, Peroutka Z, Hadjilooei F, Lalbakhsh P, Jamshidi M, La Spada L, Mirmozafari M, Dehghani M, et al. Artificial intelligence and COVID-19: deep learning approaches for diagnosis and treatment. IEEE Access. 2020;8:109581–95.
Article
Google Scholar
Shorfuzzaman M, Hossain MS. Metacovid: a siamese neural network framework with contrastive loss for n-shot diagnosis of COVID-19 patients. Pattern Recogn. 2020;113:107700.
Article
Google Scholar
Carvelli L, Olesen AN, Brink-Kjær A, Leary EB, Peppard PE, Mignot E, Sørensen HB, Jennum P. Design of a deep learning model for automatic scoring of periodic and non-periodic leg movements during sleep validated against multiple human experts. Sleep Med. 2020;69:109–19.
Article
Google Scholar
De Fauw J, Ledsam JR, Romera-Paredes B, Nikolov S, Tomasev N, Blackwell S, Askham H, Glorot X, O’Donoghue B, Visentin D, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat Med. 2018;24(9):1342–50.
Article
Google Scholar
Topol EJ. High-performance medicine: the convergence of human and artificial intelligence. Nat Med. 2019;25(1):44–56.
Article
Google Scholar
Kermany DS, Goldbaum M, Cai W, Valentim CC, Liang H, Baxter SL, McKeown A, Yang G, Wu X, Yan F, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31.
Article
Google Scholar
Van Essen B, Kim H, Pearce R, Boakye K, Chen B. Lbann: livermore big artificial neural network HPC toolkit. In: Proceedings of the workshop on machine learning in high-performance computing environments; 2015. p. 1–6.
Saeed MM, Al Aghbari Z, Alsharidah M. Big data clustering techniques based on spark: a literature review. PeerJ Comput Sci. 2020;6:321.
Article
Google Scholar
Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, et al. Human-level control through deep reinforcement learning. Nature. 2015;518(7540):529–33.
Article
Google Scholar
Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. Deep reinforcement learning: a brief survey. IEEE Signal Process Mag. 2017;34(6):26–38.
Article
Google Scholar
Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compositionality over a sentiment treebank. In: Proceedings of the 2013 conference on empirical methods in natural language processing; 2013. p. 1631–42.
Goller C, Kuchler A. Learning task-dependent distributed representations by backpropagation through structure. In: Proceedings of international conference on neural networks (ICNN’96), vol 1. IEEE; 1996. p. 347–52.
Socher R, Lin CCY, Ng AY, Manning CD. Parsing natural scenes and natural language with recursive neural networks. In: ICML; 2011.
Louppe G, Cho K, Becot C, Cranmer K. QCD-aware recursive neural networks for jet physics. J High Energy Phys. 2019;2019(1):57.
Article
Google Scholar
Sadr H, Pedram MM, Teshnehlab M. A robust sentiment analysis method based on sequential combination of convolutional and recursive neural networks. Neural Process Lett. 2019;50(3):2745–61.
Article
Google Scholar
Urban G, Subrahmanya N, Baldi P. Inner and outer recursive neural networks for chemoinformatics applications. J Chem Inf Model. 2018;58(2):207–11.
Article
Google Scholar
Hewamalage H, Bergmeir C, Bandara K. Recurrent neural networks for time series forecasting: current status and future directions. Int J Forecast. 2020;37(1):388–427.
Article
Google Scholar
Jiang Y, Kim H, Asnani H, Kannan S, Oh S, Viswanath P. Learn codes: inventing low-latency codes via recurrent neural networks. IEEE J Sel Areas Inf Theory. 2020;1(1):207–16.
Article
Google Scholar
John RA, Acharya J, Zhu C, Surendran A, Bose SK, Chaturvedi A, Tiwari N, Gao Y, He Y, Zhang KK, et al. Optogenetics inspired transition metal dichalcogenide neuristors for in-memory deep recurrent neural networks. Nat Commun. 2020;11(1):1–9.
Article
Google Scholar
Batur Dinler Ö, Aydin N. An optimal feature parameter set based on gated recurrent unit recurrent neural networks for speech segment detection. Appl Sci. 2020;10(4):1273.
Article
Google Scholar
Jagannatha AN, Yu H. Structured prediction models for RNN based sequence labeling in clinical text. In: Proceedings of the conference on empirical methods in natural language processing. conference on empirical methods in natural language processing, vol. 2016, NIH Public Access; 2016. p. 856.
Pascanu R, Gulcehre C, Cho K, Bengio Y. How to construct deep recurrent neural networks. In: Proceedings of the second international conference on learning representations (ICLR 2014); 2014.
Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the thirteenth international conference on artificial intelligence and statistics; 2010. p. 249–56.
Gao C, Yan J, Zhou S, Varshney PK, Liu H. Long short-term memory-based deep recurrent neural networks for target tracking. Inf Sci. 2019;502:279–96.
Article
Google Scholar
Zhou DX. Theory of deep convolutional neural networks: downsampling. Neural Netw. 2020;124:319–27.
Article
MATH
Google Scholar
Jhong SY, Tseng PY, Siriphockpirom N, Hsia CH, Huang MS, Hua KL, Chen YY. An automated biometric identification system using CNN-based palm vein recognition. In: 2020 international conference on advanced robotics and intelligent systems (ARIS). IEEE; 2020. p. 1–6.
Al-Azzawi A, Ouadou A, Max H, Duan Y, Tanner JJ, Cheng J. Deepcryopicker: fully automated deep neural network for single protein particle picking in cryo-EM. BMC Bioinform. 2020;21(1):1–38.
Article
Google Scholar
Wang T, Lu C, Yang M, Hong F, Liu C. A hybrid method for heartbeat classification via convolutional neural networks, multilayer perceptrons and focal loss. PeerJ Comput Sci. 2020;6:324.
Article
Google Scholar
Li G, Zhang M, Li J, Lv F, Tong G. Efficient densely connected convolutional neural networks. Pattern Recogn. 2021;109:107610.
Article
Google Scholar
Gu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai B, Liu T, Wang X, Wang G, Cai J, et al. Recent advances in convolutional neural networks. Pattern Recogn. 2018;77:354–77.
Article
Google Scholar
Fang W, Love PE, Luo H, Ding L. Computer vision for behaviour-based safety in construction: a review and future directions. Adv Eng Inform. 2020;43:100980.
Article
Google Scholar
Palaz D, Magimai-Doss M, Collobert R. End-to-end acoustic modeling using convolutional neural networks for hmm-based automatic speech recognition. Speech Commun. 2019;108:15–32.
Article
Google Scholar
Li HC, Deng ZY, Chiang HH. Lightweight and resource-constrained learning network for face recognition with performance optimization. Sensors. 2020;20(21):6114.
Article
Google Scholar
Hubel DH, Wiesel TN. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. J Physiol. 1962;160(1):106.
Article
Google Scholar
Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift; 2015. arXiv preprint
arXiv:1502.03167
.
Ruder S. An overview of gradient descent optimization algorithms; 2016. arXiv preprint
arXiv:1609.04747
.
Bottou L. Large-scale machine learning with stochastic gradient descent. In: Proceedings of COMPSTAT’2010. Springer; 2010. p. 177–86.
Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. Cited on. 2012;14(8).
Zhang Z. Improved Adam optimizer for deep neural networks. In: 2018 IEEE/ACM 26th international symposium on quality of service (IWQoS). IEEE; 2018. p. 1–2.
Alzubaidi L, Fadhel MA, Al-Shamma O, Zhang J, Duan Y. Deep learning models for classification of red blood cells in microscopy images to aid in sickle cell anemia diagnosis. Electronics. 2020;9(3):427.
Article
Google Scholar
Alzubaidi L, Fadhel MA, Al-Shamma O, Zhang J, Santamaría J, Duan Y, Oleiwi SR. Towards a better understanding of transfer learning for medical imaging: a case study. Appl Sci. 2020;10(13):4523.
Article
Google Scholar
Alzubaidi L, Al-Shamma O, Fadhel MA, Farhan L, Zhang J, Duan Y. Optimizing the performance of breast cancer classification by employing the same domain transfer learning from hybrid deep convolutional neural network model. Electronics. 2020;9(3):445.
Article
Google Scholar
LeCun Y, Jackel LD, Bottou L, Cortes C, Denker JS, Drucker H, Guyon I, Muller UA, Sackinger E, Simard P, et al. Learning algorithms for classification: a comparison on handwritten digit recognition. Neural Netw Stat Mech Perspect. 1995;261:276.
Google Scholar
Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929–58.
MathSciNet
MATH
Google Scholar
Dahl GE, Sainath TN, Hinton GE. Improving deep neural networks for LVCSR using rectified linear units and dropout. In: 2013 IEEE international conference on acoustics, speech and signal processing. IEEE; 2013. p. 8609–13.
Xu B, Wang N, Chen T, Li M. Empirical evaluation of rectified activations in convolutional network; 2015. arXiv preprint
arXiv:1505.00853
.
Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int J Uncertain Fuzziness Knowl Based Syst. 1998;6(02):107–16.
Article
MATH
Google Scholar
Lin M, Chen Q, Yan S. Network in network; 2013. arXiv preprint
arXiv:1312.4400
.
Hsiao TY, Chang YC, Chou HH, Chiu CT. Filter-based deep-compression with global average pooling for convolutional networks. J Syst Arch. 2019;95:9–18.
Article
Google Scholar
Li Z, Wang SH, Fan RR, Cao G, Zhang YD, Guo T. Teeth category classification via seven-layer deep convolutional neural network with max pooling and global average pooling. Int J Imaging Syst Technol. 2019;29(4):577–83.
Article
Google Scholar
Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In: European conference on computer vision. Springer; 2014. p. 818–33.
Erhan D, Bengio Y, Courville A, Vincent P. Visualizing higher-layer features of a deep network. Univ Montreal. 2009;1341(3):1.
Google Scholar
Le QV. Building high-level features using large scale unsupervised learning. In: 2013 IEEE international conference on acoustics, speech and signal processing. IEEE; 2013. p. 8595–8.
Grün F, Rupprecht C, Navab N, Tombari F. A taxonomy and library for visualizing learned features in convolutional neural networks; 2016. arXiv preprint
arXiv:1606.07757
.
Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition; 2014. arXiv preprint
arXiv:1409.1556
.
Ranzato M, Huang FJ, Boureau YL, LeCun Y. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In: 2007 IEEE conference on computer vision and pattern recognition. IEEE; 2007. p. 1–8.
Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2015. p. 1–9.
Bengio Y, et al. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization; 2015.
arXiv:1502.04390
corr abs/1502.04390
Srivastava RK, Greff K, Schmidhuber J. Highway networks; 2015. arXiv preprint
arXiv:1505.00387
.
Kong W, Dong ZY, Jia Y, Hill DJ, Xu Y, Zhang Y. Short-term residential load forecasting based on LSTM recurrent neural network. IEEE Trans Smart Grid. 2017;10(1):841–51.
Article
Google Scholar
Ordóñez FJ, Roggen D. Deep convolutional and LSTM recurrent neural networks for multimodal wearable activity recognition. Sensors. 2016;16(1):115.
Article
Google Scholar
CireşAn D, Meier U, Masci J, Schmidhuber J. Multi-column deep neural network for traffic sign classification. Neural Netw. 2012;32:333–8.
Article
Google Scholar
Szegedy C, Ioffe S, Vanhoucke V, Alemi A. Inception-v4, inception-resnet and the impact of residual connections on learning; 2016. arXiv preprint
arXiv:1602.07261
.
Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 2818–26.
Wu S, Zhong S, Liu Y. Deep residual learning for image steganalysis. Multimed Tools Appl. 2018;77(9):10437–53.
Article
Google Scholar
Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 4700–08.
Rubin J, Parvaneh S, Rahman A, Conroy B, Babaeizadeh S. Densely connected convolutional networks for detection of atrial fibrillation from short single-lead ECG recordings. J Electrocardiol. 2018;51(6):S18-21.
Article
Google Scholar
Kuang P, Ma T, Chen Z, Li F. Image super-resolution with densely connected convolutional networks. Appl Intell. 2019;49(1):125–36.
Article
Google Scholar
Xie S, Girshick R, Dollár P, Tu Z, He K. Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 1492–500.
Su A, He X, Zhao X. Jpeg steganalysis based on ResNeXt with gauss partial derivative filters. Multimed Tools Appl. 2020;80(3):3349–66.
Article
Google Scholar
Yadav D, Jalal A, Garlapati D, Hossain K, Goyal A, Pant G. Deep learning-based ResNeXt model in phycological studies for future. Algal Res. 2020;50:102018.
Article
Google Scholar
Han W, Feng R, Wang L, Gao L. Adaptive spatial-scale-aware deep convolutional neural network for high-resolution remote sensing imagery scene classification. In: IGARSS 2018-2018 IEEE international geoscience and remote sensing symposium. IEEE; 2018. p. 4736–9.
Zagoruyko S, Komodakis N. Wide residual networks; 2016. arXiv preprint
arXiv:1605.07146
.
Huang G, Sun Y, Liu Z, Sedra D, Weinberger KQ. Deep networks with stochastic depth. In: European conference on computer vision. Springer; 2016. p. 646–61.
Huynh HT, Nguyen H. Joint age estimation and gender classification of Asian faces using wide ResNet. SN Comput Sci. 2020;1(5):1–9.
Article
Google Scholar
Takahashi R, Matsubara T, Uehara K. Data augmentation using random image cropping and patching for deep cnns. IEEE Trans Circuits Syst Video Technol. 2019;30(9):2917–31.
Article
Google Scholar
Han D, Kim J, Kim J. Deep pyramidal residual networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 5927–35.
Wang Y, Wang L, Wang H, Li P. End-to-end image super-resolution via deep and shallow convolutional networks. IEEE Access. 2019;7:31959–70.
Article
Google Scholar
Chollet F. Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 1251–8.
Lo WW, Yang X, Wang Y. An xception convolutional neural network for malware classification with transfer learning. In: 2019 10th IFIP international conference on new technologies, mobility and security (NTMS). IEEE; 2019. p. 1–5.
Rahimzadeh M, Attar A. A modified deep convolutional neural network for detecting COVID-19 and pneumonia from chest X-ray images based on the concatenation of xception and resnet50v2. Inform Med Unlocked. 2020;19:100360.
Article
Google Scholar
Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X. Residual attention network for image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 3156–64.
Salakhutdinov R, Larochelle H. Efficient learning of deep boltzmann machines. In: Proceedings of the thirteenth international conference on artificial intelligence and statistics; 2010. p. 693–700.
Goh H, Thome N, Cord M, Lim JH. Top-down regularization of deep belief networks. Adv Neural Inf Process Syst. 2013;26:1878–86.
Google Scholar
Guan J, Lai R, Xiong A, Liu Z, Gu L. Fixed pattern noise reduction for infrared images based on cascade residual attention CNN. Neurocomputing. 2020;377:301–13.
Article
Google Scholar
Bi Q, Qin K, Zhang H, Li Z, Xu K. RADC-Net: a residual attention based convolution network for aerial scene classification. Neurocomputing. 2020;377:345–59.
Article
Google Scholar
Jaderberg M, Simonyan K, Zisserman A, et al. Spatial transformer networks. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2015. p. 2017–25.
Google Scholar
Hu J, Shen L, Sun G. Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018. p. 7132–41.
Mou L, Zhu XX. Learning to pay attention on spectral domain: a spectral attention module-based convolutional network for hyperspectral image classification. IEEE Trans Geosci Remote Sens. 2019;58(1):110–22.
Article
Google Scholar
Woo S, Park J, Lee JY, So Kweon I. CBAM: Convolutional block attention module. In: Proceedings of the European conference on computer vision (ECCV); 2018. p. 3–19.
Roy AG, Navab N, Wachinger C. Concurrent spatial and channel ‘squeeze & excitation’ in fully convolutional networks. In: International conference on medical image computing and computer-assisted intervention. Springer; 2018. p. 421–9.
Roy AG, Navab N, Wachinger C. Recalibrating fully convolutional networks with spatial and channel “squeeze and excitation’’ blocks. IEEE Trans Med Imaging. 2018;38(2):540–9.
Article
Google Scholar
Sabour S, Frosst N, Hinton GE. Dynamic routing between capsules. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2017. p. 3856–66.
Google Scholar
Arun P, Buddhiraju KM, Porwal A. Capsulenet-based spatial-spectral classifier for hyperspectral images. IEEE J Sel Topics Appl Earth Obs Remote Sens. 2019;12(6):1849–65.
Article
Google Scholar
Xinwei L, Lianghao X, Yi Y. Compact video fingerprinting via an improved capsule net. Syst Sci Control Eng. 2020;9:1–9.
Google Scholar
Ma B, Li X, Xia Y, Zhang Y. Autonomous deep learning: a genetic DCNN designer for image classification. Neurocomputing. 2020;379:152–61.
Article
Google Scholar
Wang J, Sun K, Cheng T, Jiang B, Deng C, Zhao Y, Liu D, Mu Y, Tan M, Wang X, et al. Deep high-resolution representation learning for visual recognition. IEEE Trans Pattern Anal Mach Intell. 2020.
https://doi.org/10.1109/TPAMI.2020.2983686
.
Article
Google Scholar
Cheng B, Xiao B, Wang J, Shi H, Huang TS, Zhang L. Higherhrnet: scale-aware representation learning for bottom-up human pose estimation. In: CVPR 2020; 2020.
https://www.microsoft.com/en-us/research/publication/higherhrnet-scale-aware-representation-learning-for-bottom-up-human-pose-estimation/
.
Karimi H, Derr T, Tang J. Characterizing the decision boundary of deep neural networks; 2019. arXiv preprint
arXiv:1912.11460
.
Li Y, Ding L, Gao X. On the decision boundary of deep neural networks; 2018. arXiv preprint
arXiv:1808.05385
.
Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2014. p. 3320–8.
Google Scholar
Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A survey on deep transfer learning. In: International conference on artificial neural networks. Springer; 2018. p. 270–9.
Weiss K, Khoshgoftaar TM, Wang D. A survey of transfer learning. J Big Data. 2016;3(1):9.
Article
Google Scholar
Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. J Big Data. 2019;6(1):60.
Article
Google Scholar
Wang F, Wang H, Wang H, Li G, Situ G. Learning from simulation: an end-to-end deep-learning approach for computational ghost imaging. Opt Express. 2019;27(18):25560–72.
Article
Google Scholar
Pan W. A survey of transfer learning for collaborative recommendation with auxiliary data. Neurocomputing. 2016;177:447–53.
Article
Google Scholar
Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L. Imagenet: a large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. IEEE; 2009. p. 248–55.
Cook D, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2013;36(3):537–56.
Article
Google Scholar
Cao X, Wang Z, Yan P, Li X. Transfer learning for pedestrian detection. Neurocomputing. 2013;100:51–7.
Article
Google Scholar
Raghu M, Zhang C, Kleinberg J, Bengio S. Transfusion: understanding transfer learning for medical imaging. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2019. p. 3347–57.
Google Scholar
Pham TN, Van Tran L, Dao SVT. Early disease classification of mango leaves using feed-forward neural network and hybrid metaheuristic feature selection. IEEE Access. 2020;8:189960–73.
Article
Google Scholar
Saleh AM, Hamoud T. Analysis and best parameters selection for person recognition based on gait model using CNN algorithm and image augmentation. J Big Data. 2021;8(1):1–20.
Article
Google Scholar
Hirahara D, Takaya E, Takahara T, Ueda T. Effects of data count and image scaling on deep learning training. PeerJ Comput Sci. 2020;6:312.
Article
Google Scholar
Moreno-Barea FJ, Strazzera F, Jerez JM, Urda D, Franco L. Forward noise adjustment scheme for data augmentation. In: 2018 IEEE symposium series on computational intelligence (SSCI). IEEE; 2018. p. 728–34.
Dua D, Karra Taniskidou E. Uci machine learning repository. Irvine: University of california. School of Information and Computer Science; 2017.
http://archive.ics.uci.edu/ml
Johnson JM, Khoshgoftaar TM. Survey on deep learning with class imbalance. J Big Data. 2019;6(1):27.
Article
Google Scholar
Yang P, Zhang Z, Zhou BB, Zomaya AY. Sample subset optimization for classifying imbalanced biological data. In: Pacific-Asia conference on knowledge discovery and data mining. Springer; 2011. p. 333–44.
Yang P, Yoo PD, Fernando J, Zhou BB, Zhang Z, Zomaya AY. Sample subset optimization techniques for imbalanced and ensemble learning problems in bioinformatics applications. IEEE Trans Cybern. 2013;44(3):445–55.
Article
Google Scholar
Wang S, Sun S, Xu J. Auc-maximized deep convolutional neural fields for sequence labeling 2015. arXiv preprint
arXiv:1511.05265
.
Li Y, Wang S, Umarov R, Xie B, Fan M, Li L, Gao X. Deepre: sequence-based enzyme EC number prediction by deep learning. Bioinformatics. 2018;34(5):760–9.
Article
Google Scholar
Li Y, Huang C, Ding L, Li Z, Pan Y, Gao X. Deep learning in bioinformatics: introduction, application, and perspective in the big data era. Methods. 2019;166:4–21.
Article
Google Scholar
Choi E, Bahadori MT, Sun J, Kulas J, Schuetz A, Stewart W. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2016. p. 3504–12.
Google Scholar
Ching T, Himmelstein DS, Beaulieu-Jones BK, Kalinin AA, Do BT, Way GP, Ferrero E, Agapow PM, Zietz M, Hoffman MM, et al. Opportunities and obstacles for deep learning in biology and medicine. J R Soc Interface. 2018;15(141):20170,387.
Article
Google Scholar
Zhou J, Troyanskaya OG. Predicting effects of noncoding variants with deep learning-based sequence model. Nat Methods. 2015;12(10):931–4.
Article
Google Scholar
Pokuri BSS, Ghosal S, Kokate A, Sarkar S, Ganapathysubramanian B. Interpretable deep learning for guided microstructure-property explorations in photovoltaics. NPJ Comput Mater. 2019;5(1):1–11.
Article
Google Scholar
Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016. p. 1135–44.
Wang L, Nie R, Yu Z, Xin R, Zheng C, Zhang Z, Zhang J, Cai J. An interpretable deep-learning architecture of capsule networks for identifying cell-type gene expression programs from single-cell RNA-sequencing data. Nat Mach Intell. 2020;2(11):1–11.
Article
Google Scholar
Sundararajan M, Taly A, Yan Q. Axiomatic attribution for deep networks; 2017. arXiv preprint
arXiv:1703.01365
.
Platt J, et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Adv Large Margin Classif. 1999;10(3):61–74.
Google Scholar
Nair T, Precup D, Arnold DL, Arbel T. Exploring uncertainty measures in deep networks for multiple sclerosis lesion detection and segmentation. Med Image Anal. 2020;59:101557.
Article
Google Scholar
Herzog L, Murina E, Dürr O, Wegener S, Sick B. Integrating uncertainty in deep neural networks for MRI based stroke analysis. Med Image Anal. 2020;65:101790.
Article
Google Scholar
Pereyra G, Tucker G, Chorowski J, Kaiser Ł, Hinton G. Regularizing neural networks by penalizing confident output distributions; 2017. arXiv preprint
arXiv:1701.06548
.
Naeini MP, Cooper GF, Hauskrecht M. Obtaining well calibrated probabilities using bayesian binning. In: Proceedings of the... AAAI conference on artificial intelligence. AAAI conference on artificial intelligence, vol. 2015. NIH Public Access; 2015. p. 2901.
Li M, Sethi IK. Confidence-based classifier design. Pattern Recogn. 2006;39(7):1230–40.
Article
MATH
Google Scholar
Zadrozny B, Elkan C. Obtaining calibrated probability estimates from decision trees and Naive Bayesian classifiers. In: ICML, vol. 1, Citeseer; 2001. p. 609–16.
Steinwart I. Consistency of support vector machines and other regularized kernel classifiers. IEEE Trans Inf Theory. 2005;51(1):128–42.
Article
MathSciNet
MATH
Google Scholar
Lee K, Lee K, Shin J, Lee H. Overcoming catastrophic forgetting with unlabeled data in the wild. In: Proceedings of the IEEE international conference on computer vision; 2019. p. 312–21.
Shmelkov K, Schmid C, Alahari K. Incremental learning of object detectors without catastrophic forgetting. In: Proceedings of the IEEE international conference on computer vision; 2017. p. 3400–09.
Zenke F, Gerstner W, Ganguli S. The temporal paradox of Hebbian learning and homeostatic plasticity. Curr Opin Neurobiol. 2017;43:166–76.
Article
Google Scholar
Andersen N, Krauth N, Nabavi S. Hebbian plasticity in vivo: relevance and induction. Curr Opin Neurobiol. 2017;45:188–92.
Article
Google Scholar
Zheng R, Chakraborti S. A phase ii nonparametric adaptive exponentially weighted moving average control chart. Qual Eng. 2016;28(4):476–90.
Article
Google Scholar
Rebuffi SA, Kolesnikov A, Sperl G, Lampert CH. ICARL: Incremental classifier and representation learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 2001–10.
Hinton GE, Plaut DC. Using fast weights to deblur old memories. In: Proceedings of the ninth annual conference of the cognitive science society; 1987. p. 177–86.
Parisi GI, Kemker R, Part JL, Kanan C, Wermter S. Continual lifelong learning with neural networks: a review. Neural Netw. 2019;113:54–71.
Article
Google Scholar
Soltoggio A, Stanley KO, Risi S. Born to learn: the inspiration, progress, and future of evolved plastic artificial neural networks. Neural Netw. 2018;108:48–67.
Article
Google Scholar
Parisi GI, Tani J, Weber C, Wermter S. Lifelong learning of human actions with deep neural network self-organization. Neural Netw. 2017;96:137–49.
Article
Google Scholar
Cheng Y, Wang D, Zhou P, Zhang T. Model compression and acceleration for deep neural networks: the principles, progress, and challenges. IEEE Signal Process Mag. 2018;35(1):126–36.
Article
Google Scholar
Wiedemann S, Kirchhoffer H, Matlage S, Haase P, Marban A, Marinč T, Neumann D, Nguyen T, Schwarz H, Wiegand T, et al. Deepcabac: a universal compression algorithm for deep neural networks. IEEE J Sel Topics Signal Process. 2020;14(4):700–14.
Article
Google Scholar
Mehta N, Pandit A. Concurrence of big data analytics and healthcare: a systematic review. Int J Med Inform. 2018;114:57–65.
Article
Google Scholar
Esteva A, Robicquet A, Ramsundar B, Kuleshov V, DePristo M, Chou K, Cui C, Corrado G, Thrun S, Dean J. A guide to deep learning in healthcare. Nat Med. 2019;25(1):24–9.
Article
Google Scholar
Shawahna A, Sait SM, El-Maleh A. Fpga-based accelerators of deep learning networks for learning and classification: a review. IEEE Access. 2018;7:7823–59.
Article
Google Scholar
Min Z. Public welfare organization management system based on FPGA and deep learning. Microprocess Microsyst. 2020;80:103333.
Article
Google Scholar
Al-Shamma O, Fadhel MA, Hameed RA, Alzubaidi L, Zhang J. Boosting convolutional neural networks performance based on fpga accelerator. In: International conference on intelligent systems design and applications. Springer; 2018. p. 509–17.
Han S, Mao H, Dally WJ. Deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding; 2015. arXiv preprint
arXiv:1510.00149
.
Chen Z, Zhang L, Cao Z, Guo J. Distilling the knowledge from handcrafted features for human activity recognition. IEEE Trans Ind Inform. 2018;14(10):4334–42.
Article
Google Scholar
Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network; 2015. arXiv preprint
arXiv:1503.02531
.
Lenssen JE, Fey M, Libuschewski P. Group equivariant capsule networks. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2018. p. 8844–53.
Google Scholar
Denton EL, Zaremba W, Bruna J, LeCun Y, Fergus R. Exploiting linear structure within convolutional networks for efficient evaluation. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2014. p. 1269–77.
Google Scholar
Xu Q, Zhang M, Gu Z, Pan G. Overfitting remedy by sparsifying regularization on fully-connected layers of CNNs. Neurocomputing. 2019;328:69–74.
Article
Google Scholar
Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. Commun ACM. 2018;64(3):107–15.
Article
Google Scholar
Xu X, Jiang X, Ma C, Du P, Li X, Lv S, Yu L, Ni Q, Chen Y, Su J, et al. A deep learning system to screen novel coronavirus disease 2019 pneumonia. Engineering. 2020;6(10):1122–9.
Article
Google Scholar
Sharma K, Alsadoon A, Prasad P, Al-Dala’in T, Nguyen TQV, Pham DTH. A novel solution of using deep learning for left ventricle detection: enhanced feature extraction. Comput Methods Programs Biomed. 2020;197:105751.
Article
Google Scholar
Zhang G, Wang C, Xu B, Grosse R. Three mechanisms of weight decay regularization; 2018. arXiv preprint
arXiv:1810.12281
.
Laurent C, Pereyra G, Brakel P, Zhang Y, Bengio Y. Batch normalized recurrent neural networks. In: 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), IEEE; 2016. p. 2657–61.
Salamon J, Bello JP. Deep convolutional neural networks and data augmentation for environmental sound classification. IEEE Signal Process Lett. 2017;24(3):279–83.
Article
Google Scholar
Wang X, Qin Y, Wang Y, Xiang S, Chen H. ReLTanh: an activation function with vanishing gradient resistance for SAE-based DNNs and its application to rotating machinery fault diagnosis. Neurocomputing. 2019;363:88–98.
Article
Google Scholar
Tan HH, Lim KH. Vanishing gradient mitigation with deep learning neural network optimization. In: 2019 7th international conference on smart computing & communications (ICSCC). IEEE; 2019. p. 1–4.
MacDonald G, Godbout A, Gillcash B, Cairns S. Volume-preserving neural networks: a solution to the vanishing gradient problem; 2019. arXiv preprint
arXiv:1911.09576
.
Mittal S, Vaishay S. A survey of techniques for optimizing deep learning on GPUs. J Syst Arch. 2019;99:101635.
Article
Google Scholar
Kanai S, Fujiwara Y, Iwamura S. Preventing gradient explosions in gated recurrent units. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2017. p. 435–44.
Google Scholar
Hanin B. Which neural net architectures give rise to exploding and vanishing gradients? In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2018. p. 582–91.
Google Scholar
Ribeiro AH, Tiels K, Aguirre LA, Schön T. Beyond exploding and vanishing gradients: analysing RNN training using attractors and smoothness. In: International conference on artificial intelligence and statistics, PMLR; 2020. p. 2370–80.
D’Amour A, Heller K, Moldovan D, Adlam B, Alipanahi B, Beutel A, Chen C, Deaton J, Eisenstein J, Hoffman MD, et al. Underspecification presents challenges for credibility in modern machine learning; 2020. arXiv preprint
arXiv:2011.03395
.
Chea P, Mandell JC. Current applications and future directions of deep learning in musculoskeletal radiology. Skelet Radiol. 2020;49(2):1–15.
Article
Google Scholar
Wu X, Sahoo D, Hoi SC. Recent advances in deep learning for object detection. Neurocomputing. 2020;396:39–64.
Article
Google Scholar
Kuutti S, Bowden R, Jin Y, Barber P, Fallah S. A survey of deep learning applications to autonomous vehicle control. IEEE Trans Intell Transp Syst. 2020;22:712–33.
Article
Google Scholar
Yolcu G, Oztel I, Kazan S, Oz C, Bunyak F. Deep learning-based face analysis system for monitoring customer interest. J Ambient Intell Humaniz Comput. 2020;11(1):237–48.
Article
Google Scholar
Jiao L, Zhang F, Liu F, Yang S, Li L, Feng Z, Qu R. A survey of deep learning-based object detection. IEEE Access. 2019;7:128837–68.
Article
Google Scholar
Muhammad K, Khan S, Del Ser J, de Albuquerque VHC. Deep learning for multigrade brain tumor classification in smart healthcare systems: a prospective survey. IEEE Trans Neural Netw Learn Syst. 2020;32:507–22.
Article
Google Scholar
Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, Van Der Laak JA, Van Ginneken B, Sánchez CI. A survey on deep learning in medical image analysis. Med Image Anal. 2017;42:60–88.
Article
Google Scholar
Mukherjee D, Mondal R, Singh PK, Sarkar R, Bhattacharjee D. Ensemconvnet: a deep learning approach for human activity recognition using smartphone sensors for healthcare applications. Multimed Tools Appl. 2020;79(41):31663–90.
Article
Google Scholar
Zeleznik R, Foldyna B, Eslami P, Weiss J, Alexander I, Taron J, Parmar C, Alvi RM, Banerji D, Uno M, et al. Deep convolutional neural networks to predict cardiovascular risk from computed tomography. Nature Commun. 2021;12(1):1–9.
Article
Google Scholar
Wang J, Liu Q, Xie H, Yang Z, Zhou H. Boosted efficientnet: detection of lymph node metastases in breast cancer using convolutional neural networks. Cancers. 2021;13(4):661.
Article
Google Scholar
Yu H, Yang LT, Zhang Q, Armstrong D, Deen MJ. Convolutional neural networks for medical image analysis: state-of-the-art, comparisons, improvement and perspectives. Neurocomputing. 2021.
https://doi.org/10.1016/j.neucom.2020.04.157
.
Article
Google Scholar
Bharati S, Podder P, Mondal MRH. Hybrid deep learning for detecting lung diseases from X-ray images. Inform Med Unlocked. 2020;20:100391.
Article
Google Scholar
Dong Y, Pan Y, Zhang J, Xu W. Learning to read chest X-ray images from 16000+ examples using CNN. In: 2017 IEEE/ACM international conference on connected health: applications, systems and engineering technologies (CHASE). IEEE; 2017. p. 51–7.
Rajkomar A, Lingam S, Taylor AG, Blum M, Mongan J. High-throughput classification of radiographs using deep convolutional neural networks. J Digit Imaging. 2017;30(1):95–101.
Article
Google Scholar
Rajpurkar P, Irvin J, Zhu K, Yang B, Mehta H, Duan T, Ding D, Bagul A, Langlotz C, Shpanskaya K, et al. Chexnet: radiologist-level pneumonia detection on chest X-rays with deep learning; 2017. arXiv preprint
arXiv:1711.05225
.
Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 2097–106.
Zuo W, Zhou F, Li Z, Wang L. Multi-resolution CNN and knowledge transfer for candidate classification in lung nodule detection. IEEE Access. 2019;7:32510–21.
Article
Google Scholar
Shen W, Zhou M, Yang F, Yang C, Tian J. Multi-scale convolutional neural networks for lung nodule classification. In: International conference on information processing in medical imaging. Springer; 2015. p. 588–99.
Li R, Zhang W, Suk HI, Wang L, Li J, Shen D, Ji S. Deep learning based imaging data completion for improved brain disease diagnosis. In: International conference on medical image computing and computer-assisted intervention. Springer; 2014. p. 305–12.
Wen J, Thibeau-Sutre E, Diaz-Melo M, Samper-González J, Routier A, Bottani S, Dormont D, Durrleman S, Burgos N, Colliot O, et al. Convolutional neural networks for classification of Alzheimer’s disease: overview and reproducible evaluation. Med Image Anal. 2020;63:101694.
Article
Google Scholar
Mehmood A, Maqsood M, Bashir M, Shuyuan Y. A deep siamese convolution neural network for multi-class classification of Alzheimer disease. Brain Sci. 2020;10(2):84.
Article
Google Scholar
Hosseini-Asl E, Ghazal M, Mahmoud A, Aslantas A, Shalaby A, Casanova M, Barnes G, Gimel’farb G, Keynton R, El-Baz A. Alzheimer’s disease diagnostics by a 3d deeply supervised adaptable convolutional network. Front Biosci. 2018;23:584–96.
Article
Google Scholar
Korolev S, Safiullin A, Belyaev M, Dodonova Y. Residual and plain convolutional neural networks for 3D brain MRI classification. In: 2017 IEEE 14th international symposium on biomedical imaging (ISBI 2017). IEEE; 2017. p. 835–8.
Alzubaidi L, Fadhel MA, Oleiwi SR, Al-Shamma O, Zhang J. DFU_QUTNet: diabetic foot ulcer classification using novel deep convolutional neural network. Multimed Tools Appl. 2020;79(21):15655–77.
Article
Google Scholar
Goyal M, Reeves ND, Davison AK, Rajbhandari S, Spragg J, Yap MH. Dfunet: convolutional neural networks for diabetic foot ulcer classification. IEEE Trans Emerg Topics Comput Intell. 2018;4(5):728–39.
Article
Google Scholar
Yap MH., Hachiuma R, Alavi A, Brungel R, Goyal M, Zhu H, Cassidy B, Ruckert J, Olshansky M, Huang X, et al. Deep learning in diabetic foot ulcers detection: a comprehensive evaluation; 2020. arXiv preprint
arXiv:2010.03341
.
Tulloch J, Zamani R, Akrami M. Machine learning in the prevention, diagnosis and management of diabetic foot ulcers: a systematic review. IEEE Access. 2020;8:198977–9000.
Article
Google Scholar
Fadhel MA, Al-Shamma O, Alzubaidi L, Oleiwi SR. Real-time sickle cell anemia diagnosis based hardware accelerator. In: International conference on new trends in information and communications technology applications, Springer; 2020. p. 189–99.
Debelee TG, Kebede SR, Schwenker F, Shewarega ZM. Deep learning in selected cancers’ image analysis—a survey. J Imaging. 2020;6(11):121.
Article
Google Scholar
Khan S, Islam N, Jan Z, Din IU, Rodrigues JJC. A novel deep learning based framework for the detection and classification of breast cancer using transfer learning. Pattern Recogn Lett. 2019;125:1–6.
Article
Google Scholar
Alzubaidi L, Hasan RI, Awad FH, Fadhel MA, Alshamma O, Zhang J. Multi-class breast cancer classification by a novel two-branch deep convolutional neural network architecture. In: 2019 12th international conference on developments in eSystems engineering (DeSE). IEEE; 2019. p. 268–73.
Roy K, Banik D, Bhattacharjee D, Nasipuri M. Patch-based system for classification of breast histology images using deep learning. Comput Med Imaging Gr. 2019;71:90–103.
Article
Google Scholar
Hameed Z, Zahia S, Garcia-Zapirain B, Javier Aguirre J, María Vanegas A. Breast cancer histopathology image classification using an ensemble of deep learning models. Sensors. 2020;20(16):4373.
Article
Google Scholar
Hosny KM, Kassem MA, Foaud MM. Skin cancer classification using deep learning and transfer learning. In: 2018 9th Cairo international biomedical engineering conference (CIBEC). IEEE; 2018. p. 90–3.
Dorj UO, Lee KK, Choi JY, Lee M. The skin cancer classification using deep convolutional neural network. Multimed Tools Appl. 2018;77(8):9909–24.
Article
Google Scholar
Kassem MA, Hosny KM, Fouad MM. Skin lesions classification into eight classes for ISIC 2019 using deep convolutional neural network and transfer learning. IEEE Access. 2020;8:114822–32.
Article
Google Scholar
Heidari M, Mirniaharikandehei S, Khuzani AZ, Danala G, Qiu Y, Zheng B. Improving the performance of CNN to predict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms. Int J Med Inform. 2020;144:104284.
Article
Google Scholar
Al-Timemy AH, Khushaba RN, Mosa ZM, Escudero J. An efficient mixture of deep and machine learning models for COVID-19 and tuberculosis detection using X-ray images in resource limited settings 2020. arXiv preprint
arXiv:2007.08223
.
Abraham B, Nair MS. Computer-aided detection of COVID-19 from X-ray images using multi-CNN and Bayesnet classifier. Biocybern Biomed Eng. 2020;40(4):1436–45.
Article
Google Scholar
Nour M, Cömert Z, Polat K. A novel medical diagnosis model for COVID-19 infection detection based on deep features and Bayesian optimization. Appl Soft Comput. 2020;97:106580.
Article
Google Scholar
Mallio CA, Napolitano A, Castiello G, Giordano FM, D’Alessio P, Iozzino M, Sun Y, Angeletti S, Russano M, Santini D, et al. Deep learning algorithm trained with COVID-19 pneumonia also identifies immune checkpoint inhibitor therapy-related pneumonitis. Cancers. 2021;13(4):652.
Article
Google Scholar
Fourcade A, Khonsari R. Deep learning in medical image analysis: a third eye for doctors. J Stomatol Oral Maxillofac Surg. 2019;120(4):279–88.
Article
Google Scholar
Guo Z, Li X, Huang H, Guo N, Li Q. Deep learning-based image segmentation on multimodal medical imaging. IEEE Trans Radiat Plasma Med Sci. 2019;3(2):162–9.
Article
Google Scholar
Thakur N, Yoon H, Chong Y. Current trends of artificial intelligence for colorectal cancer pathology image analysis: a systematic review. Cancers. 2020;12(7):1884.
Article
Google Scholar
Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing on MRI. Zeitschrift für Medizinische Physik. 2019;29(2):102–27.
Article
Google Scholar
Yadav SS, Jadhav SM. Deep convolutional neural network based medical image classification for disease diagnosis. J Big Data. 2019;6(1):113.
Article
Google Scholar
Nehme E, Freedman D, Gordon R, Ferdman B, Weiss LE, Alalouf O, Naor T, Orange R, Michaeli T, Shechtman Y. DeepSTORM3D: dense 3D localization microscopy and PSF design by deep learning. Nat Methods. 2020;17(7):734–40.
Article
Google Scholar
Zulkifley MA, Abdani SR, Zulkifley NH. Pterygium-Net: a deep learning approach to pterygium detection and localization. Multimed Tools Appl. 2019;78(24):34563–84.
Article
Google Scholar
Sirazitdinov I, Kholiavchenko M, Mustafaev T, Yixuan Y, Kuleev R, Ibragimov B. Deep neural network ensemble for pneumonia localization from a large-scale chest X-ray database. Comput Electr Eng. 2019;78:388–99.
Article
Google Scholar
Zhao W, Shen L, Han B, Yang Y, Cheng K, Toesca DA, Koong AC, Chang DT, Xing L. Markerless pancreatic tumor target localization enabled by deep learning. Int J Radiat Oncol Biol Phys. 2019;105(2):432–9.
Article
Google Scholar
Roth HR, Lee CT, Shin HC, Seff A, Kim L, Yao J, Lu L, Summers RM. Anatomy-specific classification of medical images using deep convolutional nets. In: 2015 IEEE 12th international symposium on biomedical imaging (ISBI). IEEE; 2015. p. 101–4.
Shin HC, Orton MR, Collins DJ, Doran SJ, Leach MO. Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data. IEEE Trans Pattern Anal Mach Intell. 2012;35(8):1930–43.
Article
Google Scholar
Li Z, Dong M, Wen S, Hu X, Zhou P, Zeng Z. CLU-CNNs: object detection for medical images. Neurocomputing. 2019;350:53–9.
Article
Google Scholar
Gao J, Jiang Q, Zhou B, Chen D. Convolutional neural networks for computer-aided detection or diagnosis in medical image analysis: an overview. Math Biosci Eng. 2019;16(6):6536.
Article
MathSciNet
Google Scholar
Lumini A, Nanni L. Review fair comparison of skin detection approaches on publicly available datasets. Expert Syst Appl. 2020.
https://doi.org/10.1016/j.eswa.2020.113677
.
Article
Google Scholar
Chouhan V, Singh SK, Khamparia A, Gupta D, Tiwari P, Moreira C, Damaševičius R, De Albuquerque VHC. A novel transfer learning based approach for pneumonia detection in chest X-ray images. Appl Sci. 2020;10(2):559.
Article
Google Scholar
Apostolopoulos ID, Mpesiana TA. COVID-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks. Phys Eng Sci Med. 2020;43(2):635–40.
Article
Google Scholar
Mahmud T, Rahman MA, Fattah SA. CovXNet: a multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization. Comput Biol Med. 2020;122:103869.
Article
Google Scholar
Tayarani-N MH. Applications of artificial intelligence in battling against COVID-19: a literature review. Chaos Solitons Fractals. 2020;142:110338.
Article
MathSciNet
Google Scholar
Toraman S, Alakus TB, Turkoglu I. Convolutional capsnet: a novel artificial neural network approach to detect COVID-19 disease from X-ray images using capsule networks. Chaos Solitons Fractals. 2020;140:110122.
Article
MathSciNet
Google Scholar
Dascalu A, David E. Skin cancer detection by deep learning and sound analysis algorithms: a prospective clinical study of an elementary dermoscope. EBioMedicine. 2019;43:107–13.
Article
Google Scholar
Adegun A, Viriri S. Deep learning techniques for skin lesion analysis and melanoma cancer detection: a survey of state-of-the-art. Artif Intell Rev. 2020;54:1–31.
Google Scholar
Zhang N, Cai YX, Wang YY, Tian YT, Wang XL, Badami B. Skin cancer diagnosis based on optimized convolutional neural network. Artif Intell Med. 2020;102:101756.
Article
Google Scholar
Thurnhofer-Hemsi K, Domínguez E. A convolutional neural network framework for accurate skin cancer detection. Neural Process Lett. 2020.
https://doi.org/10.1007/s11063-020-10364-y
.
Article
Google Scholar
Jain MS, Massoud TF. Predicting tumour mutational burden from histopathological images using multiscale deep learning. Nat Mach Intell. 2020;2(6):356–62.
Article
Google Scholar
Lei H, Liu S, Elazab A, Lei B. Attention-guided multi-branch convolutional neural network for mitosis detection from histopathological images. IEEE J Biomed Health Inform. 2020;25(2):358–70.
Article
Google Scholar
Celik Y, Talo M, Yildirim O, Karabatak M, Acharya UR. Automated invasive ductal carcinoma detection based using deep transfer learning with whole-slide images. Pattern Recogn Lett. 2020;133:232–9.
Article
Google Scholar
Sebai M, Wang X, Wang T. Maskmitosis: a deep learning framework for fully supervised, weakly supervised, and unsupervised mitosis detection in histopathology images. Med Biol Eng Comput. 2020;58:1603–23.
Article
Google Scholar
Sebai M, Wang T, Al-Fadhli SA. Partmitosis: a partially supervised deep learning framework for mitosis detection in breast cancer histopathology images. IEEE Access. 2020;8:45133–47.
Article
Google Scholar
Mahmood T, Arsalan M, Owais M, Lee MB, Park KR. Artificial intelligence-based mitosis detection in breast cancer histopathology images using faster R-CNN and deep CNNs. J Clin Med. 2020;9(3):749.
Article
Google Scholar
Srinidhi CL, Ciga O, Martel AL. Deep neural network models for computational histopathology: a survey. Med Image Anal. 2020;67:101813.
Article
Google Scholar
Cireşan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. In: International conference on medical image computing and computer-assisted intervention. Springer; 2013. p. 411–8.
Sirinukunwattana K, Raza SEA, Tsang YW, Snead DR, Cree IA, Rajpoot NM. Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging. 2016;35(5):1196–206.
Article
Google Scholar
Xu J, Xiang L, Liu Q, Gilmore H, Wu J, Tang J, Madabhushi A. Stacked sparse autoencoder (SSAE) for nuclei detection on breast cancer histopathology images. IEEE Trans Med Imaging. 2015;35(1):119–30.
Article
Google Scholar
Albarqouni S, Baur C, Achilles F, Belagiannis V, Demirci S, Navab N. Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images. IEEE Trans Med Imaging. 2016;35(5):1313–21.
Article
Google Scholar
Abd-Ellah MK, Awad AI, Khalaf AA, Hamed HF. Two-phase multi-model automatic brain tumour diagnosis system from magnetic resonance images using convolutional neural networks. EURASIP J Image Video Process. 2018;2018(1):97.
Article
Google Scholar
Thaha MM, Kumar KPM, Murugan B, Dhanasekeran S, Vijayakarthick P, Selvi AS. Brain tumor segmentation using convolutional neural networks in MRI images. J Med Syst. 2019;43(9):294.
Article
Google Scholar
Talo M, Yildirim O, Baloglu UB, Aydin G, Acharya UR. Convolutional neural networks for multi-class brain disease detection using MRI images. Comput Med Imaging Gr. 2019;78:101673.
Article
Google Scholar
Gabr RE, Coronado I, Robinson M, Sujit SJ, Datta S, Sun X, Allen WJ, Lublin FD, Wolinsky JS, Narayana PA. Brain and lesion segmentation in multiple sclerosis using fully convolutional neural networks: a large-scale study. Mult Scler J. 2020;26(10):1217–26.
Article
Google Scholar
Chen S, Ding C, Liu M. Dual-force convolutional neural networks for accurate brain tumor segmentation. Pattern Recogn. 2019;88:90–100.
Article
Google Scholar
Hu K, Gan Q, Zhang Y, Deng S, Xiao F, Huang W, Cao C, Gao X. Brain tumor segmentation using multi-cascaded convolutional neural networks and conditional random field. IEEE Access. 2019;7:92615–29.
Article
Google Scholar
Wadhwa A, Bhardwaj A, Verma VS. A review on brain tumor segmentation of MRI images. Magn Reson Imaging. 2019;61:247–59.
Article
Google Scholar
Akkus Z, Galimzianova A, Hoogi A, Rubin DL, Erickson BJ. Deep learning for brain MRI segmentation: state of the art and future directions. J Digit Imaging. 2017;30(4):449–59.
Article
Google Scholar
Moeskops P, Viergever MA, Mendrik AM, De Vries LS, Benders MJ, Išgum I. Automatic segmentation of MR brain images with a convolutional neural network. IEEE Trans Med Imaging. 2016;35(5):1252–61.
Article
Google Scholar
Milletari F, Navab N, Ahmadi SA. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 2016 fourth international conference on 3D vision (3DV). IEEE; 2016. p. 565–71.
Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: International conference on medical image computing and computer-assisted intervention. Springer; 2015. p. 234–41.
Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. IEEE Trans Med Imaging. 2016;35(5):1240–51.
Article
Google Scholar
Havaei M, Davy A, Warde-Farley D, Biard A, Courville A, Bengio Y, Pal C, Jodoin PM, Larochelle H. Brain tumor segmentation with deep neural networks. Med Image Anal. 2017;35:18–31.
Article
Google Scholar
Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL. DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Trans Pattern Anal Mach Intell. 2017;40(4):834–48.
Article
Google Scholar
Yan Q, Wang B, Gong D, Luo C, Zhao W, Shen J, Shi Q, Jin S, Zhang L, You Z. COVID-19 chest CT image segmentation—a deep convolutional neural network solution; 2020. arXiv preprint
arXiv:2004.10987
.
Wang G, Liu X, Li C, Xu Z, Ruan J, Zhu H, Meng T, Li K, Huang N, Zhang S. A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images. IEEE Trans Med Imaging. 2020;39(8):2653–63.
Article
Google Scholar
Khan SH, Sohail A, Khan A, Lee YS. Classification and region analysis of COVID-19 infection using lung CT images and deep convolutional neural networks; 2020. arXiv preprint
arXiv:2009.08864
.
Shi F, Wang J, Shi J, Wu Z, Wang Q, Tang Z, He K, Shi Y, Shen D. Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for COVID-19. IEEE Rev Biomed Eng. 2020;14:4–5.
Article
Google Scholar
Santamaría J, Rivero-Cejudo M, Martos-Fernández M, Roca F. An overview on the latest nature-inspired and metaheuristics-based image registration algorithms. Appl Sci. 2020;10(6):1928.
Article
Google Scholar
Santamaría J, Cordón O, Damas S. A comparative study of state-of-the-art evolutionary image registration methods for 3D modeling. Comput Vision Image Underst. 2011;115(9):1340–54.
Article
Google Scholar
Yumer ME, Mitra NJ. Learning semantic deformation flows with 3D convolutional networks. In: European conference on computer vision. Springer; 2016. p. 294–311.
Ding L, Feng C. Deepmapping: unsupervised map estimation from multiple point clouds. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2019. p. 8650–9.
Mahadevan S. Imagination machines: a new challenge for artificial intelligence. AAAI. 2018;2018:7988–93.
Google Scholar
Wang L, Fang Y. Unsupervised 3D reconstruction from a single image via adversarial learning; 2017. arXiv preprint
arXiv:1711.09312
.
Hermoza R, Sipiran I. 3D reconstruction of incomplete archaeological objects using a generative adversarial network. In: Proceedings of computer graphics international 2018. Association for Computing Machinery; 2018. p. 5–11.
Fu Y, Lei Y, Wang T, Curran WJ, Liu T, Yang X. Deep learning in medical image registration: a review. Phys Med Biol. 2020;65(20):20TR01.
Article
Google Scholar
Haskins G, Kruger U, Yan P. Deep learning in medical image registration: a survey. Mach Vision Appl. 2020;31(1):8.
Article
Google Scholar
de Vos BD, Berendsen FF, Viergever MA, Sokooti H, Staring M, Išgum I. A deep learning framework for unsupervised affine and deformable image registration. Med Image Anal. 2019;52:128–43.
Article
Google Scholar
Yang X, Kwitt R, Styner M, Niethammer M. Quicksilver: fast predictive image registration—a deep learning approach. NeuroImage. 2017;158:378–96.
Article
Google Scholar
Miao S, Wang ZJ, Liao R. A CNN regression approach for real-time 2D/3D registration. IEEE Trans Med Imaging. 2016;35(5):1352–63.
Article
Google Scholar
Li P, Pei Y, Guo Y, Ma G, Xu T, Zha H. Non-rigid 2D–3D registration using convolutional autoencoders. In: 2020 IEEE 17th international symposium on biomedical imaging (ISBI). IEEE; 2020. p. 700–4.
Zhang J, Yeung SH, Shu Y, He B, Wang W. Efficient memory management for GPU-based deep learning systems; 2019. arXiv preprint
arXiv:1903.06631
.
Zhao H, Han Z, Yang Z, Zhang Q, Yang F, Zhou L, Yang M, Lau FC, Wang Y, Xiong Y, et al. Hived: sharing a {GPU} cluster for deep learning with guarantees. In: 14th {USENIX} symposium on operating systems design and implementation ({OSDI} 20); 2020. p. 515–32.
Lin Y, Jiang Z, Gu J, Li W, Dhar S, Ren H, Khailany B, Pan DZ. DREAMPlace: deep learning toolkit-enabled GPU acceleration for modern VLSI placement. IEEE Trans Comput Aided Des Integr Circuits Syst. 2020;40:748–61.
Article
Google Scholar
Hossain S, Lee DJ. Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a flying robot with GPU-based embedded devices. Sensors. 2019;19(15):3371.
Article
Google Scholar
Castro FM, Guil N, Marín-Jiménez MJ, Pérez-Serrano J, Ujaldón M. Energy-based tuning of convolutional neural networks on multi-GPUs. Concurr Comput Pract Exp. 2019;31(21):4786.
Article
Google Scholar
Gschwend D. Zynqnet: an fpga-accelerated embedded convolutional neural network; 2020. arXiv preprint
arXiv:2005.06892
.
Zhang N, Wei X, Chen H, Liu W. FPGA implementation for CNN-based optical remote sensing object detection. Electronics. 2021;10(3):282.
Article
Google Scholar
Zhao M, Hu C, Wei F, Wang K, Wang C, Jiang Y. Real-time underwater image recognition with FPGA embedded system for convolutional neural network. Sensors. 2019;19(2):350.
Article
Google Scholar
Liu X, Yang J, Zou C, Chen Q, Yan X, Chen Y, Cai C. Collaborative edge computing with FPGA-based CNN accelerators for energy-efficient and time-aware face tracking system. IEEE Trans Comput Soc Syst. 2021.
https://doi.org/10.1109/TCSS.2021.3059318
.
Article
Google Scholar
Hossin M, Sulaiman M. A review on evaluation metrics for data classification evaluations. Int J Data Min Knowl Manag Process. 2015;5(2):1.
Article
Google Scholar
Provost F, Domingos P. Tree induction for probability-based ranking. Mach Learn. 2003;52(3):199–215.
Article
MATH
Google Scholar
Rakotomamonyj A. Optimizing area under roc with SVMS. In: Proceedings of the European conference on artificial intelligence workshop on ROC curve and artificial intelligence (ROCAI 2004), 2004. p. 71–80.
Mingote V, Miguel A, Ortega A, Lleida E. Optimization of the area under the roc curve using neural network supervectors for text-dependent speaker verification. Comput Speech Lang. 2020;63:101078.
Article
Google Scholar
Fawcett T. An introduction to roc analysis. Pattern Recogn Lett. 2006;27(8):861–74.
Article
MathSciNet
Google Scholar
Huang J, Ling CX. Using AUC and accuracy in evaluating learning algorithms. IEEE Trans Knowl Data Eng. 2005;17(3):299–310.
Article
Google Scholar
Hand DJ, Till RJ. A simple generalisation of the area under the ROC curve for multiple class classification problems. Mach Learn. 2001;45(2):171–86.
Article
MATH
Google Scholar
Masoudnia S, Mersa O, Araabi BN, Vahabie AH, Sadeghi MA, Ahmadabadi MN. Multi-representational learning for offline signature verification using multi-loss snapshot ensemble of CNNs. Expert Syst Appl. 2019;133:317–30.
Article
Google Scholar
Coupé P, Mansencal B, Clément M, Giraud R, de Senneville BD, Ta VT, Lepetit V, Manjon JV. Assemblynet: a large ensemble of CNNs for 3D whole brain MRI segmentation. NeuroImage. 2020;219:117026.
Article
Google Scholar
Download references
Acknowledgements
We would like to thank the professors from the Queensland University of Technology and the University of Information Technology and Communications who gave their feedback on the paper.
Funding
This research received no external funding.
Author information
Authors and Affiliations
School of Computer Science, Queensland University of Technology, Brisbane, QLD, 4000, Australia
Laith Alzubaidi & Jinglan Zhang
Control and Systems Engineering Department, University of Technology, Baghdad, 10001, Iraq
Amjad J. Humaidi
Electrical Engineering Technical College, Middle Technical University, Baghdad, 10001, Iraq
Ayad Al-Dujaili
Faculty of Electrical Engineering & Computer Science, University of Missouri, Columbia, MO, 65211, USA
Ye Duan & Muthana Al-Amidie
AlNidhal Campus, University of Information Technology & Communications, Baghdad, 10001, Iraq
Laith Alzubaidi & Omran Al-Shamma
Department of Computer Science, University of Jaén, 23071, Jaén, Spain
J. Santamaría
College of Computer Science and Information Technology, University of Sumer, Thi Qar, 64005, Iraq
Mohammed A. Fadhel
School of Engineering, Manchester Metropolitan University, Manchester, M1 5GD, UK
Laith Farhan
Authors
Laith Alzubaidi
View author publications
You can also search for this author in
PubMed
Google Scholar
Jinglan Zhang
View author publications
You can also search for this author in
PubMed
Google Scholar
Amjad J. Humaidi
View author publications
You can also search for this author in
PubMed
Google Scholar
Ayad Al-Dujaili
View author publications
You can also search for this author in
PubMed
Google Scholar
Ye Duan
View author publications
You can also search for this author in
PubMed
Google Scholar
Omran Al-Shamma
View author publications
You can also search for this author in
PubMed
Google Scholar
J. Santamaría
View author publications
You can also search for this author in
PubMed
Google Scholar
Mohammed A. Fadhel
View author publications
You can also search for this author in
PubMed
Google Scholar
Muthana Al-Amidie
View author publications
You can also search for this author in
PubMed
Google Scholar
Laith Farhan
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
Conceptualization: LA, and JZ; methodology: LA, JZ, and JS; software: LA, and MAF; validation: LA, JZ, MA, and LF; formal analysis: LA, JZ, YD, and JS; investigation: LA, and JZ; resources: LA, JZ, and MAF; data curation: LA, and OA.; writing–original draft preparation: LA, and OA; writing—review and editing: LA, JZ, AJH, AA, YD, OA, JS, MAF, MA, and LF; visualization: LA, and MAF; supervision: JZ, and YD; project administration: JZ, YD, and JS; funding acquisition: LA, AJH, AA, and YD. All authors read and approved the final manuscript.
Corresponding author
Correspondence to
Laith Alzubaidi
.
Ethics declarations
Ethics approval and consent to participate
Not applicable.
Consent for publication
Not applicable.
Competing interests
The authors declare that they have no competing interests.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by/4.0/
.
Reprints and permissions
About this article
Cite this article
Alzubaidi, L., Zhang, J., Humaidi, A.J.
et al.
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions.
J Big Data
8
, 53 (2021). https://doi.org/10.1186/s40537-021-00444-8
Download citation
Received
:
21 January 2021
Accepted
:
22 March 2021
Published
:
31 March 2021
DOI
:
https://doi.org/10.1186/s40537-021-00444-8
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Deep learning
Machine learning
Convolution neural network (CNN)
Deep neural network architectures
Deep learning applications
Image classification
Transfer learning
Medical image analysis
Supervised learning
FPGA
GPU
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-014-0007-7):
Deep learning applications and challenges in big data analytics | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Deep learning applications and challenges in big data analytics
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
24 February 2015
Deep learning applications and challenges in big data analytics
Maryam M Najafabadi
1
,
Flavio Villanustre
2
,
Taghi M Khoshgoftaar
1
,
Naeem Seliya
1
,
Randall Wald
1
&
…
Edin Muharemagic
3
Show authors
Journal of Big Data
volume
2
, Article number:
1
(
2015
)
Cite this article
283k
Accesses
1506
Citations
66
Altmetric
Metrics
details
Abstract
Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.
Introduction
The general focus of machine learning is the representation of the input data and generalization of the learnt patterns for use on future unseen data. The goodness of the data representation has a large impact on the performance of machine learners on the data: a poor data representation is likely to reduce the performance of even an advanced, complex machine learner, while a good data representation can lead to high performance for a relatively simpler machine learner. Thus, feature engineering, which focuses on constructing features and data representations from raw data [
1
], is an important element of machine learning. Feature engineering consumes a large portion of the effort in a machine learning task, and is typically quite domain specific and involves considerable human input. For example, the Histogram of Oriented Gradients (HOG) [
2
] and Scale Invariant Feature Transform (SIFT) [
3
] are popular feature engineering algorithms developed specifically for the computer vision domain. Performing feature engineering in a more automated and general fashion would be a major breakthrough in machine learning as this would allow practitioners to automatically extract such features without direct human input.
Deep Learning algorithms are one promising avenue of research into the automated extraction of complex data representations (features) at high levels of abstraction. Such algorithms develop a layered, hierarchical architecture of learning and representing data, where higher-level (more abstract) features are defined in terms of lower-level (less abstract) features. The hierarchical learning architecture of Deep Learning algorithms is motivated by artificial intelligence emulating the deep, layered learning process of the primary sensorial areas of the neocortex in the human brain, which automatically extracts features and abstractions from the underlying data [
4
]-[
6
]. Deep Learning algorithms are quite beneficial when dealing with learning from large amounts of unsupervised data, and typically learn data representations in a greedy layer-wise fashion [
7
],[
8
]. Empirical studies have demonstrated that data representations obtained from stacking up non-linear feature extractors (as in Deep Learning) often yield better machine learning results, e.g., improved classification modeling [
9
], better quality of generated samples by generative probabilistic models [
10
], and the invariant property of data representations [
11
]. Deep Learning solutions have yielded outstanding results in different machine learning applications, including speech recognition [
12
]-[
16
], computer vision [
7
],[
8
],[
17
], and natural language processing [
18
]-[
20
]. A more detailed overview of Deep Learning is presented in Section “Deep learning in data mining and machine learning”.
Big Data represents the general realm of problems and techniques used for application domains that collect and maintain massive volumes of raw data for domain-specific data analysis. Modern data-intensive technologies as well as increased computational and data storage resources have contributed heavily to the development of Big Data science [
21
]. Technology based companies such as Google, Yahoo, Microsoft, and Amazon have collected and maintained data that is measured in exabyte proportions or larger. Moreover, social media organizations such as Facebook, YouTube, and Twitter have billions of users that constantly generate a very large quantity of data. Various organizations have invested in developing products using Big Data Analytics to addressing their monitoring, experimentation, data analysis, simulations, and other knowledge and business needs [
22
], making it a central topic in data science research.
Mining and extracting meaningful patterns from massive input data for decision-making, prediction, and other inferencing is at the core of Big Data Analytics. In addition to analyzing massive volumes of data, Big Data Analytics poses other unique challenges for machine learning and data analysis, including format variation of the raw data, fast-moving streaming data, trustworthiness of the data analysis, highly distributed input sources, noisy and poor quality data, high dimensionality, scalability of algorithms, imbalanced input data, unsupervised and un-categorized data, limited supervised/labeled data, etc. Adequate data storage, data indexing/tagging, and fast information retrieval are other key problems in Big Data Analytics. Consequently, innovative data analysis and data management solutions are warranted when working with Big Data. For example, in a recent work we examined the high-dimensionality of bioinformatics domain data and investigated feature selection techniques to address the problem [
23
]. A more detailed overview of Big Data Analytics is presented in Section “Big data analytics”.
The knowledge learnt from (and made available by) Deep Learning algorithms has been largely untapped in the context of Big Data Analytics. Certain Big Data domains, such as computer vision [
17
] and speech recognition [
13
], have seen the application of Deep Learning largely to improve classification modeling results. The ability of Deep Learning to extract high-level, complex abstractions and data representations from large volumes of data, especially unsupervised data, makes it attractive as a valuable tool for Big Data Analtyics. More specifically, Big Data problems such as semantic indexing, data tagging, fast information retrieval, and discriminative modeling can be better addressed with the aid of Deep Learning. More traditional machine learning and feature engineering algorithms are not efficient enough to extract the complex and non-linear patterns generally observed in Big Data. By extracting such features, Deep Learning enables the use of relatively simpler linear models for Big Data analysis tasks, such as classification and prediction, which is important when developing models to deal with the scale of Big Data. The novelty of this study is that it explores the application of Deep Learning algorithms for key problems in Big Data Analytics, motivating further targeted research by experts in these twofields.
The paper focuses on two key topics: (1) how Deep Learning can assist with specific problems in Big Data Analytics, and (2) how specific areas of Deep Learning can be improved to reflect certain challenges associated with Big Data Analytics. With respect to the first topic, we explore the application of Deep Learning for specific Big Data Analytics, including learning from massive volumes of data, semantic indexing, discriminative tasks, and data tagging. Our investigation regarding the second topic focuses on specific challenges Deep Learning faces due to existing problems in Big Data Analytics, including learning from streaming data, dealing with high dimensionality of data, scalability of models, and distributed and parallel computing. We conclude by identifying important future areas needing innovation in Deep Learning for Big Data Analytics, including data sampling for generating useful high-level abstractions, domain (data distribution) adaption, defining criteria for extracting good data representations for discriminative and indexing tasks, semi-supervised learning, and active learning.
The remainder of the paper is structured as follows: Section “Deep learning in data mining and machine learning” presents an overview of Deep Learning for data analysis in data mining and machine learning; Section “Big data analytics” presents an overview of Big Data Analytics, including key characteristics of Big Data and identifying specific data analysis problems faced in Big Data Analytics; Section “Applications of deep learning in big data analytics” presents a targeted survey of works investigating Deep Learning based solutions for data analysis, and discusses how Deep Learning can be applied for Big Data Analytics problems; Section “Deep learning challenges in big data analytics” discusses some challenges faced by Deep Learning experts due to specific data analysis needs of Big Data; Section “Future work on deep learning in big data analytics” presents our insights into further works that are necessary for extending the application of Deep Learning in Big Data, and poses important questions to domain experts; and in Section “Conclusion” we reiterate the focus of the paper and summarize the workpresented.
Deep learning in data mining and machine learning
The main concept in deep leaning algorithms is automating the extraction of representations (abstractions) from the data [
5
],[
24
],[
25
]. Deep learning algorithms use a huge amount of unsupervised data to automatically extract complex representation. These algorithms are largely motivated by the field of artificial intelligence, which has the general goal of emulating the human brain’s ability to observe, analyze, learn, and make decisions, especially for extremely complex problems. Work pertaining to these complex challenges has been a key motivation behind Deep Learning algorithms which strive to emulate the hierarchical learning approach of the human brain. Models based on shallow learning architectures such as decision trees, support vector machines, and case-based reasoning may fall short when attempting to extract useful information from complex structures and relationships in the input corpus. In contrast, Deep Learning architectures have the capability to generalize in non-local and global ways, generating learning patterns and relationships beyond immediate neighbors in the data [
4
]. Deep learning is in fact an important step toward artificial intelligence. It not only provides complex representations of data which are suitable for AI tasks but also makes the machines independent of human knowledge which is the ultimate goal of AI. It extracts representations directly from unsupervised data without human interference.
A key concept underlying Deep Learning methods is distributed representations of the data, in which a large number of possible configurations of the abstract features of the input data are feasible, allowing for a compact representation of each sample and leading to a richer generalization. The number of possible configurations is exponentially related to the number of extracted abstract features. Noting that the observed data was generated through interactions of several known/unknown factors, and thus when a data pattern is obtained through some configurations of learnt factors, additional (unseen) data patterns can likely be described through new configurations of the learnt factors and patterns[
5
],[
24
]. Compared to learning based on local generalizations, the number of patterns that can be obtained using a distributed representation scales quickly with the number of learnt factors.
Deep learning algorithms lead to abstract representations because more abstract representations are often constructed based on less abstract ones. An important advantage of more abstract representations is that they can be invariant to the local changes in the input data. Learning such invariant features is an ongoing major goal in pattern recognition (for example learning features that are invariant to the face orientation in a face recognition task). Beyond being invariant such representations can also disentangle the factors of variation in data. The real data used in AI-related tasks mostly arise from complicated interactions of many sources. For example an image is composed of different sources of variations such a light, object shapes, and object materials. The abstract representations provided by deep learning algorithms can separate the different sources of variations in data.
Deep learning algorithms are actually Deep architectures of consecutive layers. Each layer applies a nonlinear transformation on its input and provides a representation in its output. The objective is to learn a complicated and abstract representation of the data in a hierarchical manner by passing the data through multiple transformation layers. The sensory data (for example pixels in an image) is fed to the first layer. Consequently the output of each layer is provided as input to its next layer.
Stacking up the nonlinear transformation layers is the basic idea in deep learning algorithms. The more layers the data goes through in the deep architecture, the more complicated the nonlinear transformations which are constructed. These transformations represent the data, so Deep Learning can be considered as special case of representation learning algorithms which learn representations of the data in a Deep Architecture with multiple levels of representations. The achieved final representation is a highly non-linear function of the input data.
It is important to note that the transformations in the layers of deep architecture are non-linear transformations which try to extract underlying explanatory factors in the data. One cannot use a linear transformation like PCA as the transformation algorithms in the layers of the deep structure because the compositions of linear transformations yield another linear transformation. Therefore, there would be no point in having a deep architecture. For example by providing some face images to the Deep Learning algorithm, at the first layer it can learn the edges in different orientations; in the second layer it composes these edges to learn more complex features like different parts of a face such as lips, noses and eyes. In the third layer it composes these features to learn even more complex feature like face shapes of different persons. These final representations can be used as feature in applications of face recognition. This example is provided to simply explain in an understandable way how a deep learning algorithm finds more abstract and complicated representations of data by composing representations acquired in a hierarchical architecture. However, it must be considered that deep learning algorithms do not necessarily attempt to construct a pre-defined sequence of representations at each layer (such as edges, eyes, faces), but instead more generally perform non-linear transformations in different layers. These transformations tend to disentangle factors of variations in data. Translating this concept to appropriate training criteria is still one of the main open questions in deep learning algorithms [
5
].
The final representation of data constructed by the deep learning algorithm (output of the final layer) provides useful information from the data which can be used as features in building classifiers, or even can be used for data indexing and other applications which are more efficient when using abstract representations of data rather than high dimensional sensory data.
Learning the parameters in a deep architecture is a difficult optimization task, such as learning the parameters in neural networks with many hidden layers. In 2006 Hinton proposed learning deep architectures in an unsupervised greedy layer-wise learning manner [
7
]. At the beginning the sensory data is fed as learning data to the first layer. The first layer is then trained based on this data, and the output of the first layer (the first level of learnt representations) is provided as learning data to the second layer. Such iteration is done until the desired number of layers is obtained. At this point the deep network is trained. The representations learnt on the last layer can be used for different tasks. If the task is a classification task usually another supervised layer is put on top of the last layer and its parameters are learnt (either randomly or by using supervised data and keeping the rest of the network fixed). At the end the whole network is fine-tuned by providing supervised data to it.
Here we explain two fundamental building blocks, unsupervised single layer learning algorithms which are used to construct deeper models: Autoencoders and Restricted Boltzmann Machines (RBMs). These are often employed in tandem to construct stacked Autoencoders [
8
],[
26
] and Deep belief networks [
7
], which are constructed by stacking up Autoencoders and Restricted Boltzmann Machines respectively. Autoencoders, also called autoassociators [
27
], are networks constructed of 3 layers: input, hidden and output. Autoencoders try to learn some representations of the input in the hidden layer in a way that makes it possible to reconstruct the input in the output layer based on these intermediate representations. Thus, the target output is the input itself. A basic Autoencoder learns its parameters by minimizing the reconstruction error. This minimization is usually done by stochastic gradient descent (much like what is done in Multilayer Perceptron). If the hidden layer is linear and the mean squared error is used as the reconstruction criteria, then the Autoencoder will learn the first k principle components of the data. Alternative strategies are proposed to make Autoencoders nonlinear which are appropriate to build deep networks as well as to extract meaningful representations of data rather than performing just as a dimensionality reduction method. Bengio et al. have called these methods “regularized Autoencoders” in [
5
], and we refer an interested reader to that paper for more details on algorithms.
Another unsupervised single layer learning algorithm which is used as a building block in constructing Deep Belief Networks is the Restricted Boltzmann machine (RBM). RBMs are most likely the most popular version of Boltzmann machine [
28
]. They contains one visible layer and one hidden layer. The restriction is that there is no interaction between the units of the same layer and the connections are solely between units from different layers. The Contrastive Divergence algorithm [
29
] has mostly been used to train the Boltzmann machine.
Big data analytics
Big Data generally refers to data that exceeds the typical storage, processing, and computing capacity of conventional databases and data analysis techniques. As a resource, Big Data requires tools and methods that can be applied to analyze and extract patterns from large-scale data. The rise of Big Data has been caused by increased data storage capabilities, increased computational processing power, and availability of increased volumes of data, which give organization more data than they have computing resources and technologies to process. In addition to the obvious great volumes of data, Big Data is also associated with other specific complexities, often referred to as the four Vs: Volume, Variety, Velocity, and Veracity [
22
],[
30
],[
31
]. We note that the aim of this section is not to extensively cover Big Data, but present a brief overview of its key concepts and challenges while keeping in mind that the use of Deep Learning in Big Data Analytics is the focus of this paper.
The unmanageable large Volume of data poses an immediate challenge to conventional computing environments and requires scalable storage and a distributed strategy to data querying and analysis. However, this large Volume of data is also a major positive feature of Big Data. Many companies, such as Facebook, Yahoo, Google, already have large amounts of data and have recently begun tapping into its benefits [
21
]. A general theme in Big Data systems is that the raw data is increasingly diverse and complex, consisting of largely un-categorized/unsupervised data along with perhaps a small quantity of categorized/supervised data. Working with the Variety among different data representations in a given repository poses unique challenges with Big Data, which requires Big Data preprocessing of unstructured data in order to extract structured/ordered representations of the data for human and/or downstream consumption. In today’s data-intensive technology era, data Velocity – the increasing rate at which data is collected and obtained – is just as important as the Volume and Variety characteristics of Big Data. While the possibility of data loss exists with streaming data if it is generally not immediately processed and analyzed, there is the option to save fast-moving data into bulk storage for batch processing at a later time. However, the practical importance of dealing with Velocity associated with Big Data is the quickness of the feedback loop, that is, process of translating data input into useable information. This is especially important in the case of time-sensitive information processing. Some companies such as Twitter, Yahoo, and IBM have developed products that address the analysis of streaming data [
22
]. Veracity in Big Data deals with the trustworthiness or usefulness of results obtained from data analysis, and brings to light the old adage “Garbage-In-Garbage-Out” for decision making based on Big Data Analytics. As the number of data sources and types increases, sustaining trust in Big Data Analytics presents a practical challenge.
Big Data Analytics faces a number of challenges beyond those implied by the four Vs. While not meant to be an exhaustive list, some key problem areas include: data quality and validation, data cleansing, feature engineering, high-dimensionality and data reduction, data representations and distributed data sources, data sampling, scalability of algorithms, data visualization, parallel and distributed data processing, real-time analysis and decision making, crowdsourcing and semantic input for improved data analysis, tracing and analyzing data provenance, data discovery and integration, parallel and distributed computing, exploratory data analysis and interpretation, integrating heterogenous data, and developing new models for massive data computation.
Applications of deep learning in big data analytics
As stated previously, Deep Learning algorithms extract meaningful abstract representations of the raw data through the use of an hierarchical multi-level learning approach, where in a higher-level more abstract and complex representations are learnt based on the less abstract concepts and representations in the lower level(s) of the learning hierarchy. While Deep Learning can be applied to learn from labeled data if it is available in sufficiently large amounts, it is primarily attractive for learning from large amounts of unlabeled/unsupervised data [
4
],[
5
],[
25
], making it attractive for extracting meaningful representations and patterns from Big Data.
Once the hierarchical data abstractions are learnt from unsupervised data with Deep Learning, more conventional discriminative models can be trained with the aid of relatively fewer supervised/labeled data points, where the labeled data is typically obtained through human/expert input. Deep Learning algorithms are shown to perform better at extracting non-local and global relationships and patterns in the data, compared to relatively shallow learning architectures [
4
]. Other useful characteristics of the learnt abstract representations by Deep Learning include: (1) relatively simple linear models can work effectively with the knowledge obtained from the more complex and more abstract data representations, (2) increased automation of data representation extraction from unsupervised data enables its broad application to different data types, such as image, textural, audio, etc., and (3) relational and semantic knowledge can be obtained at the higher levels of abstraction and representation of the raw data. While there are other useful aspects of Deep Learning based representations of data, the specific characteristics mentioned above are particularly important for Big Data Analytics.
Considering each of the four Vs of Big Data characteristics, i.e., Volume, Variety, Velocity, and Veracity, Deep Learning algorithms and architectures are more aptly suited to address issues related to Volume and Variety of Big Data Analytics. Deep Learning inherently exploits the availability of massive amounts of data, i.e. Volume in Big Data, where algorithms with shallow learning hierarchies fail to explore and understand the higher complexities of data patterns. Moreover, since Deep Learning deals with data abstraction and representations, it is quite likely suited for analyzing raw data presented in different formats and/or from different sources, i.e. Variety in Big Data, and may minimize need for input from human experts to extract features from every new data type observed in Big Data. While presenting different challenges for more conventional data analysis approaches, Big Data Analytics presents an important opportunity for developing novel algorithms and models to address specific issues related to Big Data. Deep Learning concepts provide one such solution venue for data analytics experts and practitioners. For example, the extracted representations by Deep Learning can be considered as a practical source of knowledge for decision-making, semantic indexing, information retrieval, and for other purposes in Big Data Analytics, and in addition, simple linear modeling techniques can be considered for Big Data Analytics when complex data is represented in higher forms of abstraction.
In the remainder of this section, we summarize some important works that have been performed in the field of Deep Learning algorithms and architectures, including semantic indexing, discriminative tasks, and data tagging. Our focus is that by presenting these works in Deep Learning, experts can observe the novel applicability of Deep Learning techniques in Big Data Analytics, particularly since some of the application domains in the works presented involve large scale data. Deep Learning algorithms are applicable to different kinds of input data; however, in this section we focus on its application on image, textual, and audio data.
Semantic indexing
A key task associated with Big Data Analytics is information retrieval [
21
]. Efficient storage and retrieval of information is a growing problem in Big Data, particularly since very large-scale quantities of data such as text, image, video, and audio are being collected and made available across various domains, e.g., social networks, security systems, shopping and marketing systems, defense systems, fraud detection, and cyber traffic monitoring. Previous strategies and solutions for information storage and retrieval are challenged by the massive volumes of data and different data representations, both associated with Big Data. In these systems, massive amounts of data are available that needs semantic indexing rather than being stored as data bit strings. Semantic indexing presents the data in a more efficient manner and makes it useful as a source for knowledge discovery and comprehension, for example by making search engines work more quickly and efficiently.
Instead of using raw input for data indexing, Deep Learning can be used to generate high-level abstract data representations which will be used for semantic indexing. These representations can reveal complex associations and factors (especially when the raw input was Big Data), leading to semantic knowledge and understanding. Data representations play an important role in the indexing of data, for example by allowing data points/instances with relatively similar representations to be stored closer to one another in memory, aiding in efficient information retrieval. It should be noted, however, that the high-level abstract data representations need to be meaningful and demonstrate relational and semantic association in order to actually confer a good semantic understanding and comprehension of the input.
While Deep Learning aids in providing a semantic and relational understanding of the data, a vector representation (corresponding to the extracted representations) of data instances would provide faster searching and information retrieval. More specifically, since the learnt complex data representations contain semantic and relational information instead of just raw bit data, they can directly be used for semantic indexing when each data point (for example a given text document) is presented by a vector representation, allowing for a vector-based comparison which is more efficient than comparing instances based directly on raw data. The data instances that have similar vector representations are likely to have similar semantic meaning. Thus, using vector representations of complex high-level data abstractions for indexing the data makes semantic indexing feasible. In the remainder of this section, we focus on document indexing based on knowledge gained from Deep Learning. However, the general idea of indexing based on data representations obtained from Deep Learning can be extended to other forms of data.
Document (or textual) representation is a key aspect in information retrieval for many domains. The goal of document representation is to create a representation that condenses specific and unique aspects of the document, e.g. document topic. Document retrieval and classification systems are largely based on word counts, representing the number of times each word occurs in the document. Various document retrieval schemas use such a strategy, e.g., TF-IDF [
32
] and BM25 [
33
]. Such document representation schemas consider individual words to be dimensions, with different dimensions being independent. In practice, it is often observed that the occurrence of words are highly correlated. Using Deep Learning techniques to extract meaningful data representations makes it possible to obtain semantic features from such high-dimensional textual data, which in turn also leads to the reduction of the dimensions of the document data representations.
Hinton et al. [
34
] describe a Deep Learning generative model to learn the binary codes for documents. The lowest layer of the Deep Learning network represents the word-count vector of the document which accounts as high-dimensional data, while the highest layer represents the learnt binary code of the document. Using 128-bit codes, the authors demonstrate that the binary codes of the documents that are semantically similar lay relatively closer in the Hamming space. The binary code of the documents can then be used for information retrieval. For each query document, its Hamming distance compared to all other documents in the data is computed and the top
D
similar documents are retrieved. Binary codes require relatively little storage space, and in addition they allow relatively quicker searches by using algorithms such as fast-bit counting to compute the Hamming distance between two binary codes. The authors conclude that using these binary codes for document retrieval is more accurate and faster than semantic-based analysis.
Deep Learning generative models can also be used to produce shorter binary codes by forcing the highest layer in the learning hierarchy to use a relatively small number of variables. These shorter binary codes can then simply be used as memory addresses. One word of memory is used to describe each document in such a way that a small Hamming-ball around that memory address contains semantically similar documents – such a technique is referred as “semantic hashing” [
35
]. Using such a strategy, one can perform information retrieval on a very large document set with the retrieval time being independent of the document set size. Techniques such as semantic hashing are quite attractive for information retrieval, because documents that are similar to the query document can be retrieved by finding all the memory addresses that differ from the memory address of the query document by a few bits. The authors demonstrate that “memory hashing” is much faster than locality-sensitive hashing, which is one of the fastest methods among existing algorithms. In addition, it is shown that by providing a document’s binary codes to algorithms such as TF-IDF instead of providing the entire document, a higher level of accuracy can be achieved. While Deep Learning generative models can have a relatively slow learning/training time for producing binary codes for document retrieval, the resulting knowledge yields fast inferences which is one major goal of Big Data Analytics. More specifically, producing the binary code for a new document requires just a few vector matrix computations performing a feed-forward pass through the encoder component of the Deep Learning network architecture.
To learn better representations and abstractions, one can use some supervised data in training the Deep Learning model. Ranzato et al. [
36
] present a study in which parameters of the Deep Learning model are learnt based on both supervised and unsupervised data. The advantages of such a strategy are that there is no need to completely label a large collection of data (as some unlabeled data is expected) and that the model has some prior knowledge (via the supervised data) to capture relevant class/label information in the data. In other words, the model is required to learn data representations that produce good reconstructions of the input in addition to providing good predictions of document class labels. The authors show that for learning compact representations, Deep Learning models are better than shallow learning models. The compact representations are efficient because they require fewer computations when used in indexing, and in addition, also need less storage capacity.
Google’s “word2vec” tool is another technique for automated extraction of semantic representations from Big Data. This tool takes a large-scale text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words, upon which the word vector file can be used as features in many Natural Language Processing (NLP) and machine learning applications. Miklov et al. [
37
] introduce techniques to learn high-quality word vectors from huge datasets with hundreds of millions of words (including some datasets containing 1.6 billion words), and with millions of distinct words in the vocabulary. They focus on artificial neural networks to learn the distributed representation of words. To train the network on such a massive dataset, the models are implemented on top of the large-scale distributed framework “DistBelief” [
38
]. The authors find that word vectors which are trained on massive amounts of data show subtle semantic relationships between words, such as a city and the country it belongs to – for example, Paris belongs to France and Berlin belongs to Germany. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval, and question response systems. For example, in a related work, Miklov et al. [
39
] demonstrate how word2vec can be applied for natural language translation.
Deep Learning algorithms make it possible to learn complex nonlinear representations between word occurrences, which allow the capture of high-level semantic aspects of the document (which could not normally be learned with linear models). Capturing these complex representations requires massive amounts of data for the input corpus, and producing labeled data from this massive input is a difficult task. With Deep Learning one can leverage unlabeled documents (unsupervised data) to have access to a much larger amount of input data, using a smaller amount of supervised data to improve the data representations and make them more related to the specific learning and inference tasks. The extracted data representations have been shown to be effective for retrieving documents, making them very useful for search engines.
Similar to textual data, Deep Learning can be used on other kinds of data to extract semantic representations from the input corpus, allowing for semantic indexing of that data. Given the relatively recent emergence of Deep Learning, additional work needs to be done on using its hierarchical learning strategy as a method for semantic indexing of Big Data. An remaining open question is what criteria is used to define “similar” when trying to extract data representations for indexing purposes (recall, data points that are semantically similar will have similar data representations in a specific distance space).
Discriminative tasks and semantic tagging
In performing discriminative tasks in Big Data Analytics one can use Deep Learning algorithms to extract complicated nonlinear features from the raw data, and then use simple linear models to perform discriminative tasks using the extracted features as input. This approach has two advantages: (1) extracting features with Deep Learning adds nonlinearity to the data analysis, associating the discriminative tasks closely to Artificial Intelligence, and (2) applying relatively simple linear analytical models on the extracted features is more computationally efficient, which is important for Big Data Analytics. The problem of developing efficient linear models for Big Data Analytics has been extensively investigated in the literature [
21
]. Hence, developing nonlinear features from massive amounts of input data allows the data analysts to benefit from the knowledge available through the massive amounts of data, by applying the learnt knowledge to simpler linear models for further analysis. This is an important benefit of using Deep Learning in Big Data Analytics, allowing practitioners to accomplish complicated tasks related to Artificial Intelligence, such as image comprehension, object recognition in images, etc., by using simpler models. Thus discriminative tasks are made relatively easier in Big Data Analytics with the aid of Deep Learning algorithms.
Discriminative analysis in Big Data Analytics can be the primary purpose of the data analysis, or it can be performed to conduct tagging (such as semantic tagging) on the data for the purpose of searching. For example, Li et al. [
40
] explore the Microsoft Research Audio Video Indexing System (MAVIS) that uses Deep Learning (with Artificial Neural Networks) based speech recognition technology to enable searching of audio and video files with speech. To converting digital audio and video signals into words, MAVIS automatically generates closed captions and keywords that can increase accessibility and discovery of audio and video files with speech content.
Considering the development of the Internet and the explosion of online users in recent years, there has been a very rapid increase in the size of digital image collections. These come from sources such as social networks, global positioning satellites, image sharing systems, medical imaging systems, military surveillance, and security systems. Google has explored and developed systems that provide image searches (e.g., the Google Images search service), including search systems that are only based on the image file name and document contents and do not consider/relate to the image content itself [
41
],[
42
]. Towards achieving artificial intelligence in providing improved image searches, practitioners should move beyond just the textual relationships of images, especially since textual representations of images are not always available in massive image collection repositories. Experts should strive towards collecting and organizing these massive image data collections, such that they can be browsed, searched, and retrieved more efficiently. To deal with large scale image data collections, one approach to consider is to automate the process of tagging images and extracting semantic information from the images. Deep Learning presents new frontiers towards constructing complicated representations for image and video data as relatively high levels of abstractions, which can then be used for image annotation and tagging that is useful for image indexing and retrieval. In the context of Big Data Analytics, here Deep Learning would aid in the discriminative task of semantic tagging of data.
Data tagging is another way to semantically index the input data corpus. However, it should not be confused with semantic indexing as discussed in the prior section. In semantic indexing, the focus is on using the Deep Learning abstract representations directly for data indexing purposes. Here the abstract data representations are considered as features for performing the discriminative task of data tagging. This tagging on data can also be used for data indexing as well, but the primary idea here is that Deep Leaning makes it possible to tag massive amounts of data by applying simple linear modeling methods on complicated features that were extracted by Deep Learning algorithms. The remainder of this section focuses largely on some results from using Deep Leaning for discriminative tasks that involve data tagging.
At the ImageNet Computer Vision Competition, Hinton et al. [
17
] demonstrated an approach using Deep Learning and Convolutional Neural Networks which outperformed other existing approaches for image object recognition. Using the ImageNet dataset, one of the largest for image object recognition, Hinton’s team showed the importance of Deep Learning for improving image searching. Dean et al. [
38
] demonstrated further success on ImageNet by using a similar Deep Learning modeling approach with a large-scale software infrastructure for training an artificial neural network.
Some other approaches have been tried for learning and extracting features from unlabeled image data, include Restricted Boltzmann Machines (RBMs) [
7
], autoencoders [
26
], and sparse coding [
43
]. However, these were only able to extract low-level features, such as edge and blob detection. Deep Learning can also be used to build very high-level features for image detection. For example, Google and Stanford formulated a very large deep neural network that was able to learn very high-level features, such as face detection or cat detection from scratch (without any priors) by just using unlabeled data [
44
]. Their work was a large scale investigation on the feasibility of building high-level features with Deep Learning using only unlabeled (unsupervised) data, and clearly demonstrated the benefits of using Deep Learning with unsupervised data. In Google’s experimentation, they trained a 9-layered locally connected sparse autoencoder on 10 million 200 ×200 images downloaded randomly from the Internet. The model had 1 billion connections and the training time lasted for 3 days. A computational cluster of 1000 machines and 16000 cores was used to train the network with model parallelism and asynchronous SGD (Stochastic Gradient Descent). In their experiments they obtained neurons that function like face detectors, cat detectors, and human body detectors, and based on these features their approach also outperformed the state-of-the-art and recognized 22,000 object categories from the ImageNet dataset. This demonstrates the generalization ability of abstract representations extracted by Deep Learning algorithms on new/unseen data, i.e., using features extracted from a given dataset to successfully perform a discriminative task on another dataset. While Google’s work involved the question of whether it is possible to build a face feature detector by just using unlabeled data, typically in computer vision labeled images are used to learn useful features [
45
]. For example, a large collection of face images with a bounding box around the faces can be used to learn a face detector feature. However, traditionally it would require a very large amount of labeled data to find the best features. The scarcity of labeled data in image data collections poses a challengingproblem.
There are other Deep Learning works that have explored image tagging. Socher et al. [
46
] introduce recursive neural networks for predicting a tree structure for images in multiple modalities, and is the first Deep Learning method that achieves very good results on segmentation and annotation of complex image scenes. The recursive neural network architecture is able to predict hierarchical tree structures for scene images, and outperforms other methods based on conditional random fields or a combination of other methods, as well as outperforming other existing methods in segmentation, annotation and scene classification. Socher et al. [
46
] also show that their algorithm is a natural tool for predicting tree structures by using it to parse natural language sentences. This demonstrates the advantage of Deep Learning as an effective approach for extracting data representations from different varieties of data types. Kumar et al. [
47
] suggest that recurrent neural networks can be used to construct a meaningful search space via Deep Learning, where the search space can then be used for a designed-based search.
Le et al. [
48
] demonstrate that Deep Learning can be used for action scene recognition as well as video data tagging, by using an independent variant analysis to learn invariant spatio-temporal features from video data. Their approach outperforms other existing methods when combined with Deep Learning techniques such as stacking and convolution to learn hierarchical representations. Previous works used to adapt hand designed feature for images like SIFT and HOG to the video domain. The Le et al. [
48
] study shows that extracting features directly from video data is a very important research direction, which can be also generalized to many domains.
Deep Learning has achieved remarkable results in extracting useful features (i.e., representations) for performing discriminative tasks on image and video data, as well as extracting representations from other kinds of data. These discriminative results with Deep Learning are useful for data tagging and information retrieval and can be used in search engines. Thus, the high-level complex data representations obtained by Deep Learning are useful for the application of computationally feasible and relatively simpler linear models for Big Data Analytics. However, there is considerable work that remains for further exploration, including determining appropriate objectives in learning good representations for performing discriminative tasks in Big DataAnalytics [
5
],[
25
].
Deep learning challenges in big data analytics
The prior section focused on emphasizing the applicability and benefits of Deep Learning algorithms for Big Data Analytics. However, certain characteristics associated with Big Data pose challenges for modifying and adapting Deep Learning to address those issues. This section presents some areas of Big Data where Deep Learning needs further exploration, specifically, learning with streaming data, dealing with high-dimensional data, scalability of models, and distributed computing.
Incremental learning for non-stationary data
One of the challenging aspects in Big Data Analytics is dealing with streaming and fast-moving input data. Such data analysis is useful in monitoring tasks, such as fraud detection. It is important to adapt Deep Learning to handle streaming data, as there is a need for algorithms that can deal with large amounts of continuous input data. In this section, we discuss some works associated with Deep Learning and streaming data, including incremental feature learning and extraction [
49
], denoising autoencoders [
50
], and deep belief networks [
51
].
Zhou et al. [
49
] describe how a Deep Learning algorithm can be used for incremental feature learning on very large datasets, employing denoising autoencoders [
50
]. Denoising autoencoders are a variant of autoencoders which extract features from corrupted input, where the extracted features are robust to noisy data and good for classification purposes. Deep Learning algorithms in general use hidden layers to contribute towards the extraction of features or data representations. In a denoising autoencoder, there is one hidden layer which extracts features, with the number of nodes in this hidden layer initially being the same as the number of features that would be extracted. Incrementally, the samples that do not conform to the given objective function (for example, their classification error is more than a threshold, or their reconstruction error is high) are collected and are used for adding new nodes to the hidden layer, with these new nodes being initialized based on those samples. Subsequently, incoming new data samples are used to jointly retrain all the features. This incremental feature learning and mapping can improve the discriminative or generative objective function; however, monotonically adding features can lead to having a lot of redundant features and overfitting of data. Consequently, similar features are merged to produce a more compact set of features. Zhou et al. [
49
] demonstrate that the incremental feature learning method quickly converges to the optimal number of features in a large-scale online setting. This kind of incremental feature extraction is useful in applications where the distribution of data changes with respect to time in massive online data streams. Incremental feature learning and extraction can be generalized for other Deep Learning algorithms, such as RBM [
7
], and makes it possible to adapt to new incoming stream of an online large-scale data. Moreover, it avoids expensive cross-validation analysis in selecting the number of features in large-scale datasets.
Calandra et al. [
51
] introduce adaptive deep belief networks which demonstrates how Deep Learning can be generalized to learn from online non-stationary and streaming data. Their study exploits the generative property of deep belief networks to mimic the samples from the original data, where these samples and the new observed samples are used to learn the new deep belief network which has adapted to the newly observed data. However, a downside of an adaptive deep belief network is the requirement for constant memory consumption.
The targeted works presented in this section provide empirical support to further explore and develop novel Deep Learning algorithms and architectures for analyzing large-scale, fast moving streaming data, as is encountered in some Big Data application domains such as social media feeds, marketing and financial data feeds, web click stream data, operational logs, and metering data. For example, Amazon Kinesis is a managed service designed to handle real-time streaming of Big Data – though it is not based on the Deep Learning approach.
High-dimensional data
Some Deep Learning algorithms can become prohibitively computationally-expensive when dealing with high-dimensional data, such as images, likely due to the often slow learning process associated with a deep layered hierarchy of learning data abstractions and representations from a lower-level layer to a higher-level layer. That is to say, these Deep Learning algorithms can be stymied when working with Big Data that exhibits large Volume, one of the four Vs associated with Big Data Analytics. A high-dimensional data source contributes heavily to the volume of the raw data, in addition to complicating learning from the data.
Chen et al. [
52
] introduce marginalized stacked denoising autoencoders (mSDAs) which scale effectively for high-dimensional data and is computationally faster than regular stacked denoising autoencoders (SDAs). Their approach marginalizes noise in SDA training and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters. The marginalized denoising autoencoder layers to have hidden nodes, thus allowing a closed-form solution with substantial speed-ups. Moreover, marginalized SDA only has two free meta-parameters, controlling the amount of noise as well as the number of layers to be stacked, which greatly simplifies the model selection process. The fast training time, the capability to scale to large-scale and high-dimensional data, and implementation simplicity make mSDA a promising method with appeal to a large audience in data mining and machine learning.
Convolutional neural networks are another method which scales up effectively on high-dimensional data. Researchers have taken advantages of convolutional neural networks on ImageNet dataset with 256 ×256 RGB images to achieve state of the art results [
17
],[
26
]. In convolutional neural networks, the neurons in the hidden layers units do not need to be connected to all of the nodes in the previous layer, but just to the neurons that are in the same spatial area. Moreover, the resolution of the image data is also reduced when moving toward higher layers in the network.
The application of Deep Learning algorithms for Big Data Analytics involving high-dimensional data remains largely unexplored, and warrants development of Deep Learning based solutions that either adapt approaches similar to the ones presented above or develop novel solutions for addressing the high-dimensionality found in some Big Data domains.
Large-scale models
From a computation and analytics point of view, how do we scale the recent successes of Deep Learning to much larger-scale models and massive datasets? Empirical results have demonstrated the effectiveness of large-scale models [
53
]-[
55
], with particular focus on models with a very large number of model parameters which are able to extract more complicated features and representations [
38
],[
56
].
Dean et al. [
38
] consider the problem of training a Deep Learning neural network with billions of parameters using tens of thousands of CPU cores, in the context of speech recognition and computer vision. A software framework, DistBelief, is developed that can utilize computing clusters with thousands of machines to train large-scale models. The framework supports model parallelism both within a machine (via multithreading) and across machines (via message passing), with the details of parallelism, synchronization, and communication managed by DistBelief. In addition, the framework also supports data parallelism, where multiple replicas of a model are used to optimize a single objective. In order to make large-scale distributed training possible an asynchronous SGD as well as a distributed batch optimization procedure is developed that includes a distributed implementation of L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno, a quasi-Newton method for unconstrained optimization). The primary idea is to train multiple versions of the model in parallel, each running on a different node in the network and analyzing different subsets of data. The authors report that in addition to accelerating the training of conventional sized models, their framework can also train models that are larger than could be contemplated otherwise. Moreover, while the framework focuses on training large-scale neural networks, the underlying algorithms are applicable to other gradient-based learning techniques. It should be noted, however, that the extensive computational resources utilized by DistBelief are generally unavailable to a larger audience.
Coates et al. [
56
] leverage the relatively inexpensive computing power of a cluster of GPU servers. More specifically, they develop their own system (using neural networks) based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology and introduce a high-speed communication infrastructure to coordinate distributed computations. The system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and it can scale to networks with over 11 billion parameters using just 16 machines and where the scalability is comparable to that of DistBelief. In comparison to the computational resources used by DistBelief, the distributed system network based on COTS HPC is more generally available to a larger audience, making it a reasonable alternative for other Deep Learning experts exploring large-scale models.
Large-scale Deep Learning models are quite suited to handle massive volumes of input associated with Big Data, and as demonstrated in the above works they are also better at learning complex data patterns from large volumes of data. Determining the optimal number of model parameters in such large-scale models and improving their computational practicality pose challenges in Deep Learning for Big Data Analytics. In addition to the problem of handling massive volumes of data, large-scale Deep Learning models for Big Data Analytics also have to contend with other Big Data problems, such as domain adaptation (see next section) and streaming data. This lends to the need for further innovations in large-scale models for Deep Learning algorithms and architectures.
Future work on deep learning in big data analytics
In the prior sections, we discussed some recent applications of Deep Learning algorithms for Big Data Analytics, as well as identified some areas where Deep Learning research needs further exploration to address specific data analysis problems observed in Big Data. Considering the low-maturity of Deep Learning, we note that considerable work remains to done. In this section, we discuss our insights on some remaining questions in Deep Learning research, especially on work needed for improving machine learning and the formulation of the high-level abstractions and data representations for Big Data.
An important problem is whether to utilize the entire Big Data input corpus available when analyzing data with Deep Learning algorithms. The general focus is to apply Deep Learning algorithms to train the high-level data representation patterns based on a portion of the available input corpus, and then utilize the remaining input corpus with the learnt patterns for extracting the data abstractions and representations. In the context of this problem, a question to explore is what volume of input data is generally necessary to train useful (good) data representations by Deep Learning algorithms which can then be generalized for new data in the specific Big Data application domain.
Upon further exploring the above problem, we recall the Variety characteristic of Big Data Analytics, which focuses on the variation of the input data types and domains in Big Data. here, by considering the shift between the input data source (for training the representations) and the target data source (for generalizing the representations), the problem becomes one of domain adaptation for Deep Learning in Big Data Analytics. Domain adaptation during learning is an important focus of study in Deep Learning [
57
],[
58
], where the distribution of the training data (from which the representations are learnt) is different from the distribution of the test data (on which the learnt representations are deployed).
Glorot et al. [
57
] demonstrate that Deep Learning is able to discover intermediate data representations in a hierarchical learning manner, and that these representations are meaningful to, and can be shared among, different domains. In their work, a stacked denoising autoencoder is initially used to learn features and patterns from unlabeled data obtained from different source domains. Subsequently, a support vector machine (SVM) algorithm utilizes the learnt features and patterns for application on labeled data from a given source domain, resulting in a linear classification model that outperforms other methods. This domain adaptation study is successfully applied on a large industrial strength dataset consisting of 22 source domains. However, it should be noted that their study does not explicitly encode the distribution shift of the data between the source domain and the target domains. Chopra et al. [
58
] propose a Deep Learning model (based on neural networks) for domain adaptation which strives to learn a useful (for prediction purposes) representation of the unsupervised data by taking into consideration information available from the distribution shift between the training and test data. The focus is to hierarchically learn multiple intermediate representations along an interpolating path between the training and testing domains. In the context of object recognition, their study demonstrates an improvement over other methods. The two studies presented above raise the question about how to increase the generalization capacity of Deep Learning data representations and patterns, noting that the ability to generalize learnt patterns is an important requirement in Big Data Analytics where often there is a distribution shift between the input domain and the target domain.
Another key area of interest would be to explore the question of what criteria is necessary and should be defined for allowing the extracted data representations to provide useful semantic meaning to the Big Data. Earlier, we discussed some studies that utilize the data representations extracted through Deep Learning for semantic indexing. Bengio et al. [
5
] present some characteristics of what constitutes good data representations for performing discriminative tasks, and point to the open question regarding the definition of the criteria for learning good data representations in Deep Learning. Compared to more conventional learning algorithms where misclassification error is generally used as an important criterion for model training and learning patterns, defining a corresponding criteria for training Deep Learning algorithms with Big Data is unsuitable since most Big Data Analytics involve learning from largely unsupervised data. While availability of supervised data in some Big Data domains can be helpful, the question of defining the criteria for obtaining good data abstractions and representations still remains largely unexplored in Big Data Analytics. Moreover, the question of defining the criteria required for extracting good data representations leads to the question of what would constitute a good data representation that is effective for semantic indexing and/or data tagging.
In some Big Data domains, the input corpus consists of a mix of both labeled and unlabeled data, e.g., cyber security [
59
], fraud detection [
60
], and computer vision [
45
]. In such cases, Deep Learning algorithms can incorporate semi-supervised training methods towards the goal of defining criteria for good data representation learning. For example, following learning representations and patterns from the unlabeled/unsupervised data, the available labeled/supervised data can be exploited to further tune and improve the learnt representations and patterns for a specific analytics task, including semantic indexing or discriminative modeling. A variation of semi-supervised learning in data mining, active learning methods could also be applicable towards obtaining improved data representations where input from crowdsourcing or human experts can be used to obtain labels for some data samples which can then be used to better tune and improve the learnt data representations.
Conclusion
In contrast to more conventional machine learning and feature engineering algorithms, Deep Learning has an advantage of potentially providing a solution to address the data analysis and learning problems found in massive volumes of input data. More specifically, it aids in automatically extracting complex data representations from large volumes of unsupervised data. This makes it a valuable tool for Big Data Analytics, which involves data analysis from very large collections of raw data that is generally unsupervised and un-categorized. The hierarchical learning and extraction of different levels of complex, data abstractions in Deep Learning provides a certain degree of simplification for Big Data Analytics tasks, especially for analyzing massive volumes of data, semantic indexing, data tagging, information retrieval, and discriminative tasks such a classification and prediction.
In the context of discussing key works in the literature and providing our insights on those specific topics, this study focused on two important areas related to Deep Learning and Big Data: (1) the application of Deep Learning algorithms and architectures for Big Data Analytics, and (2) how certain characteristics and issues of Big Data Analytics pose unique challenges towards adapting Deep Learning algorithms for those problems. A targeted survey of important literature in Deep Learning research and application to different domains is presented in the paper as a means to identify how Deep Learning can be used for different purposes in Big Data Analytics.
The low-maturity of the Deep Learning field warrants extensive further research. In particular, more work is necessary on how we can adapt Deep Learning algorithms for problems associated with Big Data, including high dimensionality, streaming data analysis, scalability of Deep Learning models, improved formulation of data abstractions, distributed computing, semantic indexing, data tagging, information retrieval, criteria for extracting good data representations, and domain adaptation. Future works should focus on addressing one or more of these problems often seen in Big Data, thus contributing to the Deep Learning and Big Data Analytics research corpus.
References
Domingos P (2012) A few useful things to know about machine learning. Commun ACM 55(10)
Google Scholar
Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference On. IEEE Vol. 1. pp 886–893
Google Scholar
Lowe DG (1999) Object recognition from local scale-invariant features. In: Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference On. IEEE Computer Society Vol. 2. pp 1150–1157
Google Scholar
Bengio Y, LeCun Y:
Scaling learning algorithms towards, AI.
In
Large Scale Kernel Machines
Edited by: Bottou L, Chapelle O, DeCoste D, Weston J. MIT Press, Cambridge, MA; 2007, 321–360. [
http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf
] http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf
Google Scholar
Bengio Y, Courville A, Vincent P:
Representation learning: A review and new perspectives.
Pattern Analysis and Machine Intelligence, IEEE Transactions on
2013,
35
(8):1798–1828. doi:10.1109/TPAMI.2013.50 doi:10.1109/TPAMI.2013.50 10.1109/TPAMI.2013.50
Article
Google Scholar
Arel I, Rose DC, Karnowski TP:
Deep machine learning-a new frontier in artificial intelligence research [research frontier].
IEEE Comput Intell
2010,
5:
13–18. 10.1109/MCI.2010.938364
Article
Google Scholar
Hinton GE, Osindero S, Teh Y-W:
A fast learning algorithm for deep belief nets.
Neural Comput
2006,
18
(7):1527–1554. 10.1162/neco.2006.18.7.1527
Article
MATH
MathSciNet
Google Scholar
Bengio Y, Lamblin P, Popovici D, Larochelle H2007. Greedy layer-wise training of deep networks, Vol. 19. Bengio Y, Lamblin P, Popovici D, Larochelle H2007. Greedy layer-wise training of deep networks, Vol. 19.
Larochelle H, Bengio Y, Louradour J, Lamblin P:
Exploring strategies for training deep neural networks.
J Mach Learn Res
2009,
10:
1–40.
MATH
Google Scholar
Salakhutdinov R, Hinton GE (2009) Deep boltzmann machines. In: International Conference on, Artificial Intelligence and Statistics. JMLR.org. pp 448–455
Google Scholar
Goodfellow I, Lee H, Le QV, Saxe A, Ng AY (2009) Measuring invariances in deep networks. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. pp 646–654
Google Scholar
Dahl G, Ranzato M, Mohamed A-R, Hinton GE (2010) Phone recognition with the mean-covariance restricted boltzmann machine. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. pp 469–477
Google Scholar
Hinton G, Deng L, Yu D, Mohamed A-R, Jaitly N, Senior A, Vanhoucke V, Nguyen P, Sainath T, Dahl G, Kingsbury B:
Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.
Signal Process Mag IEEE
2012,
29
(6):82–97. 10.1109/MSP.2012.2205597
Article
Google Scholar
Seide F, Li G, Yu D (2011) Conversational speech transcription using context-dependent deep neural networks. In: INTERSPEECH. ISCA. pp 437–440
Google Scholar
Mohamed A-R, Dahl GE, Hinton G:
Acoustic modeling using deep belief networks.
Audio Speech Lang Process IEEE Trans
2012,
20
(1):14–22. 10.1109/TASL.2011.2109382
Article
Google Scholar
Dahl GE, Yu D, Deng L, Acero A:
Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.
Audio Speech Lang Process IEEE Trans
2012,
20
(1):30–42. 10.1109/TASL.2011.2134090
Article
Google Scholar
Krizhevsky A, Sutskever I, Hinton G (2012) Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. Vol. 25. pp 1106–1114
Google Scholar
Mikolov T, Deoras A, Kombrink S, Burget L, Cernock`y J (2011) Empirical evaluation and combination of advanced language modeling techniques. In: INTERSPEECH. ISCA. pp 605–608
Google Scholar
Socher R, Huang EH, Pennin J, Manning CD, Ng A (2011) Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. pp 801–809
Google Scholar
Bordes A, Glorot X, Weston J, Bengio Y (2012) Joint learning of words and meaning representations for open-text semantic parsing. In: International Conference on Artificial Intelligence and Statistics. JMLR.org. pp 127–135
Google Scholar
National Research Council:
Frontiers in Massive Data Analysis
. The National Academies Press, Washington, DC; 2013.
Google Scholar
Dumbill E:
What Is Big Data? An Introduction to the Big Data Landscape.
In
Strata 2012: Making Data Work
. O’Reilly, Santa Clara, CA O’Reilly; 2012.
Google Scholar
Khoshgoftaar TM (2013) Overcoming big data challenges. In: Proceedings of the 25th International Conference on Software Engineering and Knowledge Engineering, Boston, MA. ICSE. Invited Keynote Speaker
Google Scholar
Bengio Y:
Learning Deep Architectures for AI
. Now Publishers Inc., Hanover, MA, USA; 2009.
Google Scholar
Bengio Y:
Deep learning of representations: Looking forward.
In
Proceedings of the 1st International Conference on Statistical Language and Speech Processing. SLSP’13
. Springer, Tarragona, Spain; 2013:1–37. http://dx.doi.org/10.1007/978–3-642–39593–2_1 http://dx.doi.org/10.1007/978-3-642-39593-2_1 10.1007/978-3-642-39593-2_1
Chapter
Google Scholar
Hinton GE, Salakhutdinov RR (Science) Reducing the dimensionality of data with neural networks313(5786): 504–507. Hinton GE, Salakhutdinov RR (Science) Reducing the dimensionality of data with neural networks313(5786): 504–507.
Hinton GE, Zemel RS:
Autoencoders, minimum description length, and helmholtz free energy.
Adv Neural Inform Process Syst
1994,
6:
3–10.
Google Scholar
Smolensky P (1986) Information processing in dynamical systems: foundations of harmony theory. In: Parallel Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press. Vol. 1. pp 194–281
Google Scholar
Hinton GE:
Training products of experts by minimizing contrastive divergence.
Neural Comput
2002,
14
(8):1771–1800. 10.1162/089976602760128018
Article
MATH
MathSciNet
Google Scholar
Garshol LM (2013) Introduction to Big Data/Machine Learning. Online Slide Show, . http://www.slideshare.net/larsga/introduction-to-big-datamachine-learning., [
http://www.slideshare.net/larsga/introduction-to-big-datamachine-learning
]
Google Scholar
Grobelnik M (2013) Big Data Tutorial. European Data Forum. http://www.slideshare.net/EUDataForum/edf2013-big-datatutorialmarkogrobelnik?related=1., Grobelnik M (2013) Big Data Tutorial. European Data Forum. .
http://www.slideshare.net/EUDataForum/edf2013-big-datatutorialmarkogrobelnik?related=1
Grobelnik M (2013) Big Data Tutorial. European Data Forum. .
Google Scholar
Salton G, Buckley C:
Term-weighting approaches in automatic text retrieval.
Inform Process Manag
1988,
24
(5):513–523. 10.1016/0306-4573(88)90021-0
Article
Google Scholar
Robertson SE, Walker S (1994) Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In: Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. pp 232–241. Springer-Verlag New York, Inc
Google Scholar
Hinton G, Salakhutdinov R:
Discovering binary codes for documents by learning deep generative models.
Topics Cogn Sci
2011,
3
(1):74–91. 10.1111/j.1756-8765.2010.01109.x
Article
Google Scholar
Salakhutdinov R, Hinton G:
Semantic hashing.
Int J Approximate, Reasoning
2009,
50
(7):969–978. 10.1016/j.ijar.2008.11.006
Article
Google Scholar
Ranzato M, Szummer M (2008) Semi-supervised learning of compact document representations with deep=networks. In: Proceedings of the 25th International Conference on Machine Learning. ACM. pp 792–799
Google Scholar
Mikolov T, Chen K, Dean J (2013) Efficient estimation of word representations in vector space. CoRR: Computing Research Repository: 1–12. abs/1301.3781
Google Scholar
Dean J, Corrado G, Monga R, Chen K, Devin M, Le Q, Mao M, Ranzato M, Senior A, Tucker P, Yang K, Ng A (2012) Large scale distributed deep networks. In: Bartlett P, Pereira FCN, Burges CJC, Bottou L, Weinberger KQ (eds)Advances in Neural Information Processing Systems, 1232–1240. http://books.nips.cc/papers/files/nips25/NIPS2012_0598.pdf., Dean J, Corrado G, Monga R, Chen K, Devin M, Le Q, Mao M, Ranzato M, Senior A, Tucker P, Yang K, Ng A (2012) Large scale distributed deep networks. In: Bartlett P, Pereira FCN, Burges CJC, Bottou L, Weinberger KQ (eds)Advances in Neural Information Processing Systems, 1232–1240. .
http://books.nips.cc/papers/files/nips25/NIPS2012_0598.pdf
Google Scholar
Mikolov T, Le QV, Sutskever I (2013) Exploiting similarities among languages for machine translation. CoRR: Comput Res Repository: 1–10. abs/1309.4168
Google Scholar
Li G, Zhu H, Cheng G, Thambiratnam K, Chitsaz B, Yu D, Seide F (2012) Context-dependent deep neural networks for audio indexing of real-life data. In: Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE. pp 143–148
Google Scholar
Zipern A (2001) A Quick Way to Search For Images on the Web. The New York Times. News Watch Article. http://www.nytimes.com/2001/07/12/technology/news-watch-a-quick-way-to-search-for-images-on-the-web.html., Zipern A (2001) A Quick Way to Search For Images on the Web. The New York Times. News Watch Article. .
http://www.nytimes.com/2001/07/12/technology/news-watch-a-quick-way-to-search-for-images-on-the-web.html
Google Scholar
Cusumano MA:
Google: What it is and what it is not.
Commun ACM - Med Image Moeling
2005,
48
(2):15–17. doi:10.1145/1042091.1042107 doi:10.1145/1042091.1042107 10.1145/1042091.1042107
Article
Google Scholar
Lee H, Battle A, Raina R, Ng A (2006) Efficient sparse coding algorithms. In: Advances in Neural Information Processing Systems. MIT Press. pp 801–808
Google Scholar
Le Q, Ranzato M, Monga R, Devin M, Chen K, Corrado G, Dean J, Ng A (2012) Building high-level features using large scale unsupervised learning. In: Proceeding of the 29th International Conference in Machine Learning, Edingburgh, Scotland
Google Scholar
Freytag A, Rodner E, Bodesheim P, Denzler J:
Labeling Examples that Matter: Relevance-Based Active Learning with Gaussian Processes.
In
35th German Conference on Pattern Recognition (GCPR)
. Saarland University and Max-Planck-Institute for Informatics, Germany; 2013:282–291.
Google Scholar
Socher R, Lin CC, Ng A, Manning C (2011) Parsing natural scenes and natural language with recursive neural networks. In: Proceedings of the 28th International Conference on Machine Learning. Omnipress. pp 129–136
Google Scholar
Kumar R, Talton JO, Ahmad S, Klemmer SR (2012) Data-driven web design. In: Proceedings of the 29th International Conference on Machine Learning. icml.cc/Omnipress
Google Scholar
Le QV, Zou WY, Yeung SY, Ng AY (2011) Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In: Computer Vision and Pattern Recognition (CVPR) 2011 IEEE Conference On. IEEE. pp 3361–3368
Google Scholar
Zhou G, Sohn K, Lee H (2012) Online incremental feature learning with denoising autoencoders. In: International Conference on Artificial Intelligence and Statistics. JMLR.org. pp 1453–1461
Google Scholar
Vincent P, Larochelle H, Bengio Y, Manzagol P-A (2008) Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th International Conference on Machine Learning. ACM. pp 1096–1103
Google Scholar
Calandra R, Raiko T, Deisenroth MP, Pouzols FM:
Learning deep belief networks from non-stationary streams.
In
Artificial Neural Networks and Machine Learning–ICANN 2012
. Springer, Berlin Heidelberg; 2012:379–386. 10.1007/978-3-642-33266-1_47
Chapter
Google Scholar
Chen M, Xu ZE, Weinberger KQ, Sha F (2012) Marginalized denoising autoencoders for domain adaptation. In: Proceeding of the 29th International Conference in Machine Learning, Edingburgh, Scotland
Google Scholar
Coates A, Ng A (2011) The importance of encoding versus training with sparse coding and vector quantization. In: Proceedings of the 28th International Conference on Machine Learning. Omnipress. pp 921–928
Google Scholar
Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov R (2012) Improving neural networks by preventing co-adaptation of feature detectors. CoRR: Comput Res Repository: 1–18. abs/1207.0580
Google Scholar
Goodfellow IJ, Warde-Farley D, Mirza M, Courville A, Bengio Y (2013) Maxout networks. In: Proceeding of the 30th International Conference in Machine Learning, Atlanta, GA
Google Scholar
Coates A, Huval B, Wang T, Wu D, Catanzaro B, Andrew N (2013) Deep learning with cots hpc systems. In: Proceedings of the 30th International Conference on Machine Learning. pp 1337–1345
Google Scholar
Glorot X, Bordes A, Bengio Y (2011) Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp 513–520
Google Scholar
Chopra S, Balakrishnan S, Gopalan R (2013) Dlid: Deep learning for domain adaptation by interpolating betweendomains. In: Workshop on Challenges in Representation Learning, Proceedings of the 30th International Conference on Machine Learning, Atlanta, GA
Google Scholar
Suthaharan S:
Big data classification: Problems and challenges in network intrusion prediction with machine learning.
In
ACM Sigmetrics: Big Data Analytics Workshop
. ACM, Pittsburgh, PA; 2013.
Google Scholar
Wang W, Lu D, Zhou X, Zhang B, Mu J:
Statistical wavelet-based anomaly detection in big data with compressive sensing.
EURASIP J Wireless Commun Netw
2013,
2013:
269. http://www.bibsonomy.org/bibtex/25e432dc7230087ab1cdc65925be6d4cb/dblp http://www.bibsonomy.org/bibtex/25e432dc7230087ab1cdc65925be6d4cb/dblp 10.1186/1687-1499-2013-269
Article
Google Scholar
Download references
Author information
Authors and Affiliations
Florida Atlantic University, 777 Glades Road, Boca Raton, FL, USA
Maryam M Najafabadi, Taghi M Khoshgoftaar, Naeem Seliya & Randall Wald
LexisNexis Business Information Solutions, 245 Peachtree Center Avenue, Atlanta, GA, USA
Flavio Villanustre
LexisNexis Business Information Solutions, 6601 Park of Commerce Blvd, Boca Raton, FL, USA
Edin Muharemagic
Authors
Maryam M Najafabadi
View author publications
You can also search for this author in
PubMed
Google Scholar
Flavio Villanustre
View author publications
You can also search for this author in
PubMed
Google Scholar
Taghi M Khoshgoftaar
View author publications
You can also search for this author in
PubMed
Google Scholar
Naeem Seliya
View author publications
You can also search for this author in
PubMed
Google Scholar
Randall Wald
View author publications
You can also search for this author in
PubMed
Google Scholar
Edin Muharemagic
View author publications
You can also search for this author in
PubMed
Google Scholar
Corresponding author
Correspondence to
Randall Wald
.
Additional information
Competing interests
The authors declare that they have no competing interests.
Authors’ contributions
MMN performed the primary literature review and analysis for this work, and also drafted the manuscript. RW and NS worked with MMN to develop the article’s framework and focus. TMK, FV and EM introduced this topic to MMN and TMK coordinated with the other authors to complete and finalize this work. All authors read and approved the final manuscript.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.
The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.
To view a copy of this licence, visit
https://creativecommons.org/licenses/by/4.0/
.
Reprints and permissions
About this article
Cite this article
Najafabadi, M.M., Villanustre, F., Khoshgoftaar, T.M.
et al.
Deep learning applications and challenges in big data analytics.
Journal of Big Data
2
, 1 (2015). https://doi.org/10.1186/s40537-014-0007-7
Download citation
Received
:
26 March 2014
Accepted
:
14 August 2014
Published
:
24 February 2015
DOI
:
https://doi.org/10.1186/s40537-014-0007-7
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Deep learning
Big data
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00333-6):
Short-term stock market price trend prediction using a comprehensive deep learning system | Journal of Big Data | Full Text
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Submit manuscript
Short-term stock market price trend prediction using a comprehensive deep learning system
Download PDF
Download ePub
Download PDF
Download ePub
Research
Open access
Published:
28 August 2020
Short-term stock market price trend prediction using a comprehensive deep learning system
Jingyi Shen
1
&
M. Omair Shafiq
ORCID:
orcid.org/0000-0002-1859-8296
1
Journal of Big Data
volume
7
, Article number:
66
(
2020
)
Cite this article
279k
Accesses
188
Citations
90
Altmetric
Metrics
details
Abstract
In the era of big data, deep learning for predicting stock market prices and trends has become even more popular than before. We collected 2 years of data from Chinese stock market and proposed a comprehensive customization of feature engineering and deep learning-based model for predicting price trend of stock markets. The proposed solution is comprehensive as it includes pre-processing of the stock market dataset, utilization of multiple feature engineering techniques, combined with a customized deep learning based system for stock market price trend prediction. We conducted comprehensive evaluations on frequently used machine learning models and conclude that our proposed solution outperforms due to the comprehensive feature engineering that we built. The system achieves overall high accuracy for stock market trend prediction. With the detailed design and evaluation of prediction term lengths, feature engineering, and data pre-processing methods, this work contributes to the stock analysis research community both in the financial and technical domains.
Introduction
Stock market is one of the major fields that investors are dedicated to, thus stock market price trend prediction is always a hot topic for researchers from both financial and technical domains. In this research, our objective is to build a state-of-art prediction model for price trend prediction, which focuses on short-term price trend prediction.
As concluded by Fama in [
26
], financial time series prediction is known to be a notoriously difficult task due to the generally accepted, semi-strong form of market efficiency and the high level of noise. Back in 2003, Wang et al. in [
44
] already applied artificial neural networks on stock market price prediction and focused on volume, as a specific feature of stock market. One of the key findings by them was that the volume was not found to be effective in improving the forecasting performance on the datasets they used, which was S&P 500 and DJI. Ince and Trafalis in [
15
] targeted short-term forecasting and applied support vector machine (SVM) model on the stock price prediction. Their main contribution is performing a comparison between multi-layer perceptron (MLP) and SVM then found that most of the scenarios SVM outperformed MLP, while the result was also affected by different trading strategies. In the meantime, researchers from financial domains were applying conventional statistical methods and signal processing techniques on analyzing stock market data.
The optimization techniques, such as principal component analysis (PCA) were also applied in short-term stock price prediction [
22
]. During the years, researchers are not only focused on stock price-related analysis but also tried to analyze stock market transactions such as volume burst risks, which expands the stock market analysis research domain broader and indicates this research domain still has high potential [
39
]. As the artificial intelligence techniques evolved in recent years, many proposed solutions attempted to combine machine learning and deep learning techniques based on previous approaches, and then proposed new metrics that serve as training features such as Liu and Wang [
23
]. This type of previous works belongs to the feature engineering domain and can be considered as the inspiration of feature extension ideas in our research. Liu et al. in [
24
] proposed a convolutional neural network (CNN) as well as a long short-term memory (LSTM) neural network based model to analyze different quantitative strategies in stock markets. The CNN serves for the stock selection strategy, automatically extracts features based on quantitative data, then follows an LSTM to preserve the time-series features for improving profits.
The latest work also proposes a similar hybrid neural network architecture, integrating a convolutional neural network with a bidirectional long short-term memory to predict the stock market index [
4
]. While the researchers frequently proposed different neural network solution architectures, it brought further discussions about the topic if the high cost of training such models is worth the result or not.
There are three key contributions of our work (1) a new dataset extracted and cleansed (2) a comprehensive feature engineering, and (3) a customized long short-term memory (LSTM) based deep learning model.
We have built the dataset by ourselves from the data source as an open-sourced data API called Tushare [
43
]. The novelty of our proposed solution is that we proposed a feature engineering along with a fine-tuned system instead of just an LSTM model only. We observe from the previous works and find the gaps and proposed a solution architecture with a comprehensive feature engineering procedure before training the prediction model. With the success of feature extension method collaborating with recursive feature elimination algorithms, it opens doors for many other machine learning algorithms to achieve high accuracy scores for short-term price trend prediction. It proved the effectiveness of our proposed feature extension as feature engineering. We further introduced our customized LSTM model and further improved the prediction scores in all the evaluation metrics. The proposed solution outperformed the machine learning and deep learning-based models in similar previous works.
The remainder of this paper is organized as follows. “
Survey of related works
” section describes the survey of related works. “
The dataset
” section provides details on the data that we extracted from the public data sources and the dataset prepared. “
Methods
” section presents the research problems, methods, and design of the proposed solution. Detailed technical design with algorithms and how the model implemented are also included in this section. “
Results
” section presents comprehensive results and evaluation of our proposed model, and by comparing it with the models used in most of the related works. “
Discussion
” section provides a discussion and comparison of the results. “
Conclusion
” section presents the conclusion. This research paper has been built based on Shen [
36
].
Survey of related works
In this section, we discuss related works. We reviewed the related work in two different domains: technical and financial, respectively.
Kim and Han in [
19
] built a model as a combination of artificial neural networks (ANN) and genetic algorithms (GAs) with discretization of features for predicting stock price index. The data used in their study include the technical indicators as well as the direction of change in the daily Korea stock price index (KOSPI). They used the data containing samples of 2928 trading days, ranging from January 1989 to December 1998, and give their selected features and formulas. They also applied optimization of feature discretization, as a technique that is similar to dimensionality reduction. The strengths of their work are that they introduced GA to optimize the ANN. First, the amount of input features and processing elements in the hidden layer are 12 and not adjustable. Another limitation is in the learning process of ANN, and the authors only focused on two factors in optimization. While they still believed that GA has great potential for feature discretization optimization. Our initialized feature pool refers to the selected features. Qiu and Song in [
34
] also presented a solution to predict the direction of the Japanese stock market based on an optimized artificial neural network model. In this work, authors utilize genetic algorithms together with artificial neural network based models, and name it as a hybrid GA-ANN model.
Piramuthu in [
33
] conducted a thorough evaluation of different feature selection methods for data mining applications. He used for datasets, which were credit approval data, loan defaults data, web traffic data, tam, and kiang data, and compared how different feature selection methods optimized decision tree performance. The feature selection methods he compared included probabilistic distance measure: the Bhattacharyya measure, the Matusita measure, the divergence measure, the Mahalanobis distance measure, and the Patrick-Fisher measure. For inter-class distance measures: the Minkowski distance measure, city block distance measure, Euclidean distance measure, the Chebychev distance measure, and the nonlinear (Parzen and hyper-spherical kernel) distance measure. The strength of this paper is that the author evaluated both probabilistic distance-based and several inter-class feature selection methods. Besides, the author performed the evaluation based on different datasets, which reinforced the strength of this paper. However, the evaluation algorithm was a decision tree only. We cannot conclude if the feature selection methods will still perform the same on a larger dataset or a more complex model.
Hassan and Nath in [
9
] applied the Hidden Markov Model (HMM) on the stock market forecasting on stock prices of four different Airlines. They reduce states of the model into four states: the opening price, closing price, the highest price, and the lowest price. The strong point of this paper is that the approach does not need expert knowledge to build a prediction model. While this work is limited within the industry of Airlines and evaluated on a very small dataset, it may not lead to a prediction model with generality. One of the approaches in stock market prediction related works could be exploited to do the comparison work. The authors selected a maximum 2 years as the date range of training and testing dataset, which provided us a date range reference for our evaluation part.
Lei in [
21
] exploited Wavelet Neural Network (WNN) to predict stock price trends. The author also applied Rough Set (RS) for attribute reduction as an optimization. Rough Set was utilized to reduce the stock price trend feature dimensions. It was also used to determine the structure of the Wavelet Neural Network. The dataset of this work consists of five well-known stock market indices, i.e., (1) SSE Composite Index (China), (2) CSI 300 Index (China), (3) All Ordinaries Index (Australian), (4) Nikkei 225 Index (Japan), and (5) Dow Jones Index (USA). Evaluation of the model was based on different stock market indices, and the result was convincing with generality. By using Rough Set for optimizing the feature dimension before processing reduces the computational complexity. However, the author only stressed the parameter adjustment in the discussion part but did not specify the weakness of the model itself. Meanwhile, we also found that the evaluations were performed on indices, the same model may not have the same performance if applied on a specific stock.
Lee in [
20
] used the support vector machine (SVM) along with a hybrid feature selection method to carry out prediction of stock trends. The dataset in this research is a sub dataset of NASDAQ Index in Taiwan Economic Journal Database (TEJD) in 2008. The feature selection part was using a hybrid method, supported sequential forward search (SSFS) played the role of the wrapper. Another advantage of this work is that they designed a detailed procedure of parameter adjustment with performance under different parameter values. The clear structure of the feature selection model is also heuristic to the primary stage of model structuring. One of the limitations was that the performance of SVM was compared to back-propagation neural network (BPNN) only and did not compare to the other machine learning algorithms.
Sirignano and Cont leveraged a deep learning solution trained on a universal feature set of financial markets in [
40
]. The dataset used included buy and sell records of all transactions, and cancellations of orders for approximately 1000 NASDAQ stocks through the order book of the stock exchange. The NN consists of three layers with LSTM units and a feed-forward layer with rectified linear units (ReLUs) at last, with stochastic gradient descent (SGD) algorithm as an optimization. Their universal model was able to generalize and cover the stocks other than the ones in the training data. Though they mentioned the advantages of a universal model, the training cost was still expensive. Meanwhile, due to the inexplicit programming of the deep learning algorithm, it is unclear that if there are useless features contaminated when feeding the data into the model. Authors found out that it would have been better if they performed feature selection part before training the model and found it as an effective way to reduce the computational complexity.
Ni et al. in [
30
] predicted stock price trends by exploiting SVM and performed fractal feature selection for optimization. The dataset they used is the Shanghai Stock Exchange Composite Index (SSECI), with 19 technical indicators as features. Before processing the data, they optimized the input data by performing feature selection. When finding the best parameter combination, they also used a grid search method, which is k cross-validation. Besides, the evaluation of different feature selection methods is also comprehensive. As the authors mentioned in their conclusion part, they only considered the technical indicators but not macro and micro factors in the financial domain. The source of datasets that the authors used was similar to our dataset, which makes their evaluation results useful to our research. They also mentioned a method called k cross-validation when testing hyper-parameter combinations.
McNally et al. in [
27
] leveraged RNN and LSTM on predicting the price of Bitcoin, optimized by using the Boruta algorithm for feature engineering part, and it works similarly to the random forest classifier. Besides feature selection, they also used Bayesian optimization to select LSTM parameters. The Bitcoin dataset ranged from the 19th of August 2013 to 19th of July 2016. Used multiple optimization methods to improve the performance of deep learning methods. The primary problem of their work is overfitting. The research problem of predicting Bitcoin price trend has some similarities with stock market price prediction. Hidden features and noises embedded in the price data are threats of this work. The authors treated the research question as a time sequence problem. The best part of this paper is the feature engineering and optimization part; we could replicate the methods they exploited in our data pre-processing.
Weng et al. in [
45
] focused on short-term stock price prediction by using ensemble methods of four well-known machine learning models. The dataset for this research is five sets of data. They obtained these datasets from three open-sourced APIs and an R package named TTR. The machine learning models they used are (1) neural network regression ensemble (NNRE), (2) a Random Forest with unpruned regression trees as base learners (RFR), (3) AdaBoost with unpruned regression trees as base learners (BRT) and (4) a support vector regression ensemble (SVRE). A thorough study of ensemble methods specified for short-term stock price prediction. With background knowledge, the authors selected eight technical indicators in this study then performed a thoughtful evaluation of five datasets. The primary contribution of this paper is that they developed a platform for investors using R, which does not need users to input their own data but call API to fetch the data from online source straightforward. From the research perspective, they only evaluated the prediction of the price for 1 up to 10 days ahead but did not evaluate longer terms than two trading weeks or a shorter term than 1 day. The primary limitation of their research was that they only analyzed 20 U.S.-based stocks, the model might not be generalized to other stock market or need further revalidation to see if it suffered from overfitting problems.
Kara et al. in [
17
] also exploited ANN and SVM in predicting the movement of stock price index. The data set they used covers a time period from January 2, 1997, to December 31, 2007, of the Istanbul Stock Exchange. The primary strength of this work is its detailed record of parameter adjustment procedures. While the weaknesses of this work are that neither the technical indicator nor the model structure has novelty, and the authors did not explain how their model performed better than other models in previous works. Thus, more validation works on other datasets would help. They explained how ANN and SVM work with stock market features, also recorded the parameter adjustment. The implementation part of our research could benefit from this previous work.
Jeon et al. in [
16
] performed research on millisecond interval-based big dataset by using pattern graph tracking to complete stock price prediction tasks. The dataset they used is a millisecond interval-based big dataset of historical stock data from KOSCOM, from August 2014 to October 2014, 10G–15G capacity. The author applied Euclidean distance, Dynamic Time Warping (DTW) for pattern recognition. For feature selection, they used stepwise regression. The authors completed the prediction task by ANN and Hadoop and RHive for big data processing. The “
Results
” section is based on the result processed by a combination of SAX and Jaro–Winkler distance. Before processing the data, they generated aggregated data at 5-min intervals from discrete data. The primary strength of this work is the explicit structure of the whole implementation procedure. While they exploited a relatively old model, another weakness is the overall time span of the training dataset is extremely short. It is difficult to access the millisecond interval-based data in real life, so the model is not as practical as a daily based data model.
Huang et al. in [
12
] applied a fuzzy-GA model to complete the stock selection task. They used the key stocks of the 200 largest market capitalization listed as the investment universe in the Taiwan Stock Exchange. Besides, the yearly financial statement data and the stock returns were taken from the Taiwan Economic Journal (TEJ) database at
www.tej.com.tw/
for the time period from year 1995 to year 2009. They conducted the fuzzy membership function with model parameters optimized with GA and extracted features for optimizing stock scoring. The authors proposed an optimized model for selection and scoring of stocks. Different from the prediction model, the authors more focused on stock rankings, selection, and performance evaluation. Their structure is more practical among investors. But in the model validation part, they did not compare the model with existed algorithms but the statistics of the benchmark, which made it challenging to identify if GA would outperform other algorithms.
Fischer and Krauss in [
5
] applied long short-term memory (LSTM) on financial market prediction. The dataset they used is S&P 500 index constituents from Thomson Reuters. They obtained all month-end constituent lists for the S&P 500 from Dec 1989 to Sep 2015, then consolidated the lists into a binary matrix to eliminate survivor bias. The authors also used RMSprop as an optimizer, which is a mini-batch version of rprop. The primary strength of this work is that the authors used the latest deep learning technique to perform predictions. They relied on the LSTM technique, lack of background knowledge in the financial domain. Although the LSTM outperformed the standard DNN and logistic regression algorithms, while the author did not mention the effort to train an LSTM with long-time dependencies.
Tsai and Hsiao in [
42
] proposed a solution as a combination of different feature selection methods for prediction of stocks. They used Taiwan Economic Journal (TEJ) database as data source. The data used in their analysis was from year 2000 to 2007. In their work, they used a sliding window method and combined it with multi layer perceptron (MLP) based artificial neural networks with back propagation, as their prediction model. In their work, they also applied principal component analysis (PCA) for dimensionality reduction, genetic algorithms (GA) and the classification and regression trees (CART) to select important features. They did not just rely on technical indices only. Instead, they also included both fundamental and macroeconomic indices in their analysis. The authors also reported a comparison on feature selection methods. The validation part was done by combining the model performance stats with statistical analysis.
Pimenta et al. in [
32
] leveraged an automated investing method by using multi-objective genetic programming and applied it in the stock market. The dataset was obtained from Brazilian stock exchange market (BOVESPA), and the primary techniques they exploited were a combination of multi-objective optimization, genetic programming, and technical trading rules. For optimization, they leveraged genetic programming (GP) to optimize decision rules. The novelty of this paper was in the evaluation part. They included a historical period, which was a critical moment of Brazilian politics and economics when performing validation. This approach reinforced the generalization strength of their proposed model. When selecting the sub-dataset for evaluation, they also set criteria to ensure more asset liquidity. While the baseline of the comparison was too basic and fundamental, and the authors did not perform any comparison with other existing models.
Huang and Tsai in [
13
] conducted a filter-based feature selection assembled with a hybrid self-organizing feature map (SOFM) support vector regression (SVR) model to forecast Taiwan index futures (FITX) trend. They divided the training samples into clusters to marginally improve the training efficiency. The authors proposed a comprehensive model, which was a combination of two novel machine learning techniques in stock market analysis. Besides, the optimizer of feature selection was also applied before the data processing to improve the prediction accuracy and reduce the computational complexity of processing daily stock index data. Though they optimized the feature selection part and split the sample data into small clusters, it was already strenuous to train daily stock index data of this model. It would be difficult for this model to predict trading activities in shorter time intervals since the data volume would be increased drastically. Moreover, the evaluation is not strong enough since they set a single SVR model as a baseline, but did not compare the performance with other previous works, which caused difficulty for future researchers to identify the advantages of SOFM-SVR model why it outperforms other algorithms.
Thakur and Kumar in [
41
] also developed a hybrid financial trading support system by exploiting multi-category classifiers and random forest (RAF). They conducted their research on stock indices from NASDAQ, DOW JONES, S&P 500, NIFTY 50, and NIFTY BANK. The authors proposed a hybrid model combined random forest (RF) algorithms with a weighted multicategory generalized eigenvalue support vector machine (WMGEPSVM) to generate “Buy/Hold/Sell” signals. Before processing the data, they used Random Forest (RF) for feature pruning. The authors proposed a practical model designed for real-life investment activities, which could generate three basic signals for investors to refer to. They also performed a thorough comparison of related algorithms. While they did not mention the time and computational complexity of their works. Meanwhile, the unignorable issue of their work was the lack of financial domain knowledge background. The investors regard the indices data as one of the attributes but could not take the signal from indices to operate a specific stock straightforward.
Hsu in [
11
] assembled feature selection with a back propagation neural network (BNN) combined with genetic programming to predict the stock/futures price. The dataset in this research was obtained from Taiwan Stock Exchange Corporation (TWSE). The authors have introduced the description of the background knowledge in detail. While the weakness of their work is that it is a lack of data set description. This is a combination of the model proposed by other previous works. Though we did not see the novelty of this work, we can still conclude that the genetic programming (GP) algorithm is admitted in stock market research domain. To reinforce the validation strengths, it would be good to consider adding GP models into evaluation if the model is predicting a specific price.
Hafezi et al. in [
7
] built a bat-neural network multi-agent system (BN-NMAS) to predict stock price. The dataset was obtained from the Deutsche bundes-bank. They also applied the Bat algorithm (BA) for optimizing neural network weights. The authors illustrated their overall structure and logic of system design in clear flowcharts. While there were very few previous works that had performed on DAX data, it would be difficult to recognize if the model they proposed still has the generality if migrated on other datasets. The system design and feature selection logic are fascinating, which worth referring to. Their findings in optimization algorithms are also valuable for the research in the stock market price prediction research domain. It is worth trying the Bat algorithm (BA) when constructing neural network models.
Long et al. in [
25
] conducted a deep learning approach to predict the stock price movement. The dataset they used is the Chinese stock market index CSI 300. For predicting the stock price movement, they constructed a multi-filter neural network (MFNN) with stochastic gradient descent (SGD) and back propagation optimizer for learning NN parameters. The strength of this paper is that the authors exploited a novel model with a hybrid model constructed by different kinds of neural networks, it provides an inspiration for constructing hybrid neural network structures.
Atsalakis and Valavanis in [
1
] proposed a solution of a neuro-fuzzy system, which is composed of controller named as Adaptive Neuro Fuzzy Inference System (ANFIS), to achieve short-term stock price trend prediction. The noticeable strength of this work is the evaluation part. Not only did they compare their proposed system with the popular data models, but also compared with investment strategies. While the weakness that we found from their proposed solution is that their solution architecture is lack of optimization part, which might limit their model performance. Since our proposed solution is also focusing on short-term stock price trend prediction, this work is heuristic for our system design. Meanwhile, by comparing with the popular trading strategies from investors, their work inspired us to compare the strategies used by investors with techniques used by researchers.
Nekoeiqachkanloo et al. in [
29
] proposed a system with two different approaches for stock investment. The strengths of their proposed solution are obvious. First, it is a comprehensive system that consists of data pre-processing and two different algorithms to suggest the best investment portions. Second, the system also embedded with a forecasting component, which also retains the features of the time series. Last but not least, their input features are a mix of fundamental features and technical indices that aim to fill in the gap between the financial domain and technical domain. However, their work has a weakness in the evaluation part. Instead of evaluating the proposed system on a large dataset, they chose 25 well-known stocks. There is a high possibility that the well-known stocks might potentially share some common hidden features.
As another related latest work, Idrees et al. [
14
] published a time series-based prediction approach for the volatility of the stock market. ARIMA is not a new approach in the time series prediction research domain. Their work is more focusing on the feature engineering side. Before feeding the features into ARIMA models, they designed three steps for feature engineering: Analyze the time series, identify if the time series is stationary or not, perform estimation by plot ACF and PACF charts and look for parameters. The only weakness of their proposed solution is that the authors did not perform any customization on the existing ARIMA model, which might limit the system performance to be improved.
One of the main weaknesses found in the related works is limited data-preprocessing mechanisms built and used. Technical works mostly tend to focus on building prediction models. When they select the features, they list all the features mentioned in previous works and go through the feature selection algorithm then select the best-voted features. Related works in the investment domain have shown more interest in behavior analysis, such as how herding behaviors affect the stock performance, or how the percentage of inside directors hold the firm’s common stock affects the performance of a certain stock. These behaviors often need a pre-processing procedure of standard technical indices and investment experience to recognize.
In the related works, often a thorough statistical analysis is performed based on a special dataset and conclude new features rather than performing feature selections. Some data, such as the percentage of a certain index fluctuation has been proven to be effective on stock performance. We believe that by extracting new features from data, then combining such features with existed common technical indices will significantly benefit the existing and well-tested prediction models.
The dataset
This section details the data that was extracted from the public data sources, and the final dataset that was prepared. Stock market-related data are diverse, so we first compared the related works from the survey of financial research works in stock market data analysis to specify the data collection directions. After collecting the data, we defined a data structure of the dataset. Given below, we describe the dataset in detail, including the data structure, and data tables in each category of data with the segment definitions.
Description of our dataset
In this section, we will describe the dataset in detail. This dataset consists of 3558 stocks from the Chinese stock market. Besides the daily price data, daily fundamental data of each stock ID, we also collected the suspending and resuming history, top 10 shareholders, etc. We list two reasons that we choose 2 years as the time span of this dataset: (1) most of the investors perform stock market price trend analysis using the data within the latest 2 years, (2) using more recent data would benefit the analysis result. We collected data through the open-sourced API, namely Tushare [
43
], mean-while we also leveraged a web-scraping technique to collect data from Sina Finance web pages, SWS Research website.
Data structure
Figure
1
illustrates all the data tables in the dataset. We collected four categories of data in this dataset: (1) basic data, (2) trading data, (3) finance data, and (4) other reference data. All the data tables can be linked to each other by a common field called “Stock ID” It is a unique stock identifier registered in the Chinese Stock market. Table
1
shows an overview of the dataset.
Fig. 1
Data structure for the extracted dataset
Full size image
Table 1 Dataset overview table with different categories and subsets of fields
Full size table
The Table
1
lists the field information of each data table as well as which category the data table belongs to.
Methods
In this section, we present the proposed methods and the design of the proposed solution. Moreover, we also introduce the architecture design as well as algorithmic and implementation details.
Problem statement
We analyzed the best possible approach for predicting short-term price trends from different aspects: feature engineering, financial domain knowledge, and prediction algorithm. Then we addressed three research questions in each aspect, respectively: How can feature engineering benefit model prediction accuracy? How do findings from the financial domain benefit prediction model design? And what is the best algorithm for predicting short-term price trends?
The first research question is about feature engineering. We would like to know how the feature selection method benefits the performance of prediction models. From the abundance of the previous works, we can conclude that stock price data embedded with a high level of noise, and there are also correlations between features, which makes the price prediction notoriously difficult. That is also the primary reason for most of the previous works introduced the feature engineering part as an optimization module.
The second research question is evaluating the effectiveness of findings we extracted from the financial domain. Different from the previous works, besides the common evaluation of data models such as the training costs and scores, our evaluation will emphasize the effectiveness of newly added features that we extracted from the financial domain. We introduce some features from the financial domain. While we only obtained some specific findings from previous works, and the related raw data needs to be processed into usable features. After extracting related features from the financial domain, we combine the features with other common technical indices for voting out the features with a higher impact. There are numerous features said to be effective from the financial domain, and it would be impossible for us to cover all of them. Thus, how to appropriately convert the findings from the financial domain to a data processing module of our system design is a hidden research question that we attempt to answer.
The third research question is that which algorithms are we going to model our data? From the previous works, researchers have been putting efforts into the exact price prediction. We decompose the problem into predicting the trend and then the exact number. This paper focuses on the first step. Hence, the objective has been converted to resolve a binary classification problem, meanwhile, finding an effective way to eliminate the negative effect brought by the high level of noise. Our approach is to decompose the complex problem into sub-problems which have fewer dependencies and resolve them one by one, and then compile the resolutions into an ensemble model as an aiding system for investing behavior reference.
In the previous works, researchers have been using a variety of models for predicting stock price trends. While most of the best-performed models are based on machine learning techniques, in this work, we will compare our approach with the outperformed machine learning models in the evaluation part and find the solution for this research question.
Proposed solution
The high-level architecture of our proposed solution could be separated into three parts. First is the feature selection part, to guarantee the selected features are highly effective. Second, we look into the data and perform the dimensionality reduction. And the last part, which is the main contribution of our work is to build a prediction model of target stocks. Figure
2
depicts a high-level architecture of the proposed solution.
Fig. 2
High-level architecture of the proposed solution
Full size image
There are ways to classify different categories of stocks. Some investors prefer long-term investments, while others show more interest in short-term investments. It is common to see the stock-related reports showing an average performance, while the stock price is increasing drastically; this is one of the phenomena that indicate the stock price prediction has no fixed rules, thus finding effective features before training a model on data is necessary.
In this research, we focus on the short-term price trend prediction. Currently, we only have the raw data with no labels. So, the very first step is to label the data. We mark the price trend by comparing the current closing price with the closing price of n trading days ago, the range of n is from 1 to 10 since our research is focusing on the short-term. If the price trend goes up, we mark it as 1 or mark as 0 in the opposite case. To be more specified, we use the indices from the indices of
n
−
1
th
day to predict the price trend of the
n
th
day.
According to the previous works, some researchers who applied both financial domain knowledge and technical methods on stock data were using rules to filter the high-quality stocks. We referred to their works and exploited their rules to contribute to our feature extension design.
However, to ensure the best performance of the prediction model, we will look into the data first. There are a large number of features in the raw data; if we involve all the features into our consideration, it will not only drastically increase the computational complexity but will also cause side effects if we would like to perform unsupervised learning in further research. So, we leverage the recursive feature elimination (RFE) to ensure all the selected features are effective.
We found most of the previous works in the technical domain were analyzing all the stocks, while in the financial domain, researchers prefer to analyze the specific scenario of investment, to fill the gap between the two domains, we decide to apply a feature extension based on the findings we gathered from the financial domain before we start the RFE procedure.
Since we plan to model the data into time series, the number of the features, the more complex the training procedure will be. So, we will leverage the dimensionality reduction by using randomized PCA at the beginning of our proposed solution architecture.
Detailed technical design elaboration
This section provides an elaboration of the detailed technical design as being a comprehensive solution based on utilizing, combining, and customizing several existing data preprocessing, feature engineering, and deep learning techniques. Figure
3
provides the detailed technical design from data processing to prediction, including the data exploration. We split the content by main procedures, and each procedure contains algorithmic steps. Algorithmic details are elaborated in the next section. The contents of this section will focus on illustrating the data workflow.
Fig. 3
Detailed technical design of the proposed solution
Full size image
Based on the literature review, we select the most commonly used technical indices and then feed them into the feature extension procedure to get the expanded feature set. We will select the most effective
i
features from the expanded feature set. Then we will feed the data with
i
selected features into the PCA algorithm to reduce the dimension into
j
features. After we get the best combination of
i
and
j
, we process the data into finalized the feature set and feed them into the LSTM [
10
] model to get the price trend prediction result.
The novelty of our proposed solution is that we will not only apply the technical method on raw data but also carry out the feature extensions that are used among stock market investors. Details on feature extension are given in the next subsection. Experiences gained from applying and optimizing deep learning based solutions in [
37
,
38
] were taken into account while designing and customizing feature engineering and deep learning solution in this work.
Applying feature extension
The first main procedure in Fig.
3
is the feature extension. In this block, the input data is the most commonly used technical indices concluded from related works. The three feature extension methods are max–min scaling, polarizing, and calculating fluctuation percentage. Not all the technical indices are applicable for all three of the feature extension methods; this procedure only applies the meaningful extension methods on technical indices. We choose meaningful extension methods while looking at how the indices are calculated. The technical indices and the corresponding feature extension methods are illustrated in Table
2
.
Table 2 Feature extension method selection
Full size table
After the feature extension procedure, the expanded features will be combined with the most commonly used technical indices, i.e., input data with output data, and feed into RFE block as input data in the next step.
Applying recursive feature elimination
After the feature extension above, we explore the most effective
i
features by using the Recursive Feature Elimination (RFE) algorithm [
6
]. We estimate all the features by two attributes, coefficient, and feature importance. We also limit the features that remove from the pool by one, which means we will remove one feature at each step and retain all the relevant features. Then the output of the RFE block will be the input of the next step, which refers to PCA.
Applying principal component analysis (PCA)
The very first step before leveraging PCA is feature pre-processing. Because some of the features after RFE are percentage data, while others are very large numbers, i.e., the output from RFE are in different units. It will affect the principal component extraction result. Thus, before feeding the data into the PCA algorithm [
8
], a feature pre-processing is necessary. We also illustrate the effectiveness and methods comparison in “
Results
” section.
After performing feature pre-processing, the next step is to feed the processed data with selected
i
features into the PCA algorithm to reduce the feature matrix scale into
j
features. This step is to retain as many effective features as possible and meanwhile eliminate the computational complexity of training the model. This research work also evaluates the best combination of
i
and
j,
which has relatively better prediction accuracy, meanwhile, cuts the computational consumption. The result can be found in the “
Results
” section, as well. After the PCA step, the system will get a reshaped matrix with
j
columns.
Fitting long short-term memory (LSTM) model
PCA reduced the dimensions of the input data, while the data pre-processing is mandatory before feeding the data into the LSTM layer. The reason for adding the data pre-processing step before the LSTM model is that the input matrix formed by principal components has no time steps. While one of the most important parameters of training an LSTM is the number of time steps. Hence, we have to model the matrix into corresponding time steps for both training and testing dataset.
After performing the data pre-processing part, the last step is to feed the training data into LSTM and evaluate the performance using testing data. As a variant neural network of RNN, even with one LSTM layer, the NN structure is still a deep neural network since it can process sequential data and memorizes its hidden states through time. An LSTM layer is composed of one or more LSTM units, and an LSTM unit consists of cells and gates to perform classification and prediction based on time series data.
The LSTM structure is formed by two layers. The input dimension is determined by j after the PCA algorithm. The first layer is the input LSTM layer, and the second layer is the output layer. The final output will be 0 or 1 indicates if the stock price trend prediction result is going down or going up, as a supporting suggestion for the investors to perform the next investment decision.
Design discussion
Feature extension is one of the novelties of our proposed price trend predicting system. In the feature extension procedure, we use technical indices to collaborate with the heuristic processing methods learned from investors, which fills the gap between the financial research area and technical research area.
Since we proposed a system of price trend prediction, feature engineering is extremely important to the final prediction result. Not only the feature extension method is helpful to guarantee we do not miss the potentially correlated feature, but also feature selection method is necessary for pooling the effective features. The more irrelevant features are fed into the model, the more noise would be introduced. Each main procedure is carefully considered contributing to the whole system design.
Besides the feature engineering part, we also leverage LSTM, the state-of-the-art deep learning method for time-series prediction, which guarantees the prediction model can capture both complex hidden pattern and the time-series related pattern.
It is known that the training cost of deep learning models is expansive in both time and hardware aspects; another advantage of our system design is the optimization procedure—PCA. It can retain the principal components of the features while reducing the scale of the feature matrix, thus help the system to save the training cost of processing the large time-series feature matrix.
Algorithm elaboration
This section provides comprehensive details on the algorithms we built while utilizing and customizing different existing techniques. Details about the terminologies, parameters, as well as optimizers. From the legend on the right side of Fig.
3
, we note the algorithm steps as octagons, all of them can be found in this “
Algorithm elaboration
” section.
Before dive deep into the algorithm steps, here is the brief introduction of data pre-processing: since we will go through the supervised learning algorithms, we also need to program the ground truth. The ground truth of this research is programmed by comparing the closing price of the current trading date with the closing price of the previous trading date the users want to compare with. Label the price increase as 1, else the ground truth will be labeled as 0. Because this research work is not only focused on predicting the price trend of a specific period of time but short-term in general, the ground truth processing is according to a range of trading days. While the algorithms will not change with the prediction term length, we can regard the term length as a parameter.
The algorithmic detail is elaborated, respectively, the first algorithm is the hybrid feature engineering part for preparing high-quality training and testing data. It corresponds to the Feature extension, RFE, and PCA blocks in Fig.
3
. The second algorithm is the LSTM procedure block, including time-series data pre-processing, NN constructing, training, and testing.
Algorithm 1: Short-term stock market price trend prediction—applying feature engineering using FE + RFE + PCA
The function FE is corresponding to the feature extension block. For the feature extension procedure, we apply three different processing methods to translate the findings from the financial domain to a technical module in our system design. While not all the indices are applicable for expanding, we only choose the proper method(s) for certain features to perform the feature extension (FE), according to Table
2
.
Normalize method preserves the relative frequencies of the terms, and transform the technical indices into the range of [0, 1]. Polarize is a well-known method often used by real-world investors, sometimes they prefer to consider if the technical index value is above or below zero, we program some of the features using polarize method and prepare for RFE. Max-min (or min-max) [
35
] scaling is a transformation method often used as an alternative to zero mean and unit variance scaling. Another well-known method used is fluctuation percentage, and we transform the technical indices fluctuation percentage into the range of [− 1, 1].
The function RFE () in the first algorithm refers to recursive feature elimination. Before we perform the training data scale reduction, we will have to make sure that the features we selected are effective. Ineffective features will not only drag down the classification precision but also add more computational complexity. For the feature selection part, we choose recursive feature elimination (RFE). As [
45
] explained, the process of recursive feature elimination can be split into the ranking algorithm, resampling, and external validation.
For the ranking algorithm, it fits the model to the features and ranks by the importance to the model. We set the parameter to retain
i
numbers of features, and at each iteration of feature selection retains
Si
top-ranked features, then refit the model and assess the performance again to begin another iteration. The ranking algorithm will eventually determine the top
Si
features.
The RFE algorithm is known to have suffered from the over-fitting problem. To eliminate the over-fitting issue, we will run the RFE algorithm multiple times on randomly selected stocks as the training set and ensure all the features we select are high-weighted. This procedure is called data resampling. Resampling can be built as an optimization step as an outer layer of the RFE algorithm.
The last part of our hybrid feature engineering algorithm is for optimization purposes. For the training data matrix scale reduction, we apply Randomized principal component analysis (PCA) [
31
], before we decide the features of the classification model.
Financial ratios of a listed company are used to present the growth ability, earning ability, solvency ability, etc. Each financial ratio consists of a set of technical indices, each time we add a technical index (or feature) will add another column of data into the data matrix and will result in low training efficiency and redundancy. If non-relevant or less relevant features are included in training data, it will also decrease the precision of classification.
The above equation represents the explanation power of principal components extracted by PCA method for original data. If an ACR is below 85%, the PCA method would be unsuitable due to a loss of original information. Because the covariance matrix is sensitive to the order of magnitudes of data, there should be a data standardize procedure before performing the PCA. The commonly used standardized methods are mean-standardization and normal-standardization and are noted as given below:
Mean-standardization:
\(X_{ij}^{*} = X_{ij} /\overline{{X_{j} }}\)
, which
\(\overline{{X_{j} }}\)
represents the mean value.
Normal-standardization:
\(X_{ij}^{*} = (X_{ij} - \overline{{X_{j} }} )/s_{j}\)
, which
\(\overline{{X_{j} }}\)
represents the mean value, and
\(s_{j}\)
is the standard deviation.
The array
fe_array
is defined according to Table
2
, row number maps to the features, columns 0, 1, 2, 3 note for the extension methods of normalize, polarize, max–min scale, and fluctuation percentage, respectively. Then we fill in the values for the array by the rule where 0 stands for no necessity to expand and 1 for features need to apply the corresponding extension methods. The final algorithm of data preprocessing using RFE and PCA can be illustrated as Algorithm 1.
Algorithm 2: Price trend prediction model using LSTM
After the principal component extraction, we will get the scale-reduced matrix, which means
i
most effective features are converted into
j
principal components for training the prediction model. We utilized an LSTM model and added a conversion procedure for our stock price dataset. The detailed algorithm design is illustrated in Alg 2. The function
TimeSeriesConversion
() converts the principal components matrix into time series by shifting the input data frame according to the number of time steps [
3
], i.e., term length in this research. The processed dataset consists of the input sequence and forecast sequence. In this research, the parameter of
LAG
is 1, because the model is detecting the pattern of features fluctuation on a daily basis. Meanwhile, the
N_TIME_STEPS
is varied from 1 trading day to 10 trading days. The functions
DataPartition (), FitModel (), EvaluateModel ()
are regular steps without customization. The NN structure design, optimizer decision, and other parameters are illustrated in function
ModelCompile ()
.
Results
Some procedures impact the efficiency but do not affect the accuracy or precision and vice versa, while other procedures may affect both efficiency and prediction result. To fully evaluate our algorithm design, we structure the evaluation part by main procedures and evaluate how each procedure affects the algorithm performance. First, we evaluated our solution on a machine with 2.2 GHz i7 processor, with 16 GB of RAM. Furthermore, we also evaluated our solution on Amazon EC2 instance, 3.1 GHz Processor with 16 vCPUs, and 64 GB RAM.
In the implementation part, we expanded 20 features into 54 features, while we retain 30 features that are the most effective. In this section, we discuss the evaluation of feature selection. The dataset was divided into two different subsets, i.e., training and testing datasets. Test procedure included two parts, one testing dataset is for feature selection, and another one is for model testing. We note the feature selection dataset and model testing dataset as DS_test_f and DS_test_m, respectively.
We randomly selected two-thirds of the stock data by stock ID for RFE training and note the dataset as DS_train_f; all the data consist of full technical indices and expanded features throughout 2018. The estimator of the RFE algorithm is SVR with linear kernels. We rank the 54 features by voting and get 30 effective features then process them using the PCA algorithm to perform dimension reduction and reduce the features into 20 principal components. The rest of the stock data forms the testing dataset DS_test_f to validate the effectiveness of principal components we extracted from selected features. We reformed all the data from 2018 as the training dataset of the data model and noted as DS_train_m. The model testing dataset DS_test_m consists of the first 3 months of data in 2019, which has no overlap with the dataset we utilized in the previous steps. This approach is to prevent the hidden problem caused by overfitting.
Term length
To build an efficient prediction model, instead of the approach of modeling the data to time series, we determined to use 1 day ahead indices data to predict the price trend of the next day. We tested the RFE algorithm on a range of short-term from 1 day to 2 weeks (ten trading days), to evaluate how the commonly used technical indices correlated to price trends. For evaluating the prediction term length, we fully expanded the features as Table
2
, and feed them to RFE. During the test, we found that different length of the term has a different level of sensitive-ness to the same indices set.
We get the close price of the first trading date and compare it with the close price of the
n
_
th
trading date. Since we are predicting the price trend, we do not consider the term lengths if the cross-validation score is below 0.5. And after the test, as we can see from Fig.
4
, there are three-term lengths that are most sensitive to the indices we selected from the related works. They are
n
= {2, 5, 10}, which indicates that price trend prediction of every other day, 1 week, and 2 weeks using the indices set are likely to be more reliable.
Fig. 4
How do term lengths affect the cross-validation score of RFE
Full size image
While these curves have different patterns, for the length of 2 weeks, the cross-validation score increases with the number of features selected. If the prediction term length is 1 week, the cross-validation score will decrease if selected over 8 features. For every other day price trend prediction, the best cross-validation score is achieved by selecting 48 features. Biweekly prediction requires 29 features to achieve the best score. In Table
3
, we listed the top 15 effective features for these three-period lengths. If we predict the price trend of every other day, the cross-validation score merely fluctuates with the number of features selected. So, in the next step, we will evaluate the RFE result for these three-term lengths, as shown in Fig.
4
.
Table 3 Effective features corresponding to term lengths
Full size table
We compare the output feature set of RFE with the all-original feature set as a baseline, the all-original feature set consists of n features and we choose
n
most effective features from RFE output features to evaluate the result using linear SVR. We used two different approaches to evaluate feature effectiveness. The first method is to combine all the data into one large matrix and evaluate them by running the RFE algorithm once. Another method is to run RFE for each individual stock and calculate the most effective features by voting.
Feature extension and RFE
From the result of the previous subsection, we can see that when predicting the price trend for every other day or biweekly, the best result is achieved by selecting a large number of features. Within the selected features, some features processed from extension methods have better ranks than original features, which proves that the feature extension method is useful for optimizing the model. The feature extension affects both precision and efficiency, while in this part, we only discuss the precision aspect and leave efficiency part in the next step since PCA is the most effective method for training efficiency optimization in our design. We involved an evaluation of how feature extension affects RFE and use the test result to measure the improvement of involving feature extension.
We further test the effectiveness of feature extension, i.e., if polarize, max–min scale, and calculate fluctuation percentage works better than original technical indices. The best case to leverage this test is the weekly prediction since it has the least effective feature selected. From the result we got from the last section, we know the best cross-validation score appears when selecting 8 features. The test consists of two steps, and the first step is to test the feature set formed by original features only, in this case, only SLOWK, SLOWD, and RSI_5 are included. The next step is to test the feature set of all 8 features we selected in the previous subsection. We leveraged the test by defining the simplest DNN model with three layers.
The normalized confusion matrix of testing the two feature sets are illustrated in Fig.
5
. The left one is the confusion matrix of the feature set with expanded features, and the right one besides is the test result of using original features only. Both precisions of true positive and true negative have been improved by 7% and 10%, respectively, which proves that our feature extension method design is reasonably effective.
Fig. 5
Confusion matrix of validating feature extension effectiveness
Full size image
Feature reduction using principal component analysis
PCA will affect the algorithm performance on both prediction accuracy and training efficiency, while this part should be evaluated with the NN model, so we also defined the simplest DNN model with three layers as we used in the previous step to perform the evaluation. This part introduces the evaluation method and result of the optimization part of the model from computational efficiency and accuracy impact perspectives.
In this section, we will choose bi-weekly prediction to perform a use case analysis, since it has a smoothly increasing cross-validation score curve, moreover, unlike every other day prediction, it has excluded more than 20 ineffective features already. In the first step, we select all 29 effective features and train the NN model without performing PCA. It creates a baseline of the accuracy and training time for comparison. To evaluate the accuracy and efficiency, we keep the number of the principal component as 5, 10, 15, 20, 25. Table
4
recorded how the number of features affects the model training efficiency, then uses the stack bar chart in Fig.
6
to illustrate how PCA affects training efficiency. Table
6
shows accuracy and efficiency analysis on different procedures for the pre-processing of features. The times taken shown in Tables
4
,
6
are based on experiments conducted in a standard user machine to show the viability of our solution with limited or average resource availability.
Table 4 Relationship between the number of principal components and training efficiency
Full size table
Fig. 6
Relationship between feature number and training time
Full size image
We also listed the confusion matrix of each test in Fig.
7
. The stack bar chart shows that the overall time spends on training the model is decreasing by the number of selected features, while the PCA method is significantly effective in optimizing training dataset preparation. For the time spent on the training stage, PCA is not as effective as the data preparation stage. While there is the possibility that the optimization effect of PCA is not drastic enough because of the simple structure of the NN model.
Fig. 7
How does the number of principal components affect evaluation results
Full size image
Table
5
indicates that the overall prediction accuracy is not drastically affected by reducing the dimension. However, the accuracy could not fully support if the PCA has no side effect to model prediction, so we looked into the confusion matrices of test results.
Table 5 How does the number of selected features affect the prediction accuracy
Full size table
From Fig.
7
we can conclude that PCA does not have a severe negative impact on prediction precision. The true positive rate and false positive rate are barely be affected, while the false negative and true negative rates are influenced by 2% to 4%. Besides evaluating how the number of selected features affects the training efficiency and model performance, we also leveraged a test upon how data pre-processing procedures affect the training procedure and predicting result. Normalizing and max–min scaling is the most commonly seen data pre-procedure performed before PCA, since the measure units of features are varied, and it is said that it could increase the training efficiency afterward.
Table 6 Accuracy and efficiency analysis on feature pre-processing procedures
Full size table
We leveraged another test on adding pre-procedures before extracting 20 principal components from the original dataset and make the comparison in the aspects of time elapse of training stage and prediction precision. However, the test results lead to different conclusions. In Table
6
we can conclude that feature pre-processing does not have a significant impact on training efficiency, but it does influence the model prediction accuracy. Moreover, the first confusion matrix in Fig.
8
indicates that without any feature pre-processing procedure, the false-negative rate and true negative rate are severely affected, while the true positive rate and false positive rate are not affected. If it performs the normalization before PCA, both true positive rate and true negative rate are decreasing by approximately 10%. This test also proved that the best feature pre-processing method for our feature set is exploiting the max–min scale.
Fig. 8
Confusion matrices of different feature pre-processing methods
Full size image
Discussion
In this section, we discuss and compare the results of our proposed model, other approaches, and the most related works.
Comparison with related works
From the previous works, we found the most commonly exploited models for short-term stock market price trend prediction are support vector machine (SVM), multilayer perceptron artificial neural network (MLP), Naive Bayes classifier (NB), random forest classifier (RAF) and logistic regression classifier (LR). The test case of comparison is also bi-weekly price trend prediction, to evaluate the best result of all models, we keep all 29 features selected by the RFE algorithm. For MLP evaluation, to test if the number of hidden layers would affect the metric scores, we noted layer number as
n
and tested
n
= {1, 3, 5}, 150 training epochs for all the tests, found slight differences in the model performance, which indicates that the variable of MLP layer number hardly affects the metric scores.
From the confusion matrices in Fig.
9
, we can see all the machine learning models perform well when training with the full feature set we selected by RFE. From the perspective of training time, training the NB model got the best efficiency. LR algorithm cost less training time than other algorithms while it can achieve a similar prediction result with other costly models such as SVM and MLP. RAF algorithm achieved a relatively high true-positive rate while the poor performance in predicting negative labels. For our proposed LSTM model, it achieves a binary accuracy of 93.25%, which is a significantly high precision of predicting the bi-weekly price trend. We also pre-processed data through PCA and got five principal components, then trained for 150 epochs. The learning curve of our proposed solution, based on feature engineering and the LSTM model, is illustrated in Fig.
10
. The confusion matrix is the figure on the right in Fig.
11
, and detailed metrics scores can be found in Table
9
.
Fig. 9
Model prediction comparison—confusion matrices
Full size image
Fig. 10
Learning curve of proposed solution
Full size image
Fig. 11
Proposed model prediction precision comparison—confusion matrices
Full size image
The detailed evaluate results are recorded in Table
7
. We will also initiate a discussion upon the evaluation result in the next section.
Table 7 Model performance comparison—metric scores
Full size table
Because the resulting structure of our proposed solution is different from most of the related works, it would be difficult to make naïve comparison with previous works. For example, it is hard to find the exact accuracy number of price trend prediction in most of the related works since the authors prefer to show the gain rate of simulated investment. Gain rate is a processed number based on simulated investment tests, sometimes one correct investment decision with a large trading volume can achieve a high gain rate regardless of the price trend prediction accuracy. Besides, it is also a unique and heuristic innovation in our proposed solution, we transform the problem of predicting an exact price straight forward to two sequential problems, i.e., predicting the price trend first, focus on building an accurate binary classification model, construct a solid foundation for predicting the exact price change in future works. Besides the different result structure, the datasets that previous works researched on are also different from our work. Some of the previous works involve news data to perform sentiment analysis and exploit the SE part as another system component to support their prediction model.
The latest related work that can compare is Zubair et al. [
47
], the authors take multiple r-square for model accuracy measurement. Multiple r-square is also called the coefficient of determination, and it shows the strength of predictor variables explaining the variation in stock return [
28
]. They used three datasets (KSE 100 Index, Lucky Cement Stock, Engro Fertilizer Limited) to evaluate the proposed multiple regression model and achieved 95%, 89%, and 97%, respectively. Except for the KSE 100 Index, the dataset choice in this related work is individual stocks; thus, we choose the evaluation result of the first dataset of their proposed model.
We listed the leading stock price trend prediction model performance in Table
8
, from the comparable metrics, the metric scores of our proposed solution are generally better than other related works. Instead of concluding arbitrarily that our proposed model outperformed other models in related works, we first look into the dataset column of Table
8
. By looking into the dataset used by each work [
18
], only trained and tested their proposed solution on three individual stocks, which is difficult to prove the generalization of their proposed model. Ayo [
2
] leveraged analysis on the stock data from the New York Stock Exchange (NYSE), while the weakness is they only performed analysis on closing price, which is a feature embedded with high noise. Zubair et al. [
47
] trained their proposed model on both individual stocks and index price, but as we have mentioned in the previous section, index price only consists of the limited number of features and stock IDs, which will further affect the model training quality. For our proposed solution, we collected sufficient data from the Chinese stock market, and applied FE + RFE algorithm on the original indices to get more effective features, the comprehensive evaluation result of 3558 stock IDs can reasonably explain the generalization and effectiveness of our proposed solution in Chinese stock market. However, the authors of Khaidem and Dey [
18
] and Ayo [
2
] chose to analyze the stock market in the United States, Zubair et al. [
47
] performed analysis on Pakistani stock market price, and we obtained the dataset from Chinese stock market, the policies of different countries might impact the model performance, which needs further research to validate.
Table 8 Comparison of proposed solution with related works
Full size table
Proposed model evaluation—PCA effectiveness
Besides comparing the performance across popular machine learning models, we also evaluated how the PCA algorithm optimizes the training procedure of the proposed LSTM model. We recorded the confusion matrices comparison between training the model by 29 features and by five principal components in Fig.
11
. The model training using the full 29 features takes 28.5 s per epoch on average. While it only takes 18 s on average per epoch training on the feature set of five principal components. PCA has significantly improved the training efficiency of the LSTM model by 36.8%. The detailed metrics data are listed in Table
9
. We will leverage a discussion in the next section about complexity analysis.
Table 9 Proposed model performance comparison—with and without PCA
Full size table
Complexity analysis of proposed solution
This section analyzes the complexity of our proposed solution. The Long Short-term Memory is different from other NNs, and it is a variant of standard RNN, which also has time steps with memory and gate architecture. In the previous work [
46
], the author performed an analysis of the RNN architecture complexity. They introduced a method to regard RNN as a directed acyclic graph and proposed a concept of recurrent depth, which helps perform the analysis on the intricacy of RNN.
The recurrent depth is a positive rational number, and we denote it as
\(d_{rc}\)
. As the growth of
\(n\)
\(d_{rc}\)
measures, the nonlinear transformation average maximum number of each time step. We then unfold the directed acyclic graph of RNN and denote the processed graph as
\(g_{c}\)
, meanwhile, denote
\(C(g_{c} )\)
as the set of directed cycles in this graph. For the vertex
\(v\)
, we note
\(\sigma_{s} (v)\)
as the sum of edge weights and
\(l(v)\)
as the length. The equation below is proved under a mild assumption, which could be found in [
46
].
$$d_{rc} = \max_{{v \in C(g_{c} )}} \frac{l(v)}{{\sigma_{s} (v)}}$$
They also found that another crucial factor that impacts the performance of LSTM, which is the recurrent skip coefficients. We note
\(s_{rc}\)
as the reciprocal of the recurrent skip coefficient. Please be aware that
\(s_{rc}\)
is also a positive rational number.
$$s_{rc} = \min_{{v \in C(g_{c} )}} \frac{{\sigma_{s} (v)}}{l(v)}$$
According to the above definition, our proposed model is a 2-layers stacked LSTM, which
\(d_{rc} = 2\)
and
\(s_{rc} = 1\)
. From the experiments performed in previous work, the authors also found that when facing the problems of long-term dependency, LSTMs may benefit from decreasing the reciprocal of recurrent skip coefficients and from increasing recurrent depth. The empirical findings above mentioned are useful to enhance the performance of our proposed model further.
Conclusion
This work consists of three parts: data extraction and pre-processing of the Chinese stock market dataset, carrying out feature engineering, and stock price trend prediction model based on the long short-term memory (LSTM). We collected, cleaned-up, and structured 2 years of Chinese stock market data. We reviewed different techniques often used by real-world investors, developed a new algorithm component, and named it as feature extension, which is proved to be effective. We applied the feature expansion (FE) approaches with recursive feature elimination (RFE), followed by principal component analysis (PCA), to build a feature engineering procedure that is both effective and efficient. The system is customized by assembling the feature engineering procedure with an LSTM prediction model, achieved high prediction accuracy that outperforms the leading models in most related works. We also carried out a comprehensive evaluation of this work. By comparing the most frequently used machine learning models with our proposed LSTM model under the feature engineering part of our proposed system, we conclude many heuristic findings that could be future research questions in both technical and financial research domains.
Our proposed solution is a unique customization as compared to the previous works because rather than just proposing yet another state-of-the-art LSTM model, we proposed a fine-tuned and customized deep learning prediction system along with utilization of comprehensive feature engineering and combined it with LSTM to perform prediction. By researching into the observations from previous works, we fill in the gaps between investors and researchers by proposing a feature extension algorithm before recursive feature elimination and get a noticeable improvement in the model performance.
Though we have achieved a decent outcome from our proposed solution, this research has more potential towards research in future. During the evaluation procedure, we also found that the RFE algorithm is not sensitive to the term lengths other than 2-day, weekly, biweekly. Getting more in-depth research into what technical indices would influence the irregular term lengths would be a possible future research direction. Moreover, by combining latest sentiment analysis techniques with feature engineering and deep learning model, there is also a high potential to develop a more comprehensive prediction system which is trained by diverse types of information such as tweets, news, and other text-based data.
Abbreviations
LSTM:
Long short term memory
PCA:
Principal component analysis
RNN:
Recurrent neural networks
ANN:
Artificial neural network
DNN:
Deep neural network
DTW:
Dynamic Time Warping
RFE:
Recursive feature elimination
SVM:
Support vector machine
CNN:
Convolutional neural network
SGD:
Stochastic gradient descent
ReLU:
Rectified linear unit
MLP:
Multi layer perceptron
References
Atsalakis GS, Valavanis KP. Forecasting stock market short-term trends using a neuro-fuzzy based methodology. Expert Syst Appl. 2009;36(7):10696–707.
Article
Google Scholar
Ayo CK. Stock price prediction using the ARIMA model. In: 2014 UKSim-AMSS 16th international conference on computer modelling and simulation. 2014.
https://doi.org/10.1109/UKSim.2014.67
.
Brownlee J. Deep learning for time series forecasting: predict the future with MLPs, CNNs and LSTMs in Python. Machine Learning Mastery. 2018.
https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/
Eapen J, Bein D, Verma A. Novel deep learning model with CNN and bi-directional LSTM for improved stock market index prediction. In: 2019 IEEE 9th annual computing and communication workshop and conference (CCWC). 2019. pp. 264–70.
https://doi.org/10.1109/CCWC.2019.8666592
.
Fischer T, Krauss C. Deep learning with long short-term memory networks for financial market predictions. Eur J Oper Res. 2018;270(2):654–69.
https://doi.org/10.1016/j.ejor.2017.11.054
.
Article
MathSciNet
MATH
Google Scholar
Guyon I, Weston J, Barnhill S, Vapnik V. Gene selection for cancer classification using support vector machines. Mach Learn 2002;46:389–422.
Article
Google Scholar
Hafezi R, Shahrabi J, Hadavandi E. A bat-neural network multi-agent system (BNNMAS) for stock price prediction: case study of DAX stock price. Appl Soft Comput J. 2015;29:196–210.
https://doi.org/10.1016/j.asoc.2014.12.028
.
Article
Google Scholar
Halko N, Martinsson PG, Tropp JA. Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev. 2001;53(2):217–88.
Article
MathSciNet
Google Scholar
Hassan MR, Nath B. Stock market forecasting using Hidden Markov Model: a new approach. In: Proceedings—5th international conference on intelligent systems design and applications 2005, ISDA’05. 2005. pp. 192–6.
https://doi.org/10.1109/ISDA.2005.85
.
Hochreiter S, Schmidhuber J. Long short-term memory. J Neural Comput. 1997;9(8):1735–80.
https://doi.org/10.1162/neco.1997.9.8.1735
.
Article
Google Scholar
Hsu CM. A hybrid procedure with feature selection for resolving stock/futures price forecasting problems. Neural Comput Appl. 2013;22(3–4):651–71.
https://doi.org/10.1007/s00521-011-0721-4
.
Article
Google Scholar
Huang CF, Chang BR, Cheng DW, Chang CH. Feature selection and parameter optimization of a fuzzy-based stock selection model using genetic algorithms. Int J Fuzzy Syst. 2012;14(1):65–75.
https://doi.org/10.1016/J.POLYMER.2016.08.021
.
Article
MathSciNet
Google Scholar
Huang CL, Tsai CY. A hybrid SOFM-SVR with a filter-based feature selection for stock market forecasting. Expert Syst Appl. 2009;36(2 PART 1):1529–39.
https://doi.org/10.1016/j.eswa.2007.11.062
.
Article
Google Scholar
Idrees SM, Alam MA, Agarwal P. A prediction approach for stock market volatility based on time series data. IEEE Access. 2019;7:17287–98.
https://doi.org/10.1109/ACCESS.2019.2895252
.
Article
Google Scholar
Ince H, Trafalis TB. Short term forecasting with support vector machines and application to stock price prediction. Int J Gen Syst. 2008;37:677–87.
https://doi.org/10.1080/03081070601068595
.
Article
MathSciNet
MATH
Google Scholar
Jeon S, Hong B, Chang V. Pattern graph tracking-based stock price prediction using big data. Future Gener Comput Syst. 2018;80:171–87.
https://doi.org/10.1016/j.future.2017.02.010
.
Article
Google Scholar
Kara Y, Acar Boyacioglu M, Baykan ÖK. Predicting direction of stock price index movement using artificial neural networks and support vector machines: the sample of the Istanbul Stock Exchange. Expert Syst Appl. 2011;38(5):5311–9.
https://doi.org/10.1016/j.eswa.2010.10.027
.
Article
Google Scholar
Khaidem L, Dey SR. Predicting the direction of stock market prices using random forest. 2016. pp. 1–20.
Kim K, Han I. Genetic algorithms approach to feature discretization in artificial neural networks for the prediction of stock price index. Expert Syst Appl. 2000;19:125–32.
https://doi.org/10.1016/S0957-4174(00)00027-0
.
Article
Google Scholar
Lee MC. Using support vector machine with a hybrid feature selection method to the stock trend prediction. Expert Syst Appl. 2009;36(8):10896–904.
https://doi.org/10.1016/j.eswa.2009.02.038
.
Article
Google Scholar
Lei L. Wavelet neural network prediction method of stock price trend based on rough set attribute reduction. Appl Soft Comput J. 2018;62:923–32.
https://doi.org/10.1016/j.asoc.2017.09.029
.
Article
Google Scholar
Lin X, Yang Z, Song Y. Expert systems with applications short-term stock price prediction based on echo state networks. Expert Syst Appl. 2009;36(3):7313–7.
https://doi.org/10.1016/j.eswa.2008.09.049
.
Article
Google Scholar
Liu G, Wang X. A new metric for individual stock trend prediction. Eng Appl Artif Intell. 2019;82(March):1–12.
https://doi.org/10.1016/j.engappai.2019.03.019
.
Article
Google Scholar
Liu S, Zhang C, Ma J. CNN-LSTM neural network model for quantitative strategy analysis in stock markets. 2017;1:198–206.
https://doi.org/10.1007/978-3-319-70096-0
.
Long W, Lu Z, Cui L. Deep learning-based feature engineering for stock price movement prediction. Knowl Based Syst. 2018;164:163–73.
https://doi.org/10.1016/j.knosys.2018.10.034
.
Article
Google Scholar
Malkiel BG, Fama EF. Efficient capital markets: a review of theory and empirical work. J Finance. 1970;25(2):383–417.
Article
Google Scholar
McNally S, Roche J, Caton S. Predicting the price of bitcoin using machine learning. In: Proceedings—26th Euromicro international conference on parallel, distributed, and network-based processing, PDP 2018. pp. 339–43.
https://doi.org/10.1109/PDP2018.2018.00060
.
Nagar A, Hahsler M. News sentiment analysis using R to predict stock market trends. 2012.
http://past.rinfinance.com/agenda/2012/talk/Nagar+Hahsler.pdf
. Accessed 20 July 2019.
Nekoeiqachkanloo H, Ghojogh B, Pasand AS, Crowley M. Artificial counselor system for stock investment. 2019. ArXiv Preprint
arXiv:1903.00955
.
Ni LP, Ni ZW, Gao YZ. Stock trend prediction based on fractal feature selection and support vector machine. Expert Syst Appl. 2011;38(5):5569–76.
https://doi.org/10.1016/j.eswa.2010.10.079
.
Article
Google Scholar
Pang X, Zhou Y, Wang P, Lin W, Chang V. An innovative neural network approach for stock market prediction. J Supercomput. 2018.
https://doi.org/10.1007/s11227-017-2228-y
.
Article
Google Scholar
Pimenta A, Nametala CAL, Guimarães FG, Carrano EG. An automated investing method for stock market based on multiobjective genetic programming. Comput Econ. 2018;52(1):125–44.
https://doi.org/10.1007/s10614-017-9665-9
.
Article
Google Scholar
Piramuthu S. Evaluating feature selection methods for learning in data mining applications. Eur J Oper Res. 2004;156(2):483–94.
https://doi.org/10.1016/S0377-2217(02)00911-6
.
Article
MathSciNet
MATH
Google Scholar
Qiu M, Song Y. Predicting the direction of stock market index movement using an optimized artificial neural network model. PLoS ONE. 2016;11(5):e0155133.
Article
Google Scholar
Scikit-learn. Scikit-learn Min-Max Scaler. 2019.
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html
. Retrieved 26 July 2020.
Shen J. Thesis, “Short-term stock market price trend prediction using a customized deep learning system”, supervised by M. Omair Shafiq, Carleton University. 2019.
Shen J, Shafiq MO. Deep learning convolutional neural networks with dropout—a parallel approach. ICMLA. 2018;2018:572–7.
Google Scholar
Shen J, Shafiq MO. Learning mobile application usage—a deep learning approach. ICMLA. 2019;2019:287–92.
Google Scholar
Shih D. A study of early warning system in volume burst risk assessment of stock with Big Data platform. In: 2019 IEEE 4th international conference on cloud computing and big data analysis (ICCCBDA). 2019. pp. 244–8.
Sirignano J, Cont R. Universal features of price formation in financial markets: perspectives from deep learning. Ssrn. 2018.
https://doi.org/10.2139/ssrn.3141294
.
Article
MATH
Google Scholar
Thakur M, Kumar D. A hybrid financial trading support system using multi-category classifiers and random forest. Appl Soft Comput J. 2018;67:337–49.
https://doi.org/10.1016/j.asoc.2018.03.006
.
Article
Google Scholar
Tsai CF, Hsiao YC. Combining multiple feature selection methods for stock prediction: union, intersection, and multi-intersection approaches. Decis Support Syst. 2010;50(1):258–69.
https://doi.org/10.1016/j.dss.2010.08.028
.
Article
Google Scholar
Tushare API. 2018.
https://github.com/waditu/tushare
. Accessed 1 July 2019.
Wang X, Lin W. Stock market prediction using neural networks: does trading volume help in short-term prediction?. n.d.
Weng B, Lu L, Wang X, Megahed FM, Martinez W. Predicting short-term stock prices using ensemble methods and online data sources. Expert Syst Appl. 2018;112:258–73.
https://doi.org/10.1016/j.eswa.2018.06.016
.
Article
Google Scholar
Zhang S. Architectural complexity measures of recurrent neural networks, (NIPS). 2016. pp. 1–9.
Zubair M, Fazal A, Fazal R, Kundi M. Development of stock market trend prediction system using multiple regression. Computational and mathematical organization theory. Berlin: Springer US; 2019.
https://doi.org/10.1007/s10588-019-09292-7
.
Book
Google Scholar
Download references
Acknowledgements
This research is supported by Carleton University, in Ottawa, ON, Canada. This research paper has been built based on the thesis [
36
] of Jingyi Shen, supervised by M. Omair Shafiq at Carleton University, Canada, available at
https://curve.carleton.ca/52e9187a-7f71-48ce-bdfe-e3f6a420e31a
.
Funding
NSERC and Carleton University.
Author information
Authors and Affiliations
School of Information Technology, Carleton University, Ottawa, ON, Canada
Jingyi Shen & M. Omair Shafiq
Authors
Jingyi Shen
View author publications
You can also search for this author in
PubMed
Google Scholar
M. Omair Shafiq
View author publications
You can also search for this author in
PubMed
Google Scholar
Contributions
Yes. All authors read and approved the final manuscript.
Corresponding author
Correspondence to
M. Omair Shafiq
.
Ethics declarations
Competing interests
The authors declare that they have no competing interests.
Additional information
Publisher's Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Rights and permissions
Open Access
This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit
http://creativecommons.org/licenses/by/4.0/
.
Reprints and permissions
About this article
Cite this article
Shen, J., Shafiq, M.O. Short-term stock market price trend prediction using a comprehensive deep learning system.
J Big Data
7
, 66 (2020). https://doi.org/10.1186/s40537-020-00333-6
Download citation
Received
:
24 January 2020
Accepted
:
30 July 2020
Published
:
28 August 2020
DOI
:
https://doi.org/10.1186/s40537-020-00333-6
Share this article
Anyone you share the following link with will be able to read this content:
Get shareable link
Sorry, a shareable link is not currently available for this article.
Copy to clipboard
Provided by the Springer Nature SharedIt content-sharing initiative
Keywords
Prediction
Deep learning
Stock market trend
Feature engineering
Download PDF
Download ePub
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles/most-accessed/rss.xml):
Most Accessed Articles: Journal of Big Data
https://journalofbigdata.springeropen.com
Most Accessed Articles: Journal of Big Data
A survey on Image Data Augmentation for Deep Learning
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0
Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomen...
Survey paper
Sat, 06 Jul 2019 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0
Connor Shorten and Taghi M. Khoshgoftaar
2019-07-06T00:00:00Z
Big data in healthcare: management, analysis and future prospects
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0
‘Big data’ is massive amounts of information that can work wonders. It has become a topic of special interest for the past two decades because of a great potential that is hidden in it. Various public and priv...
Survey paper
Wed, 19 Jun 2019 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0217-0
Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma and Sandeep Kaushik
2019-06-19T00:00:00Z
Review of deep learning: concepts, CNN architectures, challenges, applications, future directions
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8
In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational a...
Survey Paper
Wed, 31 Mar 2021 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8
Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan
2021-03-31T00:00:00Z
Deep learning applications and challenges in big data analytics
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-014-0007-7
Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific info...
Research
Tue, 24 Feb 2015 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-014-0007-7
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald and Edin Muharemagic
2015-02-24T00:00:00Z
Short-term stock market price trend prediction using a comprehensive deep learning system
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00333-6
In the era of big data, deep learning for predicting stock market prices and trends has become even more popular than before. We collected 2 years of data from Chinese stock market and proposed a comprehensive...
Research
Fri, 28 Aug 2020 00:00:00 GMT
https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00333-6
Jingyi Shen and M. Omair Shafiq
2020-08-28T00:00:00Z
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/articles):
Articles | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Articles
Collections
Articles
Search by keyword
Search by citation
for results from
All volumes
Volume 11 (2024)
Volume 10 (2023)
Volume 9 (2022)
Volume 8 (2021)
Volume 7 (2020)
Volume 6 (2019)
Volume 5 (2018)
Volume 4 (2017)
Volume 3 (2016)
Volume 2 (2015)
Volume 1 (2014)
Search
Show results from
Select a volume
Volume 11 (2024)
Volume 10 (2023)
Volume 9 (2022)
Volume 8 (2021)
Volume 7 (2020)
Volume 6 (2019)
Volume 5 (2018)
Volume 4 (2017)
Volume 3 (2016)
Volume 2 (2015)
Volume 1 (2014)
Search
Page 1 of 20
Sort by
Relevance
Newest first
Oldest first
Submit
Optimizing poultry audio signal classification with deep learning and burn layer fusion
This study introduces a novel deep learning-based approach for classifying poultry audio signals, incorporating a custom Burn Layer to enhance model robustness. The methodology integrates digital audio signal ...
Authors:
Esraa Hassan, Samar Elbedwehy, Mahmoud Y. Shams, Tarek Abd El-Hafeez and Nora El-Rashidy
Citation:
Journal of Big Data
2024
11
:135
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Machine learning and deep learning models based grid search cross validation for short-term solar irradiance forecasting
In late 2023, the United Nations conference on climate change (COP28), which was held in Dubai, encouraged a quick move from fossil fuels to renewable energy. Solar energy is one of the most promising forms of...
Authors:
Doaa El-Shahat, Ahmed Tolba, Mohamed Abouhawwash and Mohamed Abdel-Basset
Citation:
Journal of Big Data
2024
11
:134
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Shielding networks: enhancing intrusion detection with hybrid feature selection and stack ensemble learning
The frequent usage of computer networks and the Internet has made computer networks vulnerable to numerous attacks, highlighting the critical need to enhance the precision of security mechanisms. One of the mo...
Authors:
Ali Mohammed Alsaffar, Mostafa Nouri-Baygi and Hamed M. Zolbanin
Citation:
Journal of Big Data
2024
11
:133
Content type:
Research
Published on:
18 September 2024
View
Full Text
View
PDF
Integrating microarray-based spatial transcriptomics and RNA-seq reveals tissue architecture in colorectal cancer
The tumor microenvironment (TME) provides a region for intricate interactions within or between immune and non-immune cells. We aimed to reveal the tissue architecture and comprehensive landscape of cells with...
Authors:
Zheng Li, Xiaojie Zhang, Chongyuan Sun, Zefeng Li, He Fei and Dongbing Zhao
Citation:
Journal of Big Data
2024
11
:132
Content type:
Research
Published on:
17 September 2024
View
Full Text
View
PDF
Development and evaluation of a deep learning model for automatic segmentation of non-perfusion area in fundus fluorescein angiography
Diabetic retinopathy (DR) is the most prevalent cause of preventable vision loss worldwide, imposing a significant economic and medical burden on society today, of which early identification is the cornerstone...
Authors:
Wei Feng, Bingjie Wang, Dan Song, Mengda Li, Anming Chen, Jing Wang, Siyong Lin, Yiran Zhao, Bin Wang, Zongyuan Ge, Shuyi Xu and Yuntao Hu
Citation:
Journal of Big Data
2024
11
:131
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Evolutionary computation-based self-supervised learning for image processing: a big data-driven approach to feature extraction and fusion for multispectral object detection
The image object recognition and detection technology are widely used in many scenarios. In recent years, big data has become increasingly abundant, and big data-driven artificial intelligence models have attr...
Authors:
Xiaoyang Shen, Haibin Li, Achyut Shankar, Wattana Viriyasitavat and Vinay Chamola
Citation:
Journal of Big Data
2024
11
:130
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Leveraging large-scale genetic data to assess the causal impact of COVID-19 on multisystemic diseases
The long-term impacts of COVID-19 on human health are a major concern, yet comprehensive evaluations of its effects on various health conditions are lacking.
Authors:
Xiangyang Zhang, Zhaohui Jiang, Jiayao Ma, Yaru Qi, Yin Li, Yan Zhang, Yihan Liu, Chaochao Wei, Yihong Chen, Ping Liu, Yinghui Peng, Jun Tan, Ying Han, Shan Zeng, Changjing Cai and Hong Shen
Citation:
Journal of Big Data
2024
11
:129
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
A model for investment type recommender system based on the potential investors based on investors and experts feedback using ANFIS and MNN
This article presents an investment recommender system based on an Adaptive Neuro-Fuzzy Inference System (ANFIS) and pre-trained weights from a Multimodal Neural Network (MNN). The model is designed to support...
Authors:
Asefeh Asemi, Adeleh Asemi and Andrea Ko
Citation:
Journal of Big Data
2024
11
:128
Content type:
Research
Published on:
12 September 2024
View
Full Text
View
PDF
Inhibitory neuron links the causal relationship from air pollution to psychiatric disorders: a large multi-omics analysis
Psychiatric disorders are severe health challenges that exert a heavy public burden. Air pollution has been widely reported as related to psychiatric disorder risk, but their casual association and pathologica...
Authors:
Xisong Liang, Jie Wen, Chunrun Qu, Nan Zhang, Ziyu Dai, Hao Zhang, Peng Luo, Ming Meng, Zhixiong Liu, Fan Fan and Quan Cheng
Citation:
Journal of Big Data
2024
11
:127
Content type:
Research
Published on:
11 September 2024
View
Full Text
View
PDF
Enhancing oil palm segmentation model with GAN-based augmentation
In digital agriculture, accurate crop detection is fundamental to developing automated systems for efficient plantation management. For oil palm, the main challenge lies in developing robust models that perfor...
Authors:
Qi Bin Kwong, Yee Thung Kon, Wan Rusydiah W. Rusik, Mohd Nor Azizi Shabudin, Shahirah Shazana A. Rahman, Harikrishna Kulaveerasingam and David Ross Appleton
Citation:
Journal of Big Data
2024
11
:126
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
AI sees beyond humans: automated diagnosis of myopia based on peripheral refraction map using interpretable deep learning
The question of whether artificial intelligence (AI) can surpass human capabilities is crucial in the application of AI in clinical medicine. To explore this, an interpretable deep learning (DL) model was deve...
Authors:
Yong Tang, Zhenghua Lin, Linjing Zhou, Weijia Wang, Longbo Wen, Yongli Zhou, Zongyuan Ge, Zhao Chen, Weiwei Dai, Zhikuan Yang, He Tang and Weizhong Lan
Citation:
Journal of Big Data
2024
11
:125
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
Modeling the impact of BDA-AI on sustainable innovation ambidexterity and environmental performance
Data has evolved into one of the principal resources for contemporary businesses. Moreover, corporations have undergone digitalization; consequently, their supply chains generate substantial amounts of data. T...
Authors:
Chin-Tsu Chen, Asif Khan and Shih-Chih Chen
Citation:
Journal of Big Data
2024
11
:124
Content type:
Research
Published on:
8 September 2024
View
Full Text
View
PDF
Efficient microservices offloading for cost optimization in diverse MEC cloud networks
In recent years, mobile applications have proliferated across domains such as E-banking, Augmented Reality, E-Transportation, and E-Healthcare. These applications are often built using microservices, an archit...
Authors:
Abdul Rasheed Mahesar, Xiaoping Li and Dileep Kumar Sajnani
Citation:
Journal of Big Data
2024
11
:123
Content type:
Research
Published on:
4 September 2024
View
Full Text
View
PDF
Predicting startup success using two bias-free machine learning: resolving data imbalance using generative adversarial networks
The success of newly established companies holds significant implications for community development and economic growth. However, startups often grapple with heightened vulnerability to market volatility, whic...
Authors:
Jungryeol Park, Saesol Choi and Yituo Feng
Citation:
Journal of Big Data
2024
11
:122
Content type:
Research
Published on:
3 September 2024
View
Full Text
View
PDF
CTGAN-ENN: a tabular GAN-based hybrid sampling method for imbalanced and overlapped data in customer churn prediction
Class imbalance is one of many problems of customer churn datasets. One of the common problems is class overlap, where the data have a similar instance between classes. The prediction task of customer churn be...
Authors:
I Nyoman Mahayasa Adiputra and Paweena Wanchai
Citation:
Journal of Big Data
2024
11
:121
Content type:
Research
Published on:
2 September 2024
View
Full Text
View
PDF
Cartographies of warfare in the Indian subcontinent: Contextualizing archaeological and historical analysis through big data approaches
Some of the most notable human behavioral palimpsests result from warfare and its durable traces in the form of defensive architecture and strategic infrastructure. For premodern periods, this architecture is ...
Authors:
Monica L. Smith and Connor Newton
Citation:
Journal of Big Data
2024
11
:120
Content type:
Case Study
Published on:
29 August 2024
View
Full Text
View
PDF
Automated subway touch button detection using image process
Subway button detection is paramount for passenger safety, yet the occurrence of inadvertent touches poses operational threats. Camera-based detection is indispensable for identifying touch occurrences, ascert...
Authors:
Junfeng An, Mengmeng Lu, Gang Li, Jiqiang Liu and Chongqing Wang
Citation:
Journal of Big Data
2024
11
:119
Content type:
Research
Published on:
29 August 2024
View
Full Text
View
PDF
Cybersecurity vulnerabilities and solutions in Ethiopian university websites
This study investigates the causes and countermeasures of cybercrime vulnerabilities, specifically focusing on selected 16 Ethiopian university websites. This study uses a cybersecurity awareness survey, and a...
Authors:
Ali Yimam Eshetu, Endris Abdu Mohammed and Ayodeji Olalekan Salau
Citation:
Journal of Big Data
2024
11
:118
Content type:
Research
Published on:
23 August 2024
View
Full Text
View
PDF
Crude oil price forecasting using K-means clustering and LSTM model enhanced by dense-sparse-dense strategy
Crude oil is an essential energy source that affects international trade, transportation, and manufacturing, highlighting its importance to the economy. Its future price prediction affects consumer prices and ...
Authors:
Alireza Jahandoost, Farhad Abedinzadeh Torghabeh, Seyyed Abed Hosseini and Mahboobeh Houshmand
Citation:
Journal of Big Data
2024
11
:117
Content type:
Research
Published on:
17 August 2024
View
Full Text
View
PDF
Rs-net: Residual Sharp U-Net architecture for pavement crack segmentation and severity assessment
U-net, a fully convolutional network-based image segmentation method, has demonstrated widespread adaptability in the crack segmentation task. The combination of the semantically dissimilar features of the enc...
Authors:
Luqman Ali, Hamad AlJassmi, Mohammed Swavaf, Wasif Khan and Fady Alnajjar
Citation:
Journal of Big Data
2024
11
:116
Content type:
Research
Published on:
17 August 2024
View
Full Text
View
PDF
Internet of things and ensemble learning-based mental and physical fatigue monitoring for smart construction sites
The construction industry substantially contributes to the economic growth of a country. However, it records a large number of workplace injuries and fatalities annually due to its hesitant adoption of automat...
Authors:
Bubryur Kim, K. R. Sri Preethaa, Sujeen Song, R. R. Lukacs, Jinwoo An, Zengshun Chen, Euijung An and Sungho Kim
Citation:
Journal of Big Data
2024
11
:115
Content type:
Research
Published on:
16 August 2024
View
Full Text
View
PDF
Toward a globally lunar calendar: a machine learning-driven approach for crescent moon visibility prediction
This paper presents a comprehensive approach to harmonizing lunar calendars across different global regions, addressing the long-standing challenge of variations in new crescent Moon sightings that mark the be...
Authors:
Samia Loucif, Murad Al-Rajab, Raed Abu Zitar and Mahmoud Rezk
Citation:
Journal of Big Data
2024
11
:114
Content type:
Research
Published on:
12 August 2024
View
Full Text
View
PDF
Enhancing K-nearest neighbor algorithm: a comprehensive review and performance analysis of modifications
The k-Nearest Neighbors (kNN) method, established in 1951, has since evolved into a pivotal tool in data mining, recommendation systems, and Internet of Things (IoT), among other areas. This paper presents a c...
Authors:
Rajib Kumar Halder, Mohammed Nasir Uddin, Md. Ashraf Uddin, Sunil Aryal and Ansam Khraisat
Citation:
Journal of Big Data
2024
11
:113
Content type:
Survey
Published on:
11 August 2024
View
Full Text
View
PDF
Deep SqueezeNet learning model for diagnosis and prediction of maize leaf diseases
The maize leaf diseases create severe yield reductions and critical problems. The maize leaf disease should be discovered early, perfectly identified, and precisely diagnosed to make greater yield. This work s...
Authors:
Prasannavenkatesan Theerthagiri, A. Usha Ruby, J. George Chellin Chandran, Tanvir Habib Sardar and Ahamed Shafeeq B. M.
Citation:
Journal of Big Data
2024
11
:112
Content type:
Research
Published on:
10 August 2024
View
Full Text
View
PDF
An aspect sentiment analysis model with Aspect Gated Convolution and Dual-Feature Filtering layers
Aspect level sentiment analysis is a basic task to determine the sentiment bias based on the contextual information near the aspect words. Some sentences contain many confusing feature words due to incomplete ...
Authors:
Hongfang Gong and Siyu Zhang
Citation:
Journal of Big Data
2024
11
:111
Content type:
Methodology
Published on:
9 August 2024
View
Full Text
View
PDF
Context-aware prediction of active and passive user engagement: Evidence from a large online social platform
The success of online social platforms hinges on their ability to predict and understand user behavior at scale. Here, we present data suggesting that context-aware modeling approaches may offer a holistic yet...
Authors:
Heinrich Peters, Yozen Liu, Francesco Barbieri, Raiyan Abdul Baten, Sandra C. Matz and Maarten W. Bos
Citation:
Journal of Big Data
2024
11
:110
Content type:
Research
Published on:
8 August 2024
View
Full Text
View
PDF
Analysis of Graeco-Latin square designs in the presence of uncertain data
This paper addresses the Graeco-Latin square design (GLSD) under neutrosophic statistics. In this work, we propose a novel approach for analyzing Graeco-Latin square designs using uncertain observations.
Authors:
Abdulrahman AlAita, Muhammad Aslam, Khaled Al Sultan and Muhammad Saleem
Citation:
Journal of Big Data
2024
11
:109
Content type:
Research
Published on:
7 August 2024
View
Full Text
View
PDF
Memetic multilabel feature selection using pruned refinement process
With the growing complexity of data structures, which include high-dimensional and multilabel datasets, the significance of feature selection has become more emphasized. Multilabel feature selection endeavors ...
Authors:
Wangduk Seo, Jaegyun Park, Sanghyuck Lee, A-Seong Moon, Dae-Won Kim and Jaesung Lee
Citation:
Journal of Big Data
2024
11
:108
Content type:
Research
Published on:
6 August 2024
View
Full Text
View
PDF
Sentiment-based predictive models for online purchases in the era of marketing 5.0: a systematic review
The convergence of artificial intelligence (AI), big data (DB), and Internet of Things (IoT) in Society 5.0, has given rise to Marketing 5.0, revolutionizing personalized customer experiences. In this study, a...
Authors:
Veerajay Gooljar, Tomayess Issa, Sarita Hardin-Ramanan and Bilal Abu-Salih
Citation:
Journal of Big Data
2024
11
:107
Content type:
Survey
Published on:
5 August 2024
View
Full Text
View
PDF
Unlocking the potential of Naive Bayes for spatio temporal classification: a novel approach to feature expansion
Prediction processes in areas ranging from climate and disease spread to disasters and air pollution rely heavily on spatial–temporal data. Understanding and forecasting the distribution patterns of disease ca...
Authors:
Sri Suryani Prasetiyowati and Yuliant Sibaroni
Citation:
Journal of Big Data
2024
11
:106
Content type:
Research
Published on:
5 August 2024
View
Full Text
View
PDF
Advancing cybersecurity: a comprehensive review of AI-driven detection techniques
As the number and cleverness of cyber-attacks keep increasing rapidly, it's more important than ever to have good ways to detect and prevent them. Recognizing cyber threats quickly and accurately is crucial be...
Authors:
Aya H. Salem, Safaa M. Azzam, O. E. Emam and Amr A. Abohany
Citation:
Journal of Big Data
2024
11
:105
Content type:
Survey
Published on:
4 August 2024
View
Full Text
View
PDF
Interpolation-split: a data-centric deep learning approach with big interpolated data to boost airway segmentation performance
The morphology and distribution of airway tree abnormalities enable diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critic...
Authors:
Wing Keung Cheung, Ashkan Pakzad, Nesrin Mogulkoc, Sarah Helen Needleman, Bojidar Rangelov, Eyjolfur Gudmundsson, An Zhao, Mariam Abbas, Davina McLaverty, Dimitrios Asimakopoulos, Robert Chapman, Recep Savas, Sam M. Janes, Yipeng Hu, Daniel C. Alexander, John R. Hurst…
Citation:
Journal of Big Data
2024
11
:104
Content type:
Research
Published on:
4 August 2024
View
Full Text
View
PDF
DiabSense: early diagnosis of non-insulin-dependent diabetes mellitus using smartphone-based human activity recognition and diabetic retinopathy analysis with Graph Neural Network
Non-Insulin-Dependent Diabetes Mellitus (NIDDM) is a chronic health condition caused by high blood sugar levels, and if not treated early, it can lead to serious complications i.e. blindness. Human Activity Re...
Authors:
Md Nuho Ul Alam, Ibrahim Hasnine, Erfanul Hoque Bahadur, Abdul Kadar Muhammad Masum, Mercedes Briones Urbano, Manuel Masias Vergara, Jia Uddin, Imran Ashraf and Md. Abdus Samad
Citation:
Journal of Big Data
2024
11
:103
Content type:
Research
Published on:
3 August 2024
View
Full Text
View
PDF
An adaptive composite time series forecasting model for short-term traffic flow
Short-term traffic flow forecasting is a hot issue in the field of intelligent transportation. The research field of traffic forecasting has evolved greatly in past decades. With the rapid development of deep ...
Authors:
Qitan Shao, Xinglin Piao, Xiangyu Yao, Yuqiu Kong, Yongli Hu, Baocai Yin and Yong Zhang
Citation:
Journal of Big Data
2024
11
:102
Content type:
Methodology
Published on:
3 August 2024
View
Full Text
View
PDF
Fitcam: detecting and counting repetitive exercises with deep learning
Physical fitness is one of the most important traits a person could have for health longevity. Conducting regular exercise is fundamental to maintaining physical fitness, but with the caveat of occurring injur...
Authors:
Ferdinandz Japhne, Kevin Janada, Agustinus Theodorus and Andry Chowanda
Citation:
Journal of Big Data
2024
11
:101
Content type:
Research
Published on:
3 August 2024
View
Full Text
View
PDF
Tc-llama 2: fine-tuning LLM for technology and commercialization applications
This paper introduces TC-Llama 2, a novel application of large language models (LLMs) in the technology-commercialization field. Traditional methods in this field, reliant on statistical learning and expert kn...
Authors:
Jeyoon Yeom, Hakyung Lee, Hoyoon Byun, Yewon Kim, Jeongeun Byun, Yunjeong Choi, Sungjin Kim and Kyungwoo Song
Citation:
Journal of Big Data
2024
11
:100
Content type:
Research
Published on:
2 August 2024
View
Full Text
View
PDF
An ensemble machine learning model for predicting one-year mortality in elderly coronary heart disease patients with anemia
This study was designed to develop and validate a robust predictive model for one-year mortality in elderly coronary heart disease (CHD) patients with anemia using machine learning methods.
Authors:
Longcan Cheng, Yan Nie, Hongxia Wen, Yan Li, Yali Zhao, Qian Zhang, Mingxing Lei and Shihui Fu
Citation:
Journal of Big Data
2024
11
:99
Content type:
Research
Published on:
24 July 2024
View
Full Text
View
PDF
Predictive modelling of MapReduce job performance in cloud environments using machine learning techniques
Within the Hadoop ecosystem, MapReduce stands as a cornerstone for managing, processing, and mining large-scale datasets. Yet, the absence of efficient solutions for precise estimation of job execution times p...
Authors:
Mohammed Bergui, Soufiane Hourri, Said Najah and Nikola S. Nikolov
Citation:
Journal of Big Data
2024
11
:98
Content type:
Research
Published on:
23 July 2024
View
Full Text
View
PDF
Hate speech detection in the Bengali language: a comprehensive survey
The detection of hate speech (HS) in online platforms has become extremely important for maintaining a safe and inclusive environment. While significant progress has been made in English-language HS detection,...
Authors:
Abdullah Al Maruf, Ahmad Jainul Abidin, Md. Mahmudul Haque, Zakaria Masud Jiyad, Aditi Golder, Raaid Alubady and Zeyar Aung
Citation:
Journal of Big Data
2024
11
:97
Content type:
Survey
Published on:
23 July 2024
View
Full Text
View
PDF
Introducing Mplots: scaling time series recurrence plots to massive datasets
Time series similarity matrices (informally, recurrence plots or dot-plots), are useful tools for time series data mining. They can be used to guide data exploration, and various useful features can be derived...
Authors:
Maryam Shahcheraghi, Ryan Mercer, João Manuel de Almeida Rodrigues, Audrey Der, Hugo Filipe Silveira Gamboa, Zachary Zimmerman, Kerry Mauck and Eamonn Keogh
Citation:
Journal of Big Data
2024
11
:96
Content type:
Research
Published on:
20 July 2024
View
Full Text
View
PDF
Text summarization based on semantic graphs: an abstract meaning representation graph-to-text deep learning approach
Nowadays, due to the constantly growing amount of textual information, automatic text summarization constitutes an important research area in natural language processing. In this work, we present a novel frame...
Authors:
Panagiotis Kouris, Georgios Alexandridis and Andreas Stafylopatis
Citation:
Journal of Big Data
2024
11
:95
Content type:
Research
Published on:
14 July 2024
View
Full Text
View
PDF
Examining ALS: reformed PCA and random forest for effective detection of ALS
ALS (Amyotrophic Lateral Sclerosis) is a fatal neurodegenerative disease of the human motor system. It is a group of progressive diseases that affects the nerve cells in the brain and spinal cord that control ...
Authors:
Abdullah Alqahtani, Shtwai Alsubai, Mohemmed Sha and Ashit Kumar Dutta
Citation:
Journal of Big Data
2024
11
:94
Content type:
Research
Published on:
10 July 2024
View
Full Text
View
PDF
Emotion AWARE: an artificial intelligence framework for adaptable, robust, explainable, and multi-granular emotion analysis
Emotions are fundamental to human behaviour. How we feel, individually and collectively, determines how humanity evolves and advances into our shared future. The rapid digitalisation of our personal, social an...
Authors:
Gihan Gamage, Daswin De Silva, Nishan Mills, Damminda Alahakoon and Milos Manic
Citation:
Journal of Big Data
2024
11
:93
Content type:
Methodology
Published on:
10 July 2024
View
Full Text
View
PDF
Exploring AI-driven approaches for unstructured document analysis and future horizons
In the current industrial landscape, a significant number of sectors are grappling with the challenges posed by unstructured data, which incurs financial losses amounting to millions annually. If harnessed eff...
Authors:
Supriya V. Mahadevkar, Shruti Patil, Ketan Kotecha, Lim Way Soong and Tanupriya Choudhury
Citation:
Journal of Big Data
2024
11
:92
Content type:
Survey
Published on:
5 July 2024
View
Full Text
View
PDF
New custom rating for improving recommendation system performance
Recommendation system is currently attracting the interest of many explorers. Various new businesses have surfaced with the rise of online marketing (E-Commerce) in response to Covid-19 pandemic. This phenomen...
Authors:
Tora Fahrudin and Dedy Rahman Wijaya
Citation:
Journal of Big Data
2024
11
:91
Content type:
Research
Published on:
2 July 2024
View
Full Text
View
PDF
Optimization-based convolutional neural model for the classification of white blood cells
White blood cells (WBCs) are one of the most significant parts of the human immune system, and they play a crucial role in diagnosing the characteristics of pathologists and blood-related diseases. The charact...
Authors:
Tulasi Gayatri Devi and Nagamma Patil
Citation:
Journal of Big Data
2024
11
:90
Content type:
Research
Published on:
26 June 2024
View
Full Text
View
PDF
Advanced RIME architecture for global optimization and feature selection
The article introduces an innovative approach to global optimization and feature selection (FS) using the RIME algorithm, inspired by RIME-ice formation. The RIME algorithm employs a soft-RIME search strategy ...
Authors:
Ruba Abu Khurma, Malik Braik, Abdullah Alzaqebah, Krishna Gopal Dhal, Robertas Damaševičius and Bilal Abu-Salih
Citation:
Journal of Big Data
2024
11
:89
Content type:
Research
Published on:
18 June 2024
View
Full Text
View
PDF
Feature reduction for hepatocellular carcinoma prediction using machine learning algorithms
Hepatocellular carcinoma (HCC) is a highly prevalent form of liver cancer that necessitates accurate prediction models for early diagnosis and effective treatment. Machine learning algorithms have demonstrated...
Authors:
Ghada Mostafa, Hamdi Mahmoud, Tarek Abd El-Hafeez and Mohamed E. ElAraby
Citation:
Journal of Big Data
2024
11
:88
Content type:
Research
Published on:
18 June 2024
View
Full Text
View
PDF
Data oversampling and imbalanced datasets: an investigation of performance for machine learning and feature engineering
The classification of imbalanced datasets is a prominent task in text mining and machine learning. The number of samples in each class is not uniformly distributed; one class contains a large number of samples...
Authors:
Muhammad Mujahid, EROL Kına, Furqan Rustam, Monica Gracia Villar, Eduardo Silva Alvarado, Isabel De La Torre Diez and Imran Ashraf
Citation:
Journal of Big Data
2024
11
:87
Content type:
Research
Published on:
17 June 2024
View
Full Text
View
PDF
Advancing machine learning with OCR2SEQ: an innovative approach to multi-modal data augmentation
OCR2SEQ represents an innovative advancement in Optical Character Recognition (OCR) technology, leveraging a multi-modal generative augmentation strategy to overcome traditional limitations in OCR systems. Thi...
Authors:
Michael Lowe, Joseph D. Prusa, Joffrey L. Leevy and Taghi M. Khoshgoftaar
Citation:
Journal of Big Data
2024
11
:86
Content type:
Research
Published on:
13 June 2024
View
Full Text
View
PDF
Previous
page
1
2
3
4
5
…
20
Next
page
How was your experience today?
Rating
. A scale of 5 feelings conveyed using images that range from awful to great. The feelings represent how you feel about your experience today.
Please select one rating
An image of a cartoon face that is very unhappy.
The value of this radio input is:
Awful
An image of a cartoon face with a frown.
The value of this radio input is:
Bad
An image of a cartoon face with a neutral expression.
The value of this radio input is:
OK
An image of a cartoon face with a smile.
The value of this radio input is:
Good
An image of a cartoon face with an open mouth grin.
The value of this radio input is:
Great
Send feedback
Thank you for your feedback.
Tell us why (opens in a new tab)
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/top-ten-articles-2023):
Top 10 Downloaded Articles in 2023 | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
Top 10 Downloaded Articles in 2023
Top 10 Downloaded Articles in 2023
We want to thank the authors of our most downloaded articles in 2023 and highlight to our readers some of the most impactful research to have published in the journal. We hope that you will find the data interesting!
Passive buildings: a state-of-the-art review
Authors:
Vishwajit Anand, Vishnu Lakshmi Kadiri and Chandrasekhar Putcha
Meta-analysis on PET plastic as concrete aggregate using response surface methodology and regression analysis
Authors:
Beng Wei Chong and Xijun Shi
A deep reinforcement learning model for resilient road network recovery under earthquake or flooding hazards
Authors:
Xudong Fan, Xijin Zhang, Xiaowei Wang and Xiong Yu
Analysis and prediction of pipeline corrosion defects based on data analytics of in-line inspection
Authors:
Bingyan Cui and Hao Wang
Classification of failure modes of pipelines containing longitudinal surface cracks using mechanics-based and machine learning model
s
Authors:
Milad Salemi & Hao Wang
Haotian Sun, Wenxing Zhou
Modeling the rutting performance of asphalt pavements: a review
Authors:
Yong Deng, Xianming Shi
Emerging steel frames with Fe-SMA U-shaped dampers for enhancing seismic resilience
Authors:
Zhe-Xi Zhang, Jie Zhang, Cheng Fang, Yuelin Zhang and Yuanmu Li
Interdependence of social-ecological-technological systems in Phoenix, Arizona: consequences of an extreme precipitation event
Authors:
Alysha Helmrich, Amanda Kuhn, Anaís Roque, Ameyalli Santibanez, Yeowon Kim, Nancy B. Grimm and Mikhail Chester
Fault tree analysis for subway fire evacuation with agent-based modeling
Authors:
Yaning Qiao, Yikai Weng, Xiaobo Shi, Zongyou Zhu, Changyun Li, Xumiao Zhang and Jiankun Liu
Evaluating the impact of factors in vehicle based pavement sensing implementation: sensor placement, pavement temperature, speed, and threshold
Authors:
Dada Zhang, Chun-Hsing Ho and Fangfang Zhang
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
Texte du lien (https://journalofbigdata.springeropen.com/about/editorial-board):
Editorial Board | Journal of Big Data
Skip to main content
Advertisement
Search
Get published
Explore Journals
Books
About
My account
Search all SpringerOpen articles
Search
Journal of Big Data
About
Articles
Submission Guidelines
About
Contact
Editorial Board
Editorial Board
EDITORS-IN-CHIEF
Borko Furht,
Florida Atlantic University, USA
Personal Website
Taghi Koshgoftaar,
Florida Atlantic University ,USA
Personal Website
MANAGING EDITOR
Muhammad Tanveer Jan,
Florida Atlantic University, USA
Personal Website
Research areas: Computer Vision, Machine Learning, Video Processing, Autonomous Vehicles, In-Vehicle Sensors
ASSOCIATE EDITORS
Tariq Ahamed Ahanger,
Prince Sattam Bin Abdulaziz University, Saudi Arabia
Personal Website
Research areas: IoT, Cybersecurity, Big Data, AI, Edge Computing, Machine and deep learning, UAV
Shabir Ahmad,
Gachon University, Korea
Personal Website
Research areas: Internet of Things, Reinforcement Learning, Image Analysis, Medical Imaging, Big Data, Metaverse
Sabeur Aridhi, PhD,
Université de Lorraine, France
Personal Website
Research areas: big data management and analytics, scalable machine learning, scalable and deep graph learning and mining, bioinformatics
Otmane Azeroual, Dr.-Ing.,
German Center for Higher Education Research and Science Studies (DZHW), Germany
Personal Website
Research areas: Big data & business intelligence, data management, data science, artificial intelligence, data quality management, knowledge graphs & semantic web, information and data security, data ethics, digital transformation, digitization.
Muhamad Babar,
Prince Sultan University, Saudi Arabia
Personal website
Research areas: Big Data Analytics, Federated Learning, Edge Computing, Deep Learning,  Smart Cities, Internet of Things
Hamid Bouchachia,
Department of Computing and Informatics, Bournemouth University, UK
Personal Website
Research areas: AI, Machine Learning, Data Mining, Computational Intelligence
Bouziane Brik,
Sharjah University, United Arab Emirates
Personal Website
Research Areas: B5G/6G, Internet of Things, Edge Computing, Federated Deep Learning, Explainable AI
Jie-Zhi Cheng,
United Imaging Intelligence Co., Ltd, China
Personal Website
Research areas: Medical Image Analysis, Computer-Aided Diagnosis, Computer-Aided Surgery/Intervention, Computer Vision, Image Processing, Machine
Learning
Sergio Consoli,
European Commission, Join Research Centre, Italy
Personal Website
Research areas: Data Science, Artificial Intelligence, Machine Learning, Natural Language Processing, Combinatorial Optimization, (Meta)heuristics
Roberto Corizzo,
American University, United States
Personal Website
Big Data Analytics, Data Mining, Continual Learning, Machine Learning
Salvatore Distefano,
Dipartimento di Scienze Matematiche e Informatiche, Scienze Fisiche e  Scieze della Terra - MIFT University of Messina, Italy
Personal Website
Research areas: Cyber Physical Systems, Distributed (Cloud, Fog, Edge, Continuum) Computing, Quality Science and Engineering, Software and Service Engineering, Event Processing
Pierpaolo D'Urso,
Sapienza, University of Rome, Italy
Personal Website
Research areas: exploratory multivariate analysis; cluster analysis and classification; big data and data science; fuzzy clustering; robust clustering; clustering of time series; clustering of spatial data; interval-valued data analysis; clustering for data with complex structures; statistics for democracy, electoral studies, sport, social sciences, political science, finance and tourism.
Shidrokh Goudarzi, PhD,
University of West London, United Kingdom
Ezz El-Din Hemdan,
Menoufia University, Egypt
Research areas: Big Data Analytics, Data Mining and Machine Learning, Computer and Information Security, Secure and Intelligent Systems, Cloud and IoT Forensics
Vasant Honavar,
Penn State University, USA
Personal Website
Research Areas: Machine Learning, Deep Learning, Federated Learning, Causal Inference, Bioinformatics, Health Informatics
James Joshi,
University of Pittsburgh, USA
Personal Website
Research areas: Cybersecurity and Privacy, Access Control, AI/ML Privac
Atif Khan,
Islamia College Peshawar, Pakistan
Fazlullah Khan,
Abdul Wali Khan University, Pakistan
Personal Website
Research areas: Health Informatics, Data Analytics, Machine Learning, Deep Learning, Internet of Things, Security and Privacy
Yongxin Liu, PhD.,
Embry-Riddle Aeronautical University, United States
Personal Website
Research Areas: Artificial Intelligence, Cyber-Physical Systems, Data-Driven Knowledge Discovery
Ankur Mallick,
Microsoft, USA
Personal Website
Research areas: Distributed Computing, Efficient Computing, Edge Computing, Machine Learning, Federated Learning
Miasel Mongiovì,
ISTC - National Research Council, Catania, Italy
Personal Website
Research areas: Data Mining, Natural Language Processing, Graph Mining, Data Analysis
Kingsley Okoye,
Tecnologico de Monterrey, Mexico
Personal Website
Research areas: Data Science, Artificial Intelligence, Machine Learning, Big Data, Learning Analytics, Educational Technology
Vasile Palade,
Coventry University, UK
Personal Website
Research areas: Deep Learning, Computer Vision, Nature Inspired Optimization, Autonomous Driving
Alessandra Rizzardi,
University of Insubria, Italy
Personal Website
Research areas: Internet of Things, Security, Privacy, Intrusion Detection, Wireless Sensor Networks
Simona Rombo,
University of Palermo, Argentina
Personal Website
Research areas: Biological and Medical Data Analysis, Digital Advertising, Decision Support for Precision Medicine, Data Modeling and Integration, Network Analysis
Davide Tosi,
University of Insubria, Italy
Personal Website
Research areas: Big Data in Software Quality, Smart Cities, Self-Healing Software, Web Services
Paolo Trunfio,
University of Calabria, Italy
Personal Website
Research areas: Big Data Analysis, Social Media Analysis, Cloud Computing, HPC
Muhammad Umer,
Department of Computer Science & Information Technology
Islamia Universit of Bahwalpur, Pakistan
Personal Website
Research areas: Big Data Handling, Computer Vision, Deep Ensemble Learning, Feature Fusion, Text Mining, Bioinformatics
Huanjing Wang,
Western Kentucky University, USA
Personal Website
Research areas: Machine Learning, Big Data, Fraud Detection
Xianmin Wang,
China University of Geosciences (Wuhan), China
Personal Website
Research areas: Big data, Artificial intelligence, Machine and deep Learning, Image processing, Data mining
M. Arif Wani,
University of Kashmir, India
Personal Website
Research areas: Deep Learning, Deep Learning Architectures, Deep Learning Models for applications involving complex and large volumes of data, deep learning in image classification, machine learning in pattern classification
Wei Wei,
Xi'an University of Technology, China
Personal Website
Research Areas:
Internet of Things, Wireless Sensor Networks, Big Data, Machine Learning, Deep Learning, Artificial Intelligence, Image Processing, Security and Privacy
Tetsuya Yoshida,
Faculty of Engineering
Nara Women's University, Japan
Research areas: Machine Learning based on Matrix/Tensor, Clustering, Semi-Supervised Learning, Data Mining, Social Network Analysis
Peiyan Yuan,
Henan Normal University, China
Personal Website
Research areas: Social networks, Internet of Things, Edge Computing, Artificial Intelligence
Adnan Zahid, PhD,
Heriot watt university, United Kingdom
Wei Zeng,
Hong Kong University of Science and Technology, Guangzhou, China
Personal Website
Research areas: Visualization, Data Analytics, Computational Design, VR/AR
Dongfang Zhao,
University of Washington, USA
Personal Website
Research areas: Databases, High-Performance Computing, Artificial Intelligence, Computer Security
Qiang Zhu,
University of Michigan, USA
Personal Website
Research areas: Big Data Analytics, Data Integration, Data Mining, Spatio-temporal Data Processing, Query Optimization for Emerging Databases, Multidimensional Indexing
EDITORIAL BOARD
Ankur Agarwal,
Florida Atlantic University, USA
Thomas H. W. Bäck,
Universiteit Leiden, Netherlands
Marcello M. Bersani,
Politecnico di Milano, Italy
Xue-wen (William) Chen,
Wayne State University, USA
Dursen Delen
, Oklahoma State University, USA
Jun Huan,
University of Kansas, USA
Nathalie Japkowicz,
American University, USA
Mohan Kankanhalli,
National University of Singapore, Singapore
Geng Lin,
Dell Inc., USA
Darrell Long,
University of California – Santa Cruz, USA
Fabrizio Marozzo,
University of Calabria, Italy
Edin Muharemagic,
LexisNexis, USA
Sathyan Munirathinam,
Micron Technology Inc., USA
Naphtali Rishe,
Florida International University, USA
Marco Roccetti,
University of Bologna, Italy
Mei-Ling Shyu,
University of Miami, USA
Athanasios V. Vasilakos,
University of Western Macedonia, Greece
Flavio Villanustre,
LexisNexis, USA
Wei Wei,
Xi'an University of Technology, China
Yinglong Xia,
IBM T.J. Watson Research Center, USA
Yelena Yesha,
University of Maryland, USA
Du Zhang,
California State University, USA
Peng Zhang,
Guangzhou University, China
Rui Zhang,
IBM Research-Almaden, USA
Xingquan Zhu,
Florida Atlantic University, USA
Submit manuscript
Editorial Board
Sign up for article alerts and news from this journal
Follow
Follow us on Twitter
Annual Journal Metrics
Citation Impact 2023
Journal Impact Factor: 8.6
5-year Journal Impact Factor: 12.4
Source Normalized Impact per Paper (SNIP): 3.853
SCImago Journal Rank (SJR): 2.068
Speed 2023
Submission to first editorial decision (median days): 56
Submission to acceptance (median days): 205
Usage 2023
Downloads: 2,559,548
Altmetric mentions: 280
More about our metrics
ISSN: 2196-1115 (electronic)
Advertisement
Support and Contact
Jobs
Language editing for authors
Scientific editing for authors
Leave feedback
Terms and conditions
Privacy statement
Accessibility
Cookies
Follow SpringerOpen
SpringerOpen Twitter page
SpringerOpen Facebook page
By using this website, you agree to our
Terms and Conditions
,
Your US state privacy rights
,
Privacy
                statement
and
Cookies
policy.
Your privacy choices/Manage cookies
we use in the preference centre.
© 2024 BioMed Central Ltd unless otherwise stated. Part of
Springer Nature
.
==================================================
